<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <title>Qiuyun&#39;s blog</title>
  
  <subtitle>qiuyunzou@qq.com</subtitle>
  <link href="/atom.xml" rel="self"/>
  
  <link href="http://qiuyun-blog.cn/"/>
  <updated>2019-04-12T00:03:59.163Z</updated>
  <id>http://qiuyun-blog.cn/</id>
  
  <author>
    <name>Qiuyun</name>
    
  </author>
  
  <generator uri="http://hexo.io/">Hexo</generator>
  
  <entry>
    <title>压缩感知综述</title>
    <link href="http://qiuyun-blog.cn/2019/04/11/%E5%8E%8B%E7%BC%A9%E6%84%9F%E7%9F%A5%E7%BB%BC%E8%BF%B0/"/>
    <id>http://qiuyun-blog.cn/2019/04/11/压缩感知综述/</id>
    <published>2019-04-11T15:18:41.000Z</published>
    <updated>2019-04-12T00:03:59.163Z</updated>
    
    <summary type="html">
    
      &lt;p&gt;压缩感知综述.pdf&lt;br&gt;&lt;/p&gt;
    
    </summary>
    
      <category term="Signal Processing" scheme="http://qiuyun-blog.cn/categories/Signal-Processing/"/>
    
    
  </entry>
  
  <entry>
    <title>The MMSE of an Equivalent Scalar Channel with a Mixtrue Gaussian Prior</title>
    <link href="http://qiuyun-blog.cn/2019/01/25/The-MMSE-of-an-Equivalent-Scalar-Channel-with-a-Mixtrue-Gaussian-Prior/"/>
    <id>http://qiuyun-blog.cn/2019/01/25/The-MMSE-of-an-Equivalent-Scalar-Channel-with-a-Mixtrue-Gaussian-Prior/</id>
    <published>2019-01-25T07:56:22.000Z</published>
    <updated>2019-01-25T08:04:27.312Z</updated>
    
    <summary type="html">
    
      &lt;h1 id=&quot;Mixtrue-Gaussian&quot;&gt;&lt;a href=&quot;#Mixtrue-Gaussian&quot; class=&quot;headerlink&quot; title=&quot;Mixtrue Gaussian&quot;&gt;&lt;/a&gt;Mixtrue Gaussian&lt;/h1&gt;&lt;p&gt;We consider mixtrue Gaussian distribution&lt;br&gt;\begin{align}&lt;br&gt;p(h)=\sum_{k=1}^K \rho_k \mathcal{N}_c(h|0,\sigma_k^2)&lt;br&gt;\end{align}&lt;br&gt;Followings are two conditions of mixture Gaussian distribution&lt;/p&gt;
    
    </summary>
    
      <category term="Signal Processing" scheme="http://qiuyun-blog.cn/categories/Signal-Processing/"/>
    
    
  </entry>
  
  <entry>
    <title>A Variational Inference Perspective on Expectation Propagation</title>
    <link href="http://qiuyun-blog.cn/2019/01/04/A-Variational-Inference-Perspective-on-Expectation-Propagation/"/>
    <id>http://qiuyun-blog.cn/2019/01/04/A-Variational-Inference-Perspective-on-Expectation-Propagation/</id>
    <published>2019-01-04T12:11:13.000Z</published>
    <updated>2019-01-05T07:13:17.218Z</updated>
    
    <summary type="html">
    
      &lt;p&gt;Notations:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;$\text{Diag}(\boldsymbol{a})$: a diagonal matrix with $\boldsymbol{a}$ being its diagonal element.&lt;/li&gt;
&lt;li&gt;$\text{diag}(\mathbf{A})$: a vector from the diagonal element of $\mathbf{A}$.&lt;/li&gt;
&lt;li&gt;$\boldsymbol{a}\odot \boldsymbol{b}$: componentwise multiply.&lt;/li&gt;
&lt;li&gt;$\boldsymbol{a}\oslash \boldsymbol{b}$: componentwise divide.&lt;/li&gt;&lt;/ol&gt;
    
    </summary>
    
      <category term="Signal Processing" scheme="http://qiuyun-blog.cn/categories/Signal-Processing/"/>
    
    
      <category term="EP" scheme="http://qiuyun-blog.cn/tags/EP/"/>
    
  </entry>
  
  <entry>
    <title>Variational Inference for Bayesian Linear Regression</title>
    <link href="http://qiuyun-blog.cn/2019/01/03/Variational-Inference-for-Bayesian-Linear-Regression/"/>
    <id>http://qiuyun-blog.cn/2019/01/03/Variational-Inference-for-Bayesian-Linear-Regression/</id>
    <published>2019-01-03T06:59:18.000Z</published>
    <updated>2019-01-04T00:47:19.165Z</updated>
    
    <summary type="html">
    
      &lt;p&gt;Notations:&lt;/p&gt;
&lt;blockquote&gt;
&lt;ul&gt;
&lt;li&gt;KL-divergence:  Given two distribution $p(x)$ and $q(x)$, the Kullback–Leibler divergence, also written as KL-divergence, is used to value the difference between $p(x)$ and $q(x)$ denoted as&lt;br&gt;\begin{align}&lt;br&gt;\mathcal{D}_{\text{KL} } (q(x)||p(x))=\int q(x)\log \frac{q(x)}{p(x)}\text{d}x&lt;br&gt;\end{align}&lt;br&gt;The KL-divergence is also named relative entropy in information theory.&lt;/li&gt;&lt;/ul&gt;&lt;/blockquote&gt;
    
    </summary>
    
      <category term="PRML" scheme="http://qiuyun-blog.cn/categories/PRML/"/>
    
    
      <category term="Variational Inference" scheme="http://qiuyun-blog.cn/tags/Variational-Inference/"/>
    
  </entry>
  
  <entry>
    <title>Expectation Maximization</title>
    <link href="http://qiuyun-blog.cn/2019/01/03/Expectation-Maximization/"/>
    <id>http://qiuyun-blog.cn/2019/01/03/Expectation-Maximization/</id>
    <published>2019-01-03T04:41:41.000Z</published>
    <updated>2019-01-03T06:57:54.881Z</updated>
    
    <summary type="html">
    
      &lt;p&gt;Notations: We use $\mathbf{x}$ to denote vector. $\mathbf{X}$ is matrix. $(\cdot)^T$ represents transposition. $\mathcal{N}(\mathbf{x}|\mathbf{a},\mathbf{A})$ denotes a Gaussian distribution with mean $\mathbf{a}$, variance $\mathbf{A}$ and argument $\mathbf{x}$. $\mathbb{E}\left\{\cdot\right\}$ refers to expectation operation.&lt;/p&gt;
    
    </summary>
    
      <category term="Signal Processing" scheme="http://qiuyun-blog.cn/categories/Signal-Processing/"/>
    
    
      <category term="EM" scheme="http://qiuyun-blog.cn/tags/EM/"/>
    
  </entry>
  
  <entry>
    <title>2018年总结与2019年规划</title>
    <link href="http://qiuyun-blog.cn/2018/12/31/2018%E5%B9%B4%E6%80%BB%E7%BB%93%E4%B8%8E2019%E5%B9%B4%E8%A7%84%E5%88%92/"/>
    <id>http://qiuyun-blog.cn/2018/12/31/2018年总结与2019年规划/</id>
    <published>2018-12-31T00:56:02.000Z</published>
    <updated>2018-12-31T01:25:57.849Z</updated>
    
    <summary type="html">
    
      &lt;h1 id=&quot;2018年总结&quot;&gt;&lt;a href=&quot;#2018年总结&quot; class=&quot;headerlink&quot; title=&quot;2018年总结&quot;&gt;&lt;/a&gt;2018年总结&lt;/h1&gt;&lt;p&gt;[1] 发表了一篇顶级期刊，IEEE Signal Processing Letters，题目为“Concise Derivation of Approximate Message Passing Using Expectation Propagation”。&lt;br&gt;[2] 完成了北邮“申请-审核”制博士申请。&lt;br&gt;[3] 参加了一次半程马拉松，全场21.09km，成绩为1：59：47。&lt;br&gt;[4] 通过全国大学生六级考试，分数为468（425分即为合格）。&lt;br&gt;[5] 搭建了个人博客网站，用于分享一些笔记以及专业知识，目前已发表博文21篇。&lt;br&gt;[6] 初涉深度学习领域，打算做一些跨学科的研究。&lt;/p&gt;
    
    </summary>
    
      <category term="随笔" scheme="http://qiuyun-blog.cn/categories/%E9%9A%8F%E7%AC%94/"/>
    
    
  </entry>
  
  <entry>
    <title>Mutual Information and MMSE</title>
    <link href="http://qiuyun-blog.cn/2018/12/24/Mutual-Information-and-MMSE/"/>
    <id>http://qiuyun-blog.cn/2018/12/24/Mutual-Information-and-MMSE/</id>
    <published>2018-12-24T01:35:10.000Z</published>
    <updated>2018-12-24T01:41:53.729Z</updated>
    
    <summary type="html">
    
      &lt;p&gt;Notations:&lt;/p&gt;
&lt;blockquote&gt;
&lt;ul&gt;
&lt;li&gt;Mutual information (MI)&lt;br&gt;\begin{align}&lt;br&gt;I(X;Y)&lt;br&gt;&amp;amp;=\int p(x,y)\log \frac{p(x|y)}{p(x)}\text{d}x\text{d}y\\&lt;br&gt;&amp;amp;=\int p(x,y)\log \frac{p(x,y)}{p(x)p(y)}\text{d}x\text{d}y\\&lt;br&gt;&amp;amp;=\int p(x,y)\log \frac{p(y|x)}{p(y)}\text{d}x\text{d}y\\&lt;br&gt;&amp;amp;=I(Y;X)&lt;br&gt;\end{align}&lt;/li&gt;&lt;/ul&gt;&lt;/blockquote&gt;
    
    </summary>
    
      <category term="Communication" scheme="http://qiuyun-blog.cn/categories/Communication/"/>
    
    
  </entry>
  
  <entry>
    <title>Derivation of Sparse Bayesian Learning</title>
    <link href="http://qiuyun-blog.cn/2018/12/11/Derivation-of-Sparse-Bayesian-Learning/"/>
    <id>http://qiuyun-blog.cn/2018/12/11/Derivation-of-Sparse-Bayesian-Learning/</id>
    <published>2018-12-11T08:51:01.000Z</published>
    <updated>2018-12-13T06:39:13.706Z</updated>
    
    <summary type="html">
    
      &lt;h1 id=&quot;System-Model&quot;&gt;&lt;a href=&quot;#System-Model&quot; class=&quot;headerlink&quot; title=&quot;System Model&quot;&gt;&lt;/a&gt;System Model&lt;/h1&gt;&lt;p&gt;We consider following system&lt;br&gt;\begin{align}&lt;br&gt;\mathbf{y}=\mathbf{Hx}+\mathbf{w}&lt;br&gt;\end{align}&lt;br&gt;where $\mathbf{y}\in \mathbb{R}^M$ is observed signal or received signal, $\mathbf{H}\in \mathbb{R}^{M\times N}$ linear transform matix, and $\mathbf{x}\in \mathbb{R}^{N}$ is estimated signal. In addition, $\mathbf{w}\sim \mathcal{N}(\mathbf{w}|\mathbf{0},\sigma^2\mathbf{I})$ is additional Gaussian noise.&lt;/p&gt;
    
    </summary>
    
      <category term="Signal Processing" scheme="http://qiuyun-blog.cn/categories/Signal-Processing/"/>
    
    
      <category term="Sparse Bayesian Learning" scheme="http://qiuyun-blog.cn/tags/Sparse-Bayesian-Learning/"/>
    
  </entry>
  
  <entry>
    <title>误差反向传播</title>
    <link href="http://qiuyun-blog.cn/2018/12/05/%E8%AF%AF%E5%B7%AE%E5%8F%8D%E5%90%91%E4%BC%A0%E6%92%AD/"/>
    <id>http://qiuyun-blog.cn/2018/12/05/误差反向传播/</id>
    <published>2018-12-05T14:38:14.000Z</published>
    <updated>2018-12-05T14:57:29.320Z</updated>
    
    <summary type="html">
    
      &lt;h1 id=&quot;引言&quot;&gt;&lt;a href=&quot;#引言&quot; class=&quot;headerlink&quot; title=&quot;引言&quot;&gt;&lt;/a&gt;引言&lt;/h1&gt;&lt;p&gt;前馈神经网络参数的更新，其难点在于计算&lt;strong&gt;误差函数的梯度&lt;/strong&gt;。为此，我们的目标是寻找一种计算前馈神经网络的误差函数$E(\boldsymbol{w})$的梯度的一种高效方法。&lt;strong&gt;我们会看到，可以使用局部信息传递的思想完成这一点。在局部信息传递的思想中，信息在神经网络中交替地向前、向后传播&lt;/strong&gt;。这种方法被称为&lt;strong&gt;误差反向传播&lt;/strong&gt;（back propagation，BP）。&lt;/p&gt;
    
    </summary>
    
      <category term="PRML" scheme="http://qiuyun-blog.cn/categories/PRML/"/>
    
    
  </entry>
  
  <entry>
    <title>信道估计模型</title>
    <link href="http://qiuyun-blog.cn/2018/12/02/%E4%BF%A1%E9%81%93%E4%BC%B0%E8%AE%A1%E6%A8%A1%E5%9E%8B/"/>
    <id>http://qiuyun-blog.cn/2018/12/02/信道估计模型/</id>
    <published>2018-12-02T02:31:43.000Z</published>
    <updated>2018-12-02T02:34:42.092Z</updated>
    
    <summary type="html">
    
      &lt;h1 id=&quot;Notations&quot;&gt;&lt;a href=&quot;#Notations&quot; class=&quot;headerlink&quot; title=&quot;Notations&quot;&gt;&lt;/a&gt;Notations&lt;/h1&gt;&lt;ol&gt;
&lt;li&gt;矩阵拉直或矩阵列化&lt;br&gt;设$\boldsymbol{A}=(a_{ij})\in \mathbb{C}^{m\times n}$，记$\boldsymbol{a}_i=(a_{1i},\cdots,a_{mi})^T\ (i=1,\cdots, n)$，令&lt;br&gt;\begin{align}&lt;br&gt;\text{vec}(\boldsymbol{A})=&lt;br&gt;\left[&lt;br&gt;\begin{matrix}&lt;br&gt;\boldsymbol{a}_1\\&lt;br&gt;\vdots\\&lt;br&gt;\boldsymbol{a}_n&lt;br&gt;\end{matrix}&lt;br&gt;\right]&lt;br&gt;\end{align}&lt;/li&gt;&lt;/ol&gt;
    
    </summary>
    
      <category term="Communication" scheme="http://qiuyun-blog.cn/categories/Communication/"/>
    
    
      <category term="Communication" scheme="http://qiuyun-blog.cn/tags/Communication/"/>
    
  </entry>
  
  <entry>
    <title>导数、方向导数和梯度</title>
    <link href="http://qiuyun-blog.cn/2018/11/27/%E5%AF%BC%E6%95%B0%E3%80%81%E6%96%B9%E5%90%91%E5%AF%BC%E6%95%B0%E5%92%8C%E6%A2%AF%E5%BA%A6/"/>
    <id>http://qiuyun-blog.cn/2018/11/27/导数、方向导数和梯度/</id>
    <published>2018-11-27T07:02:43.000Z</published>
    <updated>2018-11-27T07:04:07.667Z</updated>
    
    <summary type="html">
    
      &lt;p&gt;&lt;strong&gt;导数的定义&lt;/strong&gt;：设函数$y=f(x)$在点$x_0$的某个邻域内有定义，当自变量$x$在$x_0$处取得增量$\triangle x$（点$x+\triangle x$仍在该邻域内）时，若存在极限&lt;br&gt;\begin{align}&lt;br&gt;f’(x_0)=\lim_{\triangle x \rightarrow 0}\frac{f(x_0+\triangle x)-f(x_0)}{\triangle x}&lt;br&gt;\end{align}&lt;br&gt;我们称函数$f(x)$在$x_0$处可导，且导数为$f’(x_0)$。&lt;/p&gt;
    
    </summary>
    
      <category term="基础数学" scheme="http://qiuyun-blog.cn/categories/%E5%9F%BA%E7%A1%80%E6%95%B0%E5%AD%A6/"/>
    
    
  </entry>
  
  <entry>
    <title>前馈神经网络</title>
    <link href="http://qiuyun-blog.cn/2018/11/26/%E5%89%8D%E9%A6%88%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/"/>
    <id>http://qiuyun-blog.cn/2018/11/26/前馈神经网络/</id>
    <published>2018-11-26T13:27:36.000Z</published>
    <updated>2018-11-26T13:30:52.298Z</updated>
    
    <summary type="html">
    
      &lt;p&gt;&lt;strong&gt;前馈神经网络&lt;/strong&gt;（feedforward neural network）也被称作&lt;strong&gt;深度前馈网络&lt;/strong&gt;（deep feedforward network）或&lt;strong&gt;多层感知器&lt;/strong&gt;（multilayer perceptron, MLP），是典型的深度学习模型。前馈网络的目标是近似某个函数$f^{\star}$，这个$f^{\star}$完成特定的分类或者回归功能。&lt;/p&gt;
    
    </summary>
    
      <category term="PRML" scheme="http://qiuyun-blog.cn/categories/PRML/"/>
    
    
  </entry>
  
  <entry>
    <title>线性高斯模型的估计方法</title>
    <link href="http://qiuyun-blog.cn/2018/11/21/%E7%BA%BF%E6%80%A7%E9%AB%98%E6%96%AF%E6%A8%A1%E5%9E%8B%E7%9A%84%E4%BC%B0%E8%AE%A1%E6%96%B9%E6%B3%95/"/>
    <id>http://qiuyun-blog.cn/2018/11/21/线性高斯模型的估计方法/</id>
    <published>2018-11-21T03:12:10.000Z</published>
    <updated>2018-11-21T13:52:30.879Z</updated>
    
    <summary type="html">
    
      &lt;p&gt;对于线性高斯模型&lt;br&gt;\begin{align}&lt;br&gt;\boldsymbol{y}=\boldsymbol{Hx}+\boldsymbol{w}&lt;br&gt;\end{align}&lt;br&gt;其中$\boldsymbol{x}\in \mathbb{R}^N$为待估计变量，其概率密度为$p(\boldsymbol{x})$。$\boldsymbol{w}$是高斯白噪声，即$\boldsymbol{w}\sim \mathcal{N}(\boldsymbol{w}|\boldsymbol{a},\boldsymbol{C}_{\boldsymbol{w} })$。信号估计的目标是根据已知的模型信息，从观测向量$\boldsymbol{y}\in \mathbb{R}^M$中恢复出原始信号$\boldsymbol{x}$。为了得到确定解，一般$\boldsymbol{y}$的维度大于$\boldsymbol{x}$的维度，即模型为超定方程组。&lt;/p&gt;
    
    </summary>
    
      <category term="统计信号处理" scheme="http://qiuyun-blog.cn/categories/%E7%BB%9F%E8%AE%A1%E4%BF%A1%E5%8F%B7%E5%A4%84%E7%90%86/"/>
    
    
  </entry>
  
  <entry>
    <title>回归问题初步之线性回归</title>
    <link href="http://qiuyun-blog.cn/2018/11/09/%E5%9B%9E%E5%BD%92%E9%97%AE%E9%A2%98%E5%88%9D%E6%AD%A5%E4%B9%8B%E7%BA%BF%E6%80%A7%E5%9B%9E%E5%BD%92/"/>
    <id>http://qiuyun-blog.cn/2018/11/09/回归问题初步之线性回归/</id>
    <published>2018-11-09T07:19:21.000Z</published>
    <updated>2018-11-11T06:28:27.737Z</updated>
    
    <summary type="html">
    
      &lt;h1 id=&quot;回归的线性模型&quot;&gt;&lt;a href=&quot;#回归的线性模型&quot; class=&quot;headerlink&quot; title=&quot;回归的线性模型&quot;&gt;&lt;/a&gt;回归的线性模型&lt;/h1&gt;&lt;p&gt;如图1所示，机器学习根据数据是否带标签分为：有监督学习（supervised learning）、无监督学习（unsupervised learning）、半监督学习/强化学习（seimi-supervised learning）。所谓有监督学习，即训练样本中包含输入矢量$\boldsymbol{x}$以及其对应的目标矢量$t$。进一步地，有监督学习主要完成回归和分类两大任务。&lt;/p&gt;
    
    </summary>
    
      <category term="PRML" scheme="http://qiuyun-blog.cn/categories/PRML/"/>
    
    
      <category term="PRML" scheme="http://qiuyun-blog.cn/tags/PRML/"/>
    
  </entry>
  
  <entry>
    <title>高斯相乘引理</title>
    <link href="http://qiuyun-blog.cn/2018/11/07/%E9%AB%98%E6%96%AF%E7%9B%B8%E4%B9%98%E5%BC%95%E7%90%86/"/>
    <id>http://qiuyun-blog.cn/2018/11/07/高斯相乘引理/</id>
    <published>2018-11-07T07:09:18.000Z</published>
    <updated>2018-11-07T07:14:31.703Z</updated>
    
    <summary type="html">
    
      &lt;h1 id=&quot;高斯PDF&quot;&gt;&lt;a href=&quot;#高斯PDF&quot; class=&quot;headerlink&quot; title=&quot;高斯PDF&quot;&gt;&lt;/a&gt;高斯PDF&lt;/h1&gt;&lt;ul&gt;
&lt;li&gt;标量实高斯分布&lt;br&gt;\begin{align}&lt;br&gt;\mathcal{N}(x|a,A)=\frac{1}{\sqrt{2\pi A} }\exp \left[{-\frac{(x-a)^2}{2A} }\right]&lt;br&gt;\end{align}&lt;/li&gt;&lt;/ul&gt;
    
    </summary>
    
      <category term="基础数学" scheme="http://qiuyun-blog.cn/categories/%E5%9F%BA%E7%A1%80%E6%95%B0%E5%AD%A6/"/>
    
    
  </entry>
  
  <entry>
    <title>条件概率与条件均值</title>
    <link href="http://qiuyun-blog.cn/2018/11/07/%E6%9D%A1%E4%BB%B6%E6%A6%82%E7%8E%87%E4%B8%8E%E6%9D%A1%E4%BB%B6%E5%9D%87%E5%80%BC/"/>
    <id>http://qiuyun-blog.cn/2018/11/07/条件概率与条件均值/</id>
    <published>2018-11-07T06:10:16.000Z</published>
    <updated>2018-11-07T06:13:21.347Z</updated>
    
    <summary type="html">
    
      &lt;p&gt;笔者在研究室内定位算法的过程中，有一些论文出现了条件均值。比如$x\sim f(x)$，那么该变量的均值为&lt;br&gt;\begin{align}&lt;br&gt;\mathbb{E}[X]=\int_{-\infty }^{+\infty }xf(x)\text{d}x&lt;br&gt;\end{align}&lt;/p&gt;
    
    </summary>
    
      <category term="基础数学" scheme="http://qiuyun-blog.cn/categories/%E5%9F%BA%E7%A1%80%E6%95%B0%E5%AD%A6/"/>
    
    
  </entry>
  
  <entry>
    <title>信号系统笔记之连续时间傅里叶变换</title>
    <link href="http://qiuyun-blog.cn/2018/11/07/%E4%BF%A1%E5%8F%B7%E7%B3%BB%E7%BB%9F%E7%AC%94%E8%AE%B0%E4%B9%8B%E8%BF%9E%E7%BB%AD%E6%97%B6%E9%97%B4%E5%82%85%E9%87%8C%E5%8F%B6%E5%8F%98%E6%8D%A2/"/>
    <id>http://qiuyun-blog.cn/2018/11/07/信号系统笔记之连续时间傅里叶变换/</id>
    <published>2018-11-07T04:36:53.000Z</published>
    <updated>2018-11-07T04:57:01.986Z</updated>
    
    <summary type="html">
    
      &lt;h1 id=&quot;引言&quot;&gt;&lt;a href=&quot;#引言&quot; class=&quot;headerlink&quot; title=&quot;引言　&quot;&gt;&lt;/a&gt;引言　&lt;/h1&gt;&lt;p&gt;学习傅里叶级数之后，我们得到一个结论，任何满足&lt;strong&gt;狄利克雷条件&lt;/strong&gt;（Dirichlet Conditions）的周期信号$f(t)$可以分解为一串&lt;strong&gt;虚指数信号的线性加权和&lt;/strong&gt;，即傅里叶级数。然而实际上，我们需要处理的信号大多为非周期信号。因此，要想对非周期信号进行频域分析，我们需要得到一个属于非周期信号的“傅里叶级数”。&lt;br&gt;&lt;/p&gt;
    
    </summary>
    
      <category term="信号系统" scheme="http://qiuyun-blog.cn/categories/%E4%BF%A1%E5%8F%B7%E7%B3%BB%E7%BB%9F/"/>
    
    
  </entry>
  
  <entry>
    <title>支持向量机</title>
    <link href="http://qiuyun-blog.cn/2018/11/07/%E6%94%AF%E6%8C%81%E5%90%91%E9%87%8F%E6%9C%BA/"/>
    <id>http://qiuyun-blog.cn/2018/11/07/支持向量机/</id>
    <published>2018-11-07T01:56:18.000Z</published>
    <updated>2018-11-09T07:32:28.499Z</updated>
    
    <summary type="html">
    
      &lt;h1 id=&quot;支持向量机定义的引出&quot;&gt;&lt;a href=&quot;#支持向量机定义的引出&quot; class=&quot;headerlink&quot; title=&quot;支持向量机定义的引出&quot;&gt;&lt;/a&gt;支持向量机定义的引出&lt;/h1&gt;&lt;p&gt;给定训练样本集$D=\left\{ {(\boldsymbol{x}_1,y_1),\cdots,(\boldsymbol{x}_m,y_m)}\right\},y_i\in \left\{ {-1,+1}\right\}$。分类最基本的出发点是找到一个超平面来区分训练样本集中的不同类别。事实上，可能存在很多这样的超平面。我们需要制定衡量标准（如：欧式距离）来确定最合适的&lt;strong&gt;超平面&lt;/strong&gt;。&lt;br&gt;&lt;/p&gt;
    
    </summary>
    
      <category term="PRML" scheme="http://qiuyun-blog.cn/categories/PRML/"/>
    
    
  </entry>
  
  <entry>
    <title>信号系统笔记之连续时间傅里叶级数</title>
    <link href="http://qiuyun-blog.cn/2018/11/06/%E4%BF%A1%E5%8F%B7%E7%B3%BB%E7%BB%9F%E7%AC%94%E8%AE%B0%E4%B9%8B%E8%BF%9E%E7%BB%AD%E6%97%B6%E9%97%B4%E5%82%85%E9%87%8C%E5%8F%B6%E7%BA%A7%E6%95%B0/"/>
    <id>http://qiuyun-blog.cn/2018/11/06/信号系统笔记之连续时间傅里叶级数/</id>
    <published>2018-11-06T09:54:57.000Z</published>
    <updated>2018-11-06T10:04:52.929Z</updated>
    
    <summary type="html">
    
      &lt;h1 id=&quot;信号的正交分解&quot;&gt;&lt;a href=&quot;#信号的正交分解&quot; class=&quot;headerlink&quot; title=&quot;信号的正交分解&quot;&gt;&lt;/a&gt;信号的正交分解&lt;/h1&gt;&lt;h2 id=&quot;矢量的几何分解&quot;&gt;&lt;a href=&quot;#矢量的几何分解&quot; class=&quot;headerlink&quot; title=&quot;矢量的几何分解&quot;&gt;&lt;/a&gt;矢量的几何分解&lt;/h2&gt;&lt;p&gt;为了理解对信号进行分解的目的，我们先从几何学的角度，回味下平面矢量以及空间矢量的分解。如图1所示，(a)$\overrightarrow{A}=c_1\overrightarrow{v_x}+c_2\overrightarrow{v_y}$，即将平面矢量分解成正交的$x$轴和$y$轴的单位矢量；（b）$\overrightarrow{A}=c_1\overrightarrow{v_x}+c_2\overrightarrow{v_y}+c_3\overrightarrow{v_z}$，即将空间矢量分解成正交的$x$轴、$y$轴和$z$轴的单位矢量。&lt;/p&gt;
    
    </summary>
    
      <category term="信号系统" scheme="http://qiuyun-blog.cn/categories/%E4%BF%A1%E5%8F%B7%E7%B3%BB%E7%BB%9F/"/>
    
    
  </entry>
  
  <entry>
    <title>深度学习笔记之人工智能、机器学习、深度学习之间的关系</title>
    <link href="http://qiuyun-blog.cn/2018/11/03/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0%E4%B9%8B%E4%BA%BA%E5%B7%A5%E6%99%BA%E8%83%BD%E3%80%81%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E3%80%81%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E4%B9%8B%E9%97%B4%E7%9A%84%E5%85%B3%E7%B3%BB/"/>
    <id>http://qiuyun-blog.cn/2018/11/03/深度学习笔记之人工智能、机器学习、深度学习之间的关系/</id>
    <published>2018-11-03T14:42:45.000Z</published>
    <updated>2018-11-06T10:09:34.601Z</updated>
    
    <summary type="html">
    
      &lt;p&gt;这两年创业圈、技术圈、互联网圈都在热烈讨论人工智能、机器学习、深度学习。那么到底什么是人工智能（AI）、机器学习（ML）和深度学习（DL），这几个概念之间又有什么样的联系呢？首先，我们通过图来理解这三者之间的关系&lt;/p&gt;
    
    </summary>
    
      <category term="深度学习" scheme="http://qiuyun-blog.cn/categories/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/"/>
    
    
  </entry>
  
</feed>
