<?xml version="1.0" encoding="utf-8"?>
<search>
  <entry>
    <title><![CDATA[深度学习笔记之线性回归与逻辑回归]]></title>
    <url>%2F2018%2F11%2F03%2F%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0%E4%B9%8B%E7%BA%BF%E6%80%A7%E5%9B%9E%E5%BD%92%E4%B8%8E%E9%80%BB%E8%BE%91%E5%9B%9E%E5%BD%92%2F</url>
    <content type="text"><![CDATA[线性回归一元线性回归给定数据集$\text{D}=\left\{ {(\mathbf{x}_1,y_1),(\mathbf{x}_2,y_2),\cdots,(\mathbf{x}_m,y_m)}\right\}$，其中$\mathbf{x}_i=[x_{i1},\cdots,x_{id}]^T$，$y_i\in \mathbb{R}$。线性回归（Linear regression）试图学得一个线性模型来尽可能准确预测实值输出标记。假设样本由$1(即，d=1)$个属性描述，线性回归试图学得如下线性模型\begin{align}f(x_i)=wx_i+b\end{align}使得$f(x_i)\approx y_i$。其中$w$和$b$需要通过最小化均方误差(mean square error, MSE)得到\begin{align}(w,b)=\underset{(w,b)}{\text{arg} \min} \sum\limits_{i=1}^m \left({f(x_i)-y_i}\right)^2\end{align}通过最小化MSE得$(w,b)$，即找到一根直线，使得所有样本到直线$y=wx+b$上的欧式距离最短。 对于数据集合$\text{D}=\left\{ {(\mathbf{x}_1,y_1),(\mathbf{x}_2,y_2),\cdots,(\mathbf{x}_m,y_m)}\right\}$，定义如下误差函数\begin{align}J=\sum\limits_{i=1}^m (y_i-(wx_i+b))^2\end{align}通过导数工具\begin{align}\frac{\partial J}{\partial b}&amp;=0 \quad \Rightarrow b=\frac{1}{m}\sum\limits_i^m(y_i-wx_i)=\bar{y}-w\bar{x}\end{align}其中$\bar{y}=\frac{1}{m}\sum_{i=1}^my_i$，$\bar{x}=\frac{1}{m}\sum_{i=1}^mx_i$。为求$w$，将$b$的表达式代入误差函数$J$中，对$w$求偏导\begin{align}\frac{\partial J}{\partial w}&amp;=\frac{\partial }{\partial w}\left[ { \sum\limits_{i=1}^m (y_i-\bar{y})-w( x_i-\bar{x} ) }\right]^2\\&amp;=-2\sum\limits_{i=1}^m [ (y_i-\bar{y} )-w( x_i-\bar{x} ) ] ( x_i -\bar{x} )\end{align}让其为0，得到\begin{align}w=\frac{\sum\limits_{i=1}^m(y_i-\bar{y})(x_i-\bar{x})}{\sum\limits_{i=1}^m(x_i-\bar{x})^2}\end{align} 多元线性回归在上一小节中，我们假设样本只要一个属性描述。这里我们假设样本有$d(d&gt;1)$个属性描述，此时，我们尝试学得多元线性模型\begin{align}f(\mathbf{x}_i)=\mathbf{w}^T\mathbf{x}_i+d\end{align}使得$f(\mathbf{x}_i)\approx y_i$，我们称之为多元线性回归（multivariate linear regression）。 对于该例子，为了简化计算过程，这里我们直接固定$d=0$，得到如下\begin{align}\mathbf{y}=\mathbf{X}^T\mathbf{w}\end{align}其中$\mathbf{X}=[\mathbf{x}_1,\cdots,\mathbf{x}_m]$，$\mathbf{w}=[w_1,\cdots,w_m]^T$，以及$\mathbf{y}=[y_1,\cdots,y_m]^T$。 定义如下误差函数\begin{align}J=||\mathbf{y}-\mathbf{X}^T\mathbf{w}||^2\end{align}通过对$\mathbf{w}$求导，并令导数为零得到\begin{align}\frac{\partial J}{\partial \mathbf{w} }=(\mathbf{XX}^T)^{-1}\mathbf{X}\mathbf{y}\end{align} 逻辑回归上一节中，我们介绍了线性回归。线性回归通过学得线性模型，对输入$\mathbf{x}$输出预测值$y$。而逻辑回归对线性回归做了压缩处理，将$f(x)$的值阈从$(-\infty,+\infty)$压缩到了$(0,1)$区间。我们通过判断输出与$\frac{1}{2}$的大小比较，输出‘ 1 ’或者‘ 0 ’。这主要归功于Sigmoid函数\begin{align}f(x)=\frac{1}{1+e^{-x} }\end{align}虽然逻辑回归的名字是回归，但其实逻辑回归是一种分类学习方法。 对于数据集合$\left\{ {(\mathbf{x}_i,y_i)}\right\}_{i=1}^m$，逻辑回归学得如下模型\begin{align}y=f(\mathbf{w}^T\mathbf{x}+b) \ \Longleftrightarrow \ \ln \frac{y}{1-y}=\mathbf{w}^T\mathbf{x}+b\end{align}我们设$y$后验概率$p(y=1|\mathbf{x})$，则有\begin{align}\ln \frac{p(y=1|\mathbf{x})}{p(y=0|\mathbf{x})} =\mathbf{w}^T\mathbf{x}+\mathbf{b}\end{align}解方程，得\begin{align}p(y=1|\mathbf{x})&amp;=\frac{e^{\mathbf{w}^T\mathbf{x}+b} }{1+e^{\mathbf{w}^T\mathbf{x}+b} }\\p(y=0|\mathbf{x})&amp;=\frac{1}{1+e^{\mathbf{w}^T\mathbf{x}+b} }\end{align}参数$(\mathbf{w},b)$通过最大似然函数确定。\begin{align}\ell(\mathbf{w},b)=\log \prod\limits_{i} p(y_i|\mathbf{x}_i;\mathbf{w},b)\end{align}]]></content>
      <categories>
        <category>深度学习</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[多元实高斯分布对协方差的偏导]]></title>
    <url>%2F2018%2F10%2F30%2F%E5%A4%9A%E5%85%83%E5%AE%9E%E9%AB%98%E6%96%AF%E5%88%86%E5%B8%83%E5%AF%B9%E5%8D%8F%E6%96%B9%E5%B7%AE%E7%9F%A9%E9%98%B5%E7%9A%84%E5%81%8F%E5%AF%BC%2F</url>
    <content type="text"><![CDATA[假设$N$维实随机矢量$\mathbf{X}$服从均值为$\mathbf{a}$，协方差为$\mathbf{A}$的高斯分布，记作$\mathbf{X}\sim \mathcal{N}(\mathbf{z}|\mathbf{a},\mathbf{A})$\begin{align}\mathcal{N}\left({\mathbf{z}|\mathbf{a},\mathbf{A} }\right)=(2\pi)^{-\frac{N}{2} }|\mathbf{A}|^{-\frac{1}{2} }\exp \left({-\frac{1}{2}(\mathbf{x}-\mathbf{a})^T\mathbf{A}^{-1}(\mathbf{x}-\mathbf{a})}\right)\end{align} 其对协方差矩阵的偏导为\begin{align}\frac{\partial \mathcal{N}(\mathbf{x}|\mathbf{a},\mathbf{A})}{\partial \mathbf{A} }&amp;=(2\pi)^{-\frac{N}{2} }\frac{\partial |\mathbf{A}|^{-\frac{1}{2} } }{\partial \mathbf{A} }\exp \left(-\frac{1}{2}(\mathbf{x}-\mathbf{a})^T\mathbf{A}^{-1}(\mathbf{x}-\mathbf{a})\right)\\&amp;\quad +(2\pi)^{-\frac{N}{2} }|\mathbf{A}|^{-\frac{1}{2} }\frac{\partial }{\partial \mathbf{A} }\exp \left[-\frac{1}{2}(\mathbf{x}-\mathbf{a})^T\mathbf{A}^{-1}(\mathbf{x}-\mathbf{a})\right]\\&amp;\overset{(a)}{=}-\frac{1}{2}\mathbf{A}^{-1}\mathcal{N}(\mathbf{z}|\mathbf{a},\mathbf{A})+\frac{1}{2}\boldsymbol{A}^{-1}(\mathbf{x}-\mathbf{a})(\mathbf{x}-\mathbf{a})^T\mathbf{A}^{-1}\mathcal{N}(\mathbf{z}|\mathbf{a},\mathbf{A})\end{align}这里最为主要的是偏导$\frac{\partial |\mathbf{A}|^{-\frac{1}{2} }}{\partial \mathbf{A} }$和$\frac{\partial }{\partial \mathbf{A} }\exp \left({-\frac{1}{2}(\mathbf{x}-\mathbf{a})^T\mathbf{A}^{-1}(\mathbf{x}-\mathbf{a})}\right)$的计算。我们给出其详细计算过程如下\begin{align}\frac{\partial |\mathbf{A}|^{-\frac{1}{2} } }{\partial \mathbf{A} }=-\frac{1}{2}|\mathbf{A}|^{-\frac{3}{2} } (|\mathbf{A}|\mathbf{A}^{-1})=-\frac{1}{2}|\mathbf{A}|^{-\frac{1}{2} }\mathbf{A}^{-1}\end{align}这里利用到偏导数公式$\frac{\partial |\mathbf{A}|}{\partial \mathbf{A} }=|\mathbf{A}|\mathbf{A}^{-1}$。 另外\begin{align}&amp;\quad \frac{\partial }{\partial \mathbf{A} }\exp \left[-\frac{1}{2}(\mathbf{x}-\mathbf{a})^T\mathbf{A}^{-1}(\mathbf{x}-\mathbf{a})\right]\\&amp;=\exp\left(-\frac{1}{2}(\mathbf{x}-\mathbf{a})^T\mathbf{A}^{-1}(\mathbf{x}-\mathbf{a})\right)\left(-\frac{1}{2}\frac{\partial }{\partial \mathbf{A} }(\mathbf{x}-\mathbf{a})^T\mathbf{A}^{-1}(\mathbf{x}-\mathbf{a})\right)\end{align}其中\begin{align}\frac{\partial (\mathbf{x}-\mathbf{a})^T\mathbf{A}^{-1}(\mathbf{x}-\mathbf{a})}{\partial \mathbf{A} }=\left({\begin{matrix}\frac{\partial (\mathbf{x}-\mathbf{a})^T\mathbf{A}^{-1}(\mathbf{x}-\mathbf{a})}{\partial A_{11} }&amp; \cdots&amp; \frac{\partial (\mathbf{x}-\mathbf{a})^T\mathbf{A}^{-1}(\mathbf{x}-\mathbf{a})}{A_{1N} }\\\vdots&amp; \ddots&amp; \vdots\\\frac{\partial (\mathbf{x}-\mathbf{a})^T\mathbf{A}^{-1}(\mathbf{x}-\mathbf{a})}{\partial A_{N1} }&amp; \cdots&amp; \frac{\partial (\mathbf{x}-\mathbf{a})^T\mathbf{A}^{-1}(\mathbf{x}-\mathbf{a})}{\partial A_{NN} }\end{matrix}}\right)\end{align}计算该矩阵元素如下\begin{align}\frac{\partial (\mathbf{x}-\mathbf{a})^T\mathbf{A}^{-1}(\mathbf{x}-\mathbf{a})}{\partial A_{ij} }&amp;=\text{tr}\left\{ {\frac{\partial (\mathbf{x}-\mathbf{a})^T\mathbf{A}^{-1}(\mathbf{x}-\mathbf{a})}{\partial \mathbf{A}^{-1} }\frac{\partial \mathbf{A}^{-1} }{\partial A_{ij} } }\right\}\\&amp;\overset{(b)}{=}\text{tr}\left\{ {-(\mathbf{x}-\mathbf{a})(\mathbf{x}-\mathbf{a})^T\mathbf{A}^{-1}\frac{\partial \mathbf{A} }{\partial A_{ij} }\mathbf{A}^{-1} }\right\}\\&amp;=\text{tr}\left\{ {-(\mathbf{x}-\mathbf{a})(\mathbf{x}-\mathbf{a})^T\mathbf{A}^{-1}\boldsymbol{e}_i\boldsymbol{e}_j^T\mathbf{A}^{-1} }\right\}\\&amp;=-\boldsymbol{e}_j^T\mathbf{A}^{-1}(\mathbf{x}-\mathbf{a})(\mathbf{x}-\mathbf{a})^T\mathbf{A}^{-1}\boldsymbol{e}_i\end{align}这里步骤$(b)$利用到偏导公式$\frac{\partial g(\mathbf{U})}{\partial x}=\text{tr}\left\{ {\frac{\partial g(\mathbf{U})}{\partial \mathbf{U} }\frac{\partial \mathbf{U} }{\partial x} }\right\}$以及$\frac{\partial \mathbf{U}^{-1} }{\partial x}=-\mathbf{U}^{-1}\frac{\partial \mathbf{U} }{\partial x}\mathbf{U}^{-1}$。因此，可以得到\begin{align}\frac{\partial (\mathbf{x}-\mathbf{a})^T\mathbf{A}^{-1}(\mathbf{x}-\mathbf{a})}{\partial \mathbf{A} }=-(\boldsymbol{A}^{-1})^T(\mathbf{x}-\mathbf{a})(\mathbf{x}-\mathbf{a})^T(\boldsymbol{A}^{-1})^T\end{align}若假设$\mathbf{A}$是对称矩阵, 则有\begin{align}\frac{\partial (\mathbf{x}-\mathbf{a})^T\mathbf{A}^{-1}(\mathbf{x}-\mathbf{a})}{\partial \mathbf{A} }=-\boldsymbol{A}^{-1}(\mathbf{x}-\mathbf{a})(\mathbf{x}-\mathbf{a})^T\mathbf{A}^{-1}\end{align}]]></content>
      <categories>
        <category>基础数学</category>
      </categories>
      <tags>
        <tag>Gaussian</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[矩阵奇异值分解]]></title>
    <url>%2F2018%2F10%2F29%2F%E7%9F%A9%E9%98%B5%E5%A5%87%E5%BC%82%E5%80%BC%E5%88%86%E8%A7%A3%2F</url>
    <content type="text"><![CDATA[Notations:(1)$\text{Diag}(\boldsymbol{x})$表示以矢量为矩阵对角线元素构成对角阵，如$\text{Diag}(a,b)=\left({\begin{array}{cccc}a&amp;0\\0&amp;b\end{array}}\right)$；(2)粗体符号表示矩阵或者矢量，如$\boldsymbol{x}$表示矢量，$\boldsymbol{A}$表示矩阵。 特征值与特征向量 矩阵的乘法对应着一种线性变换使得原向量在方向和长度上发生变化，比如$f(\boldsymbol{x})=\boldsymbol{A}\boldsymbol{x}\quad (\boldsymbol{x}\in \mathbb{R}^n, \boldsymbol{A}\in \mathbb{R}^{m\times n})$，$f$表示从$\mathbb{R}^n$空间到$\mathbb{R}^m$空间的一种线性映射关系。我们考虑$\boldsymbol{A}$是方阵的情况。\begin{align}\boldsymbol{y}=\boldsymbol{Ax}\end{align}其中$\boldsymbol{y}\in \mathbb{R}^m$。矩阵$\boldsymbol{A}$与向量$\boldsymbol{x}$相乘，表示对$\boldsymbol{x}$进行一次方向和长度上的变换，即向量$\boldsymbol{y}$。例如：$\boldsymbol{A}=\left({\begin{array}{cccc}a_{11},a_{12}\\a_{21},a_{22}\end{array} }\right)$, $\boldsymbol{x}=(b_1,b_2)^\text{T}$，则\begin{align}\boldsymbol{y}=\left({\begin{array}{cccc}a_{11}b_1+a_{12}b_2\\a_{21}b_1+a_{22}b_2\end{array}}\right)\end{align}$|\boldsymbol{x}|=\sqrt{b_1^2+b_2^2},\ |\boldsymbol{y}|=\sqrt{(a_{11}b_1+a_{12}b_2)^2+(a_{21}b_1+a_{22}b_2)^2}$$\angle(\boldsymbol{x},\boldsymbol{y})=\cos^{-1}\frac{b_1(a_{11}b_1+a_{12}b_2)+b_2(a_{21}b_1+a_{22}b_2)}{|\boldsymbol{x}||\boldsymbol{y}|}$问题：对于线性变换矩阵$\boldsymbol{A}$,是否存在这样一个向量$\boldsymbol{\xi}$, 经过这种特定的变换之后保持方向不变，只是进行长度上的拉伸，即使得$\angle (\boldsymbol{\xi},\boldsymbol{y})=0,\ |\boldsymbol{y}|=|\lambda| |\boldsymbol{\xi}|$。 定义：设$\boldsymbol{A}$是$n$阶方阵，如果数$\lambda$和$n$维非零列向量$\boldsymbol{x}$满足\begin{align}\boldsymbol{Ax}=\lambda\boldsymbol{x}\end{align}称$\lambda$是矩阵$\boldsymbol{A}$的特征值，$\boldsymbol{x}$是矩阵$\boldsymbol{A}$对应$\lambda$的特征向量[1]。 根据上面的描述，我们知道，特征向量就是这样一个满足经过线性变换阵$\boldsymbol{A}$之后，只发生长度上变换，方向不变的向量。那我们为什么求这样的特征值与特征向量呢？可以这样理解，对于一个实际的线性系统，其特性可以用矩阵$\boldsymbol{A}$来描述，对于输入向量$\boldsymbol{x}$，系统输出为$\boldsymbol{y}$会出现相位滞后、放大或者缩小等现象，而对于输入为特征向量$\xi$，该系统的输出只发生缩放，没有相位的变化。 设$\boldsymbol{\xi}_i$是矩阵对应于$\lambda_i$的特征值，则有\begin{align}&amp;\boldsymbol{A\xi}_i=\lambda_i\boldsymbol{\xi}_i\\&amp;\Rightarrow (\boldsymbol{A\xi}_1,\cdots,\boldsymbol{A\xi}_n)=(\lambda_1\boldsymbol{\xi}_1,\cdots,\lambda_n\boldsymbol{\xi}_n)\\&amp;\Rightarrow \boldsymbol{A}(\boldsymbol{\xi}_1,\cdots,\boldsymbol{\xi}_n)=(\boldsymbol{\xi}_1,\cdots,\boldsymbol{\xi}_n)\left({\begin{matrix}\lambda_1&amp; &amp; &amp; \\ &amp;\lambda_2 &amp; &amp;\\ &amp; &amp;\ddots &amp;\\ &amp; &amp; &amp;\lambda_n\end{matrix}}\right)\end{align}令$\boldsymbol{P}=(\boldsymbol{\xi}_1,\cdots,\boldsymbol{\xi}_n)$，$\boldsymbol{\Lambda}=\text{Diag}(\lambda_1,\cdots,\lambda_n)$则有\begin{align}\boldsymbol{AP}=\boldsymbol{P}\boldsymbol{\Lambda}\end{align}因此，矩阵$\boldsymbol{A}$对角化的问题就等价于方阵$\boldsymbol{P}$是否可逆，即$\boldsymbol{A}$是否有$n$个线性无关的特征向量。矩阵$\boldsymbol{A}$有$n$个线性无关的特征向量有两种情况（1）$n$阶方阵$\boldsymbol{A}$有$n$个不同的特征值，对应有$n$线性无关的特征向量。（2）$n$阶方阵$\boldsymbol{A}$有重根情况，且对应$k$重根特征值$\lambda$，有$n-\text{rank}(\boldsymbol{A}-\lambda\mathbf{I})=k$。 注意，并不是任意的矩阵都可以相似对角化。以下针对$\boldsymbol{P}$可逆的情况，那么有\begin{align}\boldsymbol{A}=\boldsymbol{P\Lambda P}^{-1}\end{align}进一步的，若$\boldsymbol{P}$是一个正交矩阵，即\begin{align}\boldsymbol{A}=\boldsymbol{P\Lambda P}^{T}=\sum\limits_{i=1}^n\lambda_i\boldsymbol{\xi}_i\boldsymbol{\xi}^\text{T}\end{align}也就是说，$\boldsymbol{A}$矩阵可以由特征向量线性组合进行表示。 奇异值分解矩阵的特征值分解仅仅是针对方阵的，对于长方形矩阵$\boldsymbol{A}\in \mathbb{R}^{m\times n}$，也存在着类似的分解，称奇异值分解[2]。定义：设矩阵$\boldsymbol{A}\in \mathbb{R}^{m\times n}$，且$rank(\boldsymbol{A})=r$，则存在$m$阶正交矩阵$\boldsymbol{V}$和$n$阶正交矩阵$\boldsymbol{U}$，使得\begin{align}\boldsymbol{A}=\boldsymbol{V\Sigma U}^\text{T}\end{align}其中$\boldsymbol{\Sigma}=\left({\begin{array}{cccc}\boldsymbol{\Lambda} &amp;\boldsymbol{0}_{(r)\times(n-r)}\\\boldsymbol{0}_{(m-r)\times r} &amp;\boldsymbol{0}_{(m-r)\times(n-r)}\end{array} }\right)$，其中$\boldsymbol{\Lambda}=\text{Diag}(\sigma_1,\sigma_2,\cdots,\sigma_r)$，并且$\sigma_1\geq\sigma_2\cdots\geq\sigma_r\geq 0$。证：因为$\text{rank}(\boldsymbol{A})=r$，因此设$\boldsymbol{A}^\text{T}\boldsymbol{A}$的特征值为\begin{align}\sigma_1^2\geq\cdots,\geq \sigma_r^2\geq0,\quad \sigma_{r+1}^2=\sigma_{n}^2=0\end{align}由于$\boldsymbol{A}^\text{T}\boldsymbol{A}$是对称矩阵，因此必可以相似对角化[1]，即存在正交矩阵$U$，使得\begin{align}\boldsymbol{U}^\text{T}\boldsymbol{A}^\text{T}\boldsymbol{A}\boldsymbol{U}=\text{Diag}(\sigma_1^2,\cdots,\sigma_r^2,\underbrace{0,\cdots,0}_{n-r})\end{align}记$\boldsymbol{U}=[\boldsymbol{U}_1,\boldsymbol{U}_2]$,其中$\boldsymbol{U}_1$是一个$n\times r$的矩阵，$\boldsymbol{U}_2$是一个$n\times(n-r)$的矩阵。因此，上式可以写为\begin{align}\boldsymbol{A}^\text{T}\boldsymbol{A}[\boldsymbol{U}_1,\boldsymbol{U}_2]=[\boldsymbol{U}_1,\boldsymbol{U}_2]\left({\begin{array}{cccc}\boldsymbol{\Lambda}^2 &amp;\boldsymbol{0}\\\boldsymbol{0} &amp;\boldsymbol{0}\end{array}}\right)\end{align}则有\begin{align}\boldsymbol{A}^\text{T}\boldsymbol{AU}_1=\boldsymbol{U}_1\boldsymbol{\Lambda}^2,\quad \boldsymbol{A}^\text{T}\boldsymbol{AU}_2=\boldsymbol{0}\end{align}记$\boldsymbol{V}=[\boldsymbol{V}_1,\boldsymbol{V}_2]$，其中$\boldsymbol{V}_1$是$m\times r$矩阵，$\boldsymbol{V}_2$是$m\times(m-r)$矩阵\begin{align}\boldsymbol{A}^\text{T}\boldsymbol{AU}_1=\boldsymbol{U}_1\boldsymbol{\Lambda}^2\ \Rightarrow \boldsymbol{A}^\text{T}\boldsymbol{AU}_1\boldsymbol{\Lambda}=\boldsymbol{U}_1\boldsymbol{\Lambda}\end{align}令$\boldsymbol{V}_1=\boldsymbol{AU}_1\boldsymbol{\Lambda}^{-1}$，有\begin{align}\boldsymbol{V}_1^\text{T}\boldsymbol{V}_1&amp;=(\boldsymbol{AU}_1\boldsymbol{\Lambda}^{-1})^\text{T}\boldsymbol{AU}_1\boldsymbol{\Lambda}^{-1}\\&amp;=\boldsymbol{\Sigma}^{-1}\boldsymbol{U}_1^\text{T}\boldsymbol{A}^\text{T}\boldsymbol{A}\boldsymbol{U}_1\boldsymbol{\Sigma}^{-1}\\&amp;=\boldsymbol{\Lambda}^{-1}\boldsymbol{\Sigma}^2\boldsymbol{\Lambda}^{-1}\\&amp;=\mathbf{I}_{r}\end{align}即$\boldsymbol{V}_1$是列正交规范化矩阵。取$\boldsymbol{V}_2$，使得$\boldsymbol{V}=[\boldsymbol{V}_1,\boldsymbol{V}_2]$是正交矩阵，因此\begin{align}\boldsymbol{V}_2\boldsymbol{\boldsymbol{AU}_1}=\boldsymbol{V}_2^\text{T}\boldsymbol{V}_1\boldsymbol{\Lambda}=\boldsymbol{0}\end{align}那么\begin{align}\boldsymbol{V}^\text{T}\boldsymbol{A}\boldsymbol{U}=\left({\begin{array}{cccc}\boldsymbol{V}_1^\text{T}\\\boldsymbol{V}_2^\text{T}\end{array}}\right)\boldsymbol{A}[\boldsymbol{U}_1,\boldsymbol{U}_2]=\left({\begin{array}{cccc}\boldsymbol{V}_1^\text{T}\boldsymbol{A}\boldsymbol{U}_1 &amp;\boldsymbol{V}_1^\text{T}\boldsymbol{A}\boldsymbol{U}_2\\\boldsymbol{V}_2\boldsymbol{AU}_1,&amp;\boldsymbol{V}_2^\text{T}\boldsymbol{AU}_2\end{array}}\right)=\left({\begin{array}{cccc}\boldsymbol{\Lambda} &amp;\boldsymbol{0}\\\boldsymbol{0} &amp;\boldsymbol{0}\end{array}}\right)\end{align}即\begin{align}\boldsymbol{A}=\boldsymbol{V}\boldsymbol{\Sigma}\boldsymbol{U}^\text{T}\end{align} Pseudo逆矩阵令$\boldsymbol{A}=\boldsymbol{V\Sigma U}^\text{T}$是矩阵$\boldsymbol{A}\in \mathbb{R}^{m\times n}$的奇异值分解，且$\text{rank}(\boldsymbol{A})=r$，定义矩阵$\boldsymbol{A}$的pseudo逆为\begin{align}\boldsymbol{A}^{+}=\boldsymbol{U}\boldsymbol{\Sigma}^{-1}\boldsymbol{V}^\text{T}\in \mathbb{R}^{n\times m}\end{align}也称为Moore-Penrose广义逆矩阵。另外一种表达式是\begin{align}\boldsymbol{A}^{+}=(\boldsymbol{A}^\text{T}\boldsymbol{A})^{-1}\boldsymbol{A}^\text{T}=\boldsymbol{A}^\text{T}(\boldsymbol{AA}^\text{T})^{-1}\end{align}可以很容易证明两种表达式是等价的，我们可以从长方形矩阵的奇异分解来解释第二个式子表达式的合理性。当$m&gt;n$时，采用$\boldsymbol{A}^{+}=(\boldsymbol{A}^\text{T}\boldsymbol{A})^{-1}\boldsymbol{A}^\text{T}$；当$m&lt;n$时，通常采用$\boldsymbol{A}^{+}=\boldsymbol{A}^\text{T}(\boldsymbol{AA}^\text{T})^{-1}$。 参考文献[1] 同济大学数学系, 线性代数[M].北京: 高等教育出版社, 2012.[2] 戴华, 矩阵论[M].北京: 科学出版社, 2015.]]></content>
      <categories>
        <category>基础数学</category>
      </categories>
      <tags>
        <tag>常用矩阵公式</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[矩阵求逆引理及其证明]]></title>
    <url>%2F2018%2F10%2F29%2F%E7%9F%A9%E9%98%B5%E6%B1%82%E9%80%86%E5%BC%95%E7%90%86%E5%8F%8A%E5%85%B6%E8%AF%81%E6%98%8E%2F</url>
    <content type="text"><![CDATA[矩阵求逆引理，或者称Sherman-Woodbury-Morrison公式\begin{align}(\boldsymbol{A}+\boldsymbol{BC})^{-1}=\boldsymbol{A}^{-1}-\boldsymbol{A}^{-1}\boldsymbol{B}(\mathbf{I}+\boldsymbol{C}\boldsymbol{A}^{-1}\boldsymbol{B})^{-1}\boldsymbol{CA}^{-1}\end{align}其中$\boldsymbol{A}\in\mathbb{R}^{n\times n}$是非奇异矩阵，$\boldsymbol{B}\in \mathbb{R}^{n\times p}$， $C\in \mathbb{R}^{p\times n}$。证明：考虑线性等式\begin{align}(\boldsymbol{A}+\boldsymbol{BC})\boldsymbol{x}=\boldsymbol{b}\end{align}其中$\boldsymbol{A}\in\mathbb{R}^{n\times n}$是非奇异矩阵，$\boldsymbol{B}\in \mathbb{R}^{n\times p}$， $C\in \mathbb{R}^{p\times n}$。定义$\boldsymbol{y}=\boldsymbol{Cx}$，则有\begin{align}\left\{ {\begin{matrix}{\boldsymbol{Ax}+\boldsymbol{By}=\boldsymbol{b} } \\{\boldsymbol{y}=\boldsymbol{Cx} }\end{matrix} } \right.\end{align}该方程组可以写成块矩阵的形式\begin{align}\left[ {\begin{matrix}{\boldsymbol{A} } &amp; {\boldsymbol{B} }\\{\boldsymbol{C} } &amp; {-\mathbf{I} }\end{matrix} } \right]\left[ {\begin{matrix}{ \boldsymbol{x} } \\{ \boldsymbol{y} }\end{matrix} } \right]{\rm{ = } }\left[ {\begin{matrix}{\boldsymbol{b} }\\{\boldsymbol{0} }\end{matrix} } \right]\end{align}根据方程组（1）式，有$\boldsymbol{x}=\boldsymbol{A}^{-1}(\boldsymbol{b}-\boldsymbol{By})$，代入方程组（2）式中有\begin{align}\boldsymbol{y}=\boldsymbol{C}\boldsymbol{A}^{-1}(\boldsymbol{b}-\boldsymbol{By})\end{align}合并同类项，有\begin{align}\boldsymbol{y}=(\mathbf{I}+\boldsymbol{CA}^{-1}\boldsymbol{B})^{-1}\boldsymbol{A}^{-1}\boldsymbol{b}\end{align}代入$\boldsymbol{x}=\boldsymbol{A}^{-1}(\boldsymbol{b}-\boldsymbol{B}\boldsymbol{y})$中，得到\begin{align}\boldsymbol{x}=(\boldsymbol{A}^{-1}-\boldsymbol{A}^{-1}\boldsymbol{B}(\mathbf{I}+\boldsymbol{C}\boldsymbol{A}^{-1}\boldsymbol{B})^{-1}\boldsymbol{CA}^{-1})\boldsymbol{b}\end{align}因此，结合$(\boldsymbol{A}+\boldsymbol{BC})\boldsymbol{x}=\boldsymbol{b}$，得到\begin{align}(\boldsymbol{A}+\boldsymbol{BC})^{-1}=\boldsymbol{A}^{-1}-\boldsymbol{A}^{-1}\boldsymbol{B}(\mathbf{I}+\boldsymbol{C}\boldsymbol{A}^{-1}\boldsymbol{B})^{-1}\boldsymbol{CA}^{-1}\end{align}特别地，$\boldsymbol{B},\boldsymbol{C}$为矢量时，有\begin{align}(\boldsymbol{A}+\boldsymbol{uv}^{T})^{-1}=\boldsymbol{A}^{-1}-\frac{\boldsymbol{A}^{-1}\boldsymbol{uv}^T\boldsymbol{A}^{-1} }{1+\boldsymbol{v}^T\boldsymbol{A}^{-1}\boldsymbol{u} }\end{align} 说明：该证明过程是翻译body的书 《Convex Optimization》 p-678的内容。 Remake：单纯的应用矩阵求逆引理并不能降低计算量，当一个矩阵$\boldsymbol{D}$可以分解成$\boldsymbol{A}+\boldsymbol{BC}$，并且已知$\boldsymbol{A}^{-1}$已知，利用矩阵求逆引理，可以得到$\boldsymbol{D}$的逆。]]></content>
      <categories>
        <category>基础数学</category>
      </categories>
      <tags>
        <tag>常用矩阵公式</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[矩阵正态分布]]></title>
    <url>%2F2018%2F10%2F29%2F%E7%9F%A9%E9%98%B5%E6%AD%A3%E6%80%81%E5%88%86%E5%B8%83%2F</url>
    <content type="text"><![CDATA[正态分布关于正态分布的由来，在文章《正态分布的前世今生》中写的很清楚，正态分布是由二项分布而来。正态分布的密度形式首次发现是在棣莫弗-拉普拉斯中心极限定理中。读者可以通过以下几个链接深入了解正态分布的含义正态分布的前世今生（上）正态分布的前世今生（下）为什么正态分布如此常见棣莫弗-拉普拉斯中心极限定理设随机变量$X_n(n=1,2,\cdots)$服从参数为$n,p（0&lt;p&lt;1）$的二项分布，则对于任意$x$， 有\begin{align}\underset{n\rightarrow \infty}{\lim} \text{P}\left\{ {\frac{X_n-np}{\sqrt{np(1-p)} }\leq x}\right\}=\int_{-\infty}^x \frac{1}{\sqrt{2\pi} }\exp \left({-\frac{t^2}{2} }\right)\text{d}t=\Phi(x)\end{align}从该定理中可以看出，当$n\rightarrow \infty$时候，可以用二项分布趋于高斯分布。我们可以通过棣莫弗-拉普拉斯中心极限定理来计算二项分布的概率。 设随机变量随机变量$X$服从正态分布（高斯分布）$X\sim \mathcal{N}(\mu,\sigma_2^2)$，则其概率密度函数表示为\begin{align}p(x)=\frac{1}{\sqrt{2\pi\sigma^2} }\exp\left[{-\frac{(x-\mu)^2}{2\sigma} }\right]\end{align} 多元正态分布设$X,Y$为独立同分布的随机变量，且$X\sim \mathcal{N}(0,1)$。则$X,Y$的联合分布为\begin{align}p(x,y)=p(x)p(y)=\frac{1}{2\pi}\exp(-\frac{1}{2}(x^2+y^2))\end{align}设$\mathbf{z}=[X,Y]^T$，则有\begin{align}p(\boldsymbol{z})=\frac{1}{\pi}\exp (-\frac{1}{2}\boldsymbol{z}^T\boldsymbol{z})=\frac{1}{\pi}\exp (-\frac{1}{2}\text{tr}(\boldsymbol{z}\boldsymbol{z}^T))\end{align}令$\mathbf{z}=\boldsymbol{A}(\mathbf{x}-\boldsymbol{\mu})$，该线性变换的雅可比行列式为\begin{align}J=|\boldsymbol{A}|\end{align}代入$\mathbf{z}$的概率公式中有,\begin{align}p(\boldsymbol{x})=\frac{|\boldsymbol{A}|}{\pi}\exp \left[{-\frac{1}{2}(\boldsymbol{x}-\boldsymbol{\mu})^T\boldsymbol{A}^T\boldsymbol{A}(\boldsymbol{x}-\boldsymbol{\mu})}\right]\end{align}令$\boldsymbol{\Sigma}^{-1}=\boldsymbol{A}^T\boldsymbol{A}$，则\begin{align}p(\boldsymbol{x})=\frac{1}{\sqrt{|\boldsymbol{\Sigma}|}2\pi}\exp \left[{-\frac{1}{2}(\boldsymbol{x}-\boldsymbol{\mu})^T\boldsymbol{\Sigma}^{-1}(\boldsymbol{x}-\boldsymbol{\mu})}\right]\end{align}若$\mathbf{x}$的维数是$n$，则有\begin{align}p(\boldsymbol{x})=\frac{1}{\sqrt{|\boldsymbol{\Sigma}|}(2\pi)^{n/2} }\exp \left[{-\frac{1}{2}(\boldsymbol{x}-\boldsymbol{\mu})^T\boldsymbol{\Sigma}^{-1}(\boldsymbol{x}-\boldsymbol{\mu})}\right]\end{align} 矩阵正态分布设随机矢量$\mathbf{x}\in \mathbb{R}^n$服从多元高斯分布$\mathbf{x}\sim \mathcal{N}(\boldsymbol{0},\mathbf{I})$，随机矢量$\mathbf{y}$与$\mathbf{x}$独立同分布，则$\mathbf{x},\mathbf{y}$的联合概率密度为\begin{align}p(\mathbf{x},\mathbf{y})=\frac{1}{(2\pi)^n}\exp\left[{-\frac{1}{2}(\mathbf{x}^T\mathbf{x}+\mathbf{y}^T\mathbf{y})}\right]\end{align}令$\mathbf{Z}=[\mathbf{x},\mathbf{y}]$，则有\begin{align}p(\mathbf{Z})=\frac{1}{(2\pi)^n}\exp\left[{-\frac{1}{2}\text{tr}(\mathbf{Z}\mathbf{Z}^T)}\right]\end{align}设$\mathbf{Z}=\boldsymbol{A}(\mathbf{X}-\boldsymbol{M})\boldsymbol{B}$, 其中$\boldsymbol{A}\in \mathbb{R}^{n\times n}, \boldsymbol{B}\in \mathbb{R}^{2\times 2}$。其雅可比行列式为\begin{align}J=|\boldsymbol{A}|^n|\boldsymbol{B}|^2\end{align}关于上式的详细解释参见附录A。因此\begin{align}p(\boldsymbol{X})&amp;=\frac{1}{(2\pi)^n}|\boldsymbol{A}|^n|\boldsymbol{B}|^2\exp\left[{-\frac{1}{2}\text{tr}(\boldsymbol{A}(\boldsymbol{X}-\boldsymbol{M})\boldsymbol{BB}^T(\boldsymbol{X}-\boldsymbol{M})^T\boldsymbol{A}^T)}\right]\\&amp;=\frac{1}{(2\pi)^n}|\boldsymbol{A}|^n|\boldsymbol{B}|^2\exp\left[{-\frac{1}{2}\text{tr}(\boldsymbol{A}^T\boldsymbol{A}(\boldsymbol{X}-\boldsymbol{M})\boldsymbol{BB}^T(\boldsymbol{X}-\boldsymbol{M})^T)}\right]\end{align}令$\boldsymbol{\Omega}^{-1}=\boldsymbol{A}^T\boldsymbol{A}$， $\boldsymbol{\Sigma}^{-1}=\boldsymbol{BB}^T$， 则\begin{align}p(\boldsymbol{X})=\frac{1}{(2\pi)^n}|\boldsymbol{\Omega}|^{-n/2}|\boldsymbol{\Sigma}|^{-2/2}\exp\left[{-\frac{1}{2}\text{tr}(\boldsymbol{\Omega}^{-1}(\boldsymbol{X}-\boldsymbol{M})\boldsymbol{\Sigma}^{-1}(\boldsymbol{X}-\boldsymbol{M})^T)}\right]\end{align}若$\boldsymbol{Z}$有$p$列，则\begin{align}p(\boldsymbol{X})=\frac{1}{(2\pi)^{np} }|\boldsymbol{\Omega}|^{-{n}/2}|\boldsymbol{\Sigma}|^{-p/2}\exp\left[{-\frac{1}{2}\text{tr}(\boldsymbol{\Omega}^{-1}(\boldsymbol{X}-\boldsymbol{M})\boldsymbol{\Sigma}^{-1}(\boldsymbol{X}-\boldsymbol{M})^T)}\right]\end{align} 附录 A设线性变换$\boldsymbol{Y}=\boldsymbol{AX}$, 其中$\boldsymbol{X}\in \mathbb{R}^{m\times n}$, $\boldsymbol{A}\in \mathbb{R}^{m\times m}$\begin{align}\text{vec}(\boldsymbol{Y})=\text{vec}(\boldsymbol{AX})=(\mathbf{I}_n \otimes \boldsymbol{A})\text{vec}(\boldsymbol{X})\end{align}因此该线性变换的雅可比行列式为\begin{align}J=|\mathbf{I}_n \otimes \boldsymbol{A}|=|\boldsymbol{A}|^{n}\end{align}设置线性变换$\boldsymbol{Y}=\boldsymbol{XB}$，其中$\boldsymbol{X}\in \mathbb{R}^{m\times n}$, $\boldsymbol{B}\in \mathbb{R}^{n\times n}$\begin{align}\text{vec}(\boldsymbol{Y})=\text{vec}(\boldsymbol{XB})=(\boldsymbol{B}^T\otimes \mathbf{I}_m)\text{vec}(\boldsymbol{X})\end{align}因此该线性变换的雅可比行列式为\begin{align}J=|\boldsymbol{B}^T\otimes \mathbf{I}_m|=|\boldsymbol{B}|^{m}\end{align}]]></content>
      <categories>
        <category>基础数学</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[深度学习笔记之主成分分析]]></title>
    <url>%2F2018%2F10%2F29%2F%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0%E4%B9%8B%E4%B8%BB%E6%88%90%E5%88%86%E5%88%86%E6%9E%90%2F</url>
    <content type="text"><![CDATA[Definitions（定义）主成分分析（principal components analysis, PCA）简称PCA，是一种广泛应用于数据降维（data dimensionality reduction）、有损数据压缩（lossy data compression）、特征提取（feature extraction）以及数据可视化等的一种技术，也被称为Karhunen-Lo`eve变换。 关于PCA的定义主要有两种： PCA是一种将数据投影到低维线性空间（principal subspace，主成子空间）使得投影之后的差异最大的正交投影。 PCA是一种最小化平均投影成本（average projection cost），投影点与数据点之间的均方距离最小，即数据损失精度最小。 Example （例子）我们通过一个有损压缩的例子来介绍PCA。假设我们有$m$个数据点$\left\{ { \boldsymbol{x}^{(1)},\cdots,\boldsymbol{x}^{(m)} }\right\}$，其中数据维度为$\boldsymbol{x}^{(1)}\in \mathbb{R}^n$。因此，存储这些数据，需要$m\times n$个单元。为了节省存储单元，我们考虑有损压缩。有损压缩，意味着我们可以用较少的存储单元储存数据，当然，这会损失些精度。因此我们要尽量的减少精度的损失。 我们将这些数据压缩成低维数据，即每一个$\boldsymbol{x}^{(i)}\in \mathbb{R}^n$都可以找到一个对应的$\boldsymbol{c}\in \mathbb{R}^l, (n\gg l)$。这里，我们用映射$f:\mathbb{R}^n\rightarrow \mathbb{R}^l$来表示，即$f(\boldsymbol{x})=\boldsymbol{c}$。对应的解压缩，我们用函数$\text{g}:\mathbb{R}^l\rightarrow \mathbb{R}^n$来表示，即$\hat{\boldsymbol{x} }=\text{g}(f(\boldsymbol{x}))$。 为了解压缩尽量简单，我们限制解压缩是经过一个线性变换矩阵$\boldsymbol{D}$来完成，则解压缩信号表示为$\text{g}(c)=\boldsymbol{D}\boldsymbol{c}$。这里，我们限制矩阵$\boldsymbol{D}$是列正交矩阵（矩阵中列两两正交）。一般来说，增大$\boldsymbol{D}$的能量，需要降低$\boldsymbol{c}$的能量，因此，我们对$\boldsymbol{D}$进行归一化处理（归一化与未归一化大部分情况的结果是相等的，但也存在一些情况下归一化的情况更好，因此通常我们会对$\boldsymbol{D}$进行归一化处理）。 我们从矩阵乘法的角度来理解$\boldsymbol{x}=\boldsymbol{Dc}$。通常，我们理解矩阵乘法从图的左图出发，但一般不会考虑右图的理解方式。这里，我们从右图的理解方式出发。由于$\boldsymbol{D}$是列正交的，因此$\left\{ { \boldsymbol{d}_i}\right\}_{i\in [l]}$张成了一个$l$空间，而$\left\{ { \boldsymbol{d}_i}\right\}_{i\in [l]}$则是这个$\mathbb{R}^l$空间的一组完备正交基。$\boldsymbol{x}$在基$\boldsymbol{d}_i$上的投影即为$c_i$。我们用$\left\{ { \boldsymbol{d}_i}\right\}_{i\in [l]}$线性表出$\boldsymbol{x}$。\begin{align}\boldsymbol{x}=\sum\limits_{i\in[l]} c_i\boldsymbol{d}_i\end{align}为了使得解压之后的数据损失尽可能少的精度，我们需要通过使得如下损失函数（也称，代价函数 cost function）最小来得到压缩数据的具体表达式\begin{align}\mathcal{R}= || \boldsymbol{x}-\boldsymbol{Dc} ||_2\end{align}损失函数是关于$\boldsymbol{c}$的线性变换$(\boldsymbol{x}-\boldsymbol{Dc})$的范数，因此是convex的（所有的范数均是convex function）。 通过导数工具，有\begin{align}\frac{\partial \mathcal{R} }{\partial \boldsymbol{c} }&amp;=\frac{\partial }{\partial \boldsymbol{c} } \left(\boldsymbol{x}^T\boldsymbol{x}-2\boldsymbol{x}^T\boldsymbol{Dc}+\boldsymbol{c}^T\boldsymbol{D}^T\boldsymbol{D}\boldsymbol{c}\right)\\&amp;\overset{(a)}{=}\frac{\partial }{\partial \boldsymbol{c} } \left(\boldsymbol{x}^T\boldsymbol{x}-2\boldsymbol{x}^T\boldsymbol{Dc}+\boldsymbol{c}^T\boldsymbol{c}\right)\\&amp;=-2\boldsymbol{D}^T\boldsymbol{x}-2\boldsymbol{c}\end{align}其中$(a)$成立，由于$\boldsymbol{D}$是列正交矩阵，因此$\boldsymbol{D}^T\boldsymbol{D}=\mathbf{I}_l$。通过令偏导为0，我们找到函数的驻点$：\boldsymbol{c}^{*}=\boldsymbol{D}^T\boldsymbol{x}$。 给定解压矩阵$\boldsymbol{D}$，我们从损失精度最小的角度出发，得到了压缩数据的表达式$\boldsymbol{c}=\boldsymbol{D}^T\boldsymbol{x}$，对应解压缩数据为$r(\boldsymbol{x})=\boldsymbol{D}\boldsymbol{D}^T\boldsymbol{x}$。为此，我们仍需要确定$\boldsymbol{D}$的形式。这里，我们仍然从损失精度最小的角度出发\begin{align}\boldsymbol{D}^{*}=\underset{\boldsymbol{D} }{\text{arg} \min} ||\boldsymbol{x}-\boldsymbol{DD}^T\boldsymbol{x}||\quad s.t. \quad \boldsymbol{D}^T\boldsymbol{D}=\mathbf{I}_{l}\end{align}显然，我们要从从这个方程中解出$\boldsymbol{D}$来是不可能的。 但是，我们是有$n$个$\boldsymbol{x}^{(i)}$数据的，定义$\boldsymbol{X}\in \mathbb{R}^{m\times n}$，其中$\boldsymbol{X}_{i:}=(\boldsymbol{x}^{(i)})^T$\begin{align}\boldsymbol{D}^*&amp;=\underset{\boldsymbol{D} }{\text{arg} \min }||\boldsymbol{X}^T-\boldsymbol{D}\boldsymbol{D}^T\boldsymbol{X}^T||_F\\&amp;=\underset{\boldsymbol{D} }{\text{arg} \min} ||\boldsymbol{X}-\boldsymbol{X}\boldsymbol{D}\boldsymbol{D}^T||_F\\&amp;=\underset{\boldsymbol{D} }{\text{arg} \min} \text{Tr}\left[{(\boldsymbol{X}-\boldsymbol{X}\boldsymbol{D}\boldsymbol{D}^T)^T(\boldsymbol{X}-\boldsymbol{X}\boldsymbol{D}\boldsymbol{D}^T) }\right]\\&amp;=\underset{\boldsymbol{D} }{\text{arg} \min} \text{Tr}\left({\boldsymbol{X}^T\boldsymbol{X} }\right)-\text{Tr}(\boldsymbol{D}^T\boldsymbol{X}^T\boldsymbol{X}\boldsymbol{D})\\&amp;=\underset{\boldsymbol{D} }{\text{arg} \max} \text{Tr}(\boldsymbol{D}^T\boldsymbol{X}^T\boldsymbol{X}\boldsymbol{D})\end{align} 这里$\boldsymbol{D}$是列正交矩阵，即$\boldsymbol{D}^T\boldsymbol{D}=\mathbf{I}_l$。为使$\text{Tr}(\boldsymbol{D}^T\boldsymbol{X}^T\boldsymbol{X}\boldsymbol{D})$最大，因此选择$\boldsymbol{X}^T\boldsymbol{X}$最大的$l$个特征矢量作为$\boldsymbol{D}$的列向量。对应地，\begin{align}\boldsymbol{X}^T\boldsymbol{X}\left[{\boldsymbol{\xi}_1,\cdots,\boldsymbol{\xi}_l}\right]=\left[{\boldsymbol{\xi}_1,\cdots,\boldsymbol{\xi}_l}\right]\text{Diag}(\lambda_1,\cdots,\lambda_l)\end{align}其中$\boldsymbol{\xi}_i$表示最大的$l$个特征值$\lambda_i\in [l]$对应的特征矢量，$[l]$表示最大的$l$个特征值组成的结合。因此$\max \text{Tr}(\boldsymbol{D}^T\boldsymbol{X}^T\boldsymbol{X}\boldsymbol{D})=\sum_i^l \lambda_i$。 Remarks: 基于上述表述，我们知道矩阵$\boldsymbol{D}$选择数据矩阵$\boldsymbol{X}^T\boldsymbol{X}$的最大$l$个特征值$\lambda_i\in [l]$所对应的特征向量$\boldsymbol{\xi}_i$作为其列，最大程度的保留了矩阵$\boldsymbol{X}^T\boldsymbol{X}$的能量。 这里所得到$\boldsymbol{D}=\left[{\boldsymbol{\xi}_1,\cdots,\boldsymbol{\xi}_l}\right]$并不是驻点，如果$|||\boldsymbol{X}^T-\boldsymbol{D}\boldsymbol{D}^T\boldsymbol{X}^T||_F$对$\boldsymbol{D}$求偏导，并令偏导为零，得到$\boldsymbol{D}=\boldsymbol{0}$，显然这并不是最后的计算结果。]]></content>
      <categories>
        <category>深度学习</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[高斯噪声与中心极限定理]]></title>
    <url>%2F2018%2F10%2F29%2F%E9%AB%98%E6%96%AF%E5%99%AA%E5%A3%B0%E4%B8%8E%E4%B8%AD%E5%BF%83%E6%9E%81%E9%99%90%E5%AE%9A%E7%90%86%2F</url>
    <content type="text"><![CDATA[前言 在众多的信号处理学科领域，噪声一直是衡量算法或系统抗噪声性能的一种指标，笔者是通信专业的学生。对于一个通信系统而言，衡量一个通信系统的质量有两个最重要的指标，一个是有效性，一个是可靠性。有效性的衡量标准是传输带宽，而可靠性的衡量准则是误码率。在误码率的计算中，取决于信噪比和码间串扰等因素。另外，信噪比的定义是信号的能量与噪声的能量的比值。那么如何合理的用数学模型来描述噪声呢？ 在长达四年的本科学习中，笔者发现，通信专业的书中一般假设噪声服从高斯分布（复信号服从循环对称高斯分布，其实部和虚部分别服从高斯分布）。笔者很是不解，为什么噪声是高斯的？记得在“通信原理”课上，当我问老师的时候，老师回答说“中心极限定理”。事实上，很多信号处理领域的学生一直不明白为什么噪声是高斯的，包括很多通信专业的学生。笔者觉得“为什么噪声是高斯的”这个问题是一个很重要的问题，它直接关系到绝大多数的理论的合理性。 实际系统中，由于存在众多噪声源，且大多噪声源（电子噪声，电磁噪声等）满足相互独立假设，当噪声源数量足够多时，且每个噪声源对于总体的贡献可忽略不计，根据中心极限定理可知，这些噪声源的累加的结果服从高斯分布。此篇推导是笔者在考研的时候完成的，现在重新整理与大家分享。由于本人所学知识有限，诚恳地希望读者批评指正。 辛钦大数定律设随机变量$X_1,X_2,\cdots ,X_n$是相互独立同分布的随机变量序列，且具有相同的数学期望$\mathbb{E}[X_i]=\mu,\ (i\in [n])$，作前$n$个随机变量的算数平均值$\frac{1} {n}\sum\nolimits_{i=1}^nX_i$，则$\forall \varepsilon &gt;0$，有\begin{align}\lim\limits_{n\rightarrow \infty} P\left\{ {\left|{\frac{1} {n}\sum\limits_{i=1}^nX_i-\mu}\right|&lt; \varepsilon }\right\}=1\end{align} 证：我们只在随机变量$D(x_i)= { {\sigma }^{2} } \ (i\in [n])$存在，这一条件下证明上述结果。因为\begin{align}\mathbb{E}\left({\frac{1} {n}\sum\limits_{i=1}^nX_i}\right)=\frac{1} {n}\sum\nolimits_{i=1}^n\mathbb{E}[X_i]=\mu\end{align}根据独立性，有\begin{align}D\left({\frac{1} {n}\sum\limits_{i=1}^nX_i}\right)=\frac{1} {n^2}\sum\limits_{i=1}^nD(x_i)=\frac{\sigma^2} {n}\end{align}由切比雪夫不等式【见附录A】，有\begin{align}1-\frac{\sigma^2/n} {\varepsilon^2}\leqP\left\{ {\left|{\frac{1} {n}\sum\limits_{i=1}^nX_i-\mu}\right|&lt; \varepsilon }\right\}\leq1\end{align}当$n\to \infty $时，由夹逼准则，可得\begin{align}\lim\limits_{n\rightarrow \infty} P\left\{ {\left|{\frac{1} {n}\sum\nolimits_{i=1}^nX_i-\mu}\right|&lt; \varepsilon }\right\}=1\end{align} Remarks: 辛钦大数定理所说明的是，当随机变量个数$n\rightarrow \infty$时，这些随机变量的算术平均$\frac{1} {n}\sum\nolimits_{i=1}^nX_i$逐渐趋于概率均值$\mu$。 另一方面，假设$\left\{ {x_i}\right\} (i\in [n])$为随机变量$X$的样本，则当样本个数$n\rightarrow \infty$时，有样本均值趋于统计均值，即$\frac{1} {n}\sum\nolimits_{i=1}^nx_i=\mathbb{E}[X]$。 特征函数大多数情况下，数字特征（均值，方差，各阶距）不能完全确定随机变量的分布（除少数分布，如高斯分布，仅需要一阶矩和二阶矩就可以确定概率分布，详见附录B），我们需要一种与概率分布对应的一种表示，并且相对于概率分布更有利于计算。特征函数就是这样的一种与随机变量对应的表示，既能完全决定随机变量的分布函数，又具有良好的性质。 定义：设$X$为实随机变量，其概率密度为$p_X(x)$，我们称\begin{align}\phi_X(t)=\mathbb{E}[\exp(itX)]=\int e^{itx}p_X(x)\text{d}x\end{align}为随机变量$X$的特征函数（characteristic funciton）这里的$t$是任意实数。 设随机变量$X$的特征函数为$\phi_X(t)$，则存在以下特性： 若随机变量具有相同的特征函数，则它们具有相同的概率分布，即若随机变量$Y$的特征函数$\phi_Y(t)=\phi_X(t)$，则有$p_Y(y)=p_X(x)$。 独立同分布随机变量和的特征函数，等于每个随机变量特征函数的乘积。 设$Z=aX$，则有$\phi_Z(t)=\phi_X(at)$。 Remarks: 从特征函数的定义上可以看出，$X$的特征函数$\phi_X(t)$也是概率密度$p_X(x)$的傅里叶变换的共轭复数。而，傅里叶变换正是一种将信号从时域投影到频域的信号分解技术，其存在的意义，就是将信号转换到频域更有利于相应的处理。因此，不难看出，特征函数与概率密度是对应关系。关于特征函数的这些特性，笔者将在附录B中给出详细证明。 中心极限定理设随机变量$X_1,\cdots ,X_n$相互独立同分布，且具有相同的数学期望和方差，即$\mathbb{E}( { {x}_{i} } )=\mu $，$D( { {x}_{i} } )= { {\sigma }^{2} } $，则随机变量之和的归一化变量\begin{align}Y_n=\frac{\sum\limits_{i=1}^nX_i-\mathbb{E}\left(\sum\limits_{i=1}^nX_i\right)}{\sqrt{D\left(\sum\limits_{i=1}^nX_i\right)} }=\frac{\sum\limits_{i=1}^nX_i-n\mu} {\sqrt{n}\sigma}\end{align}的分布函数$ { {F}_ { { { Y}_{n} } } } (x)$对$\forall x$，满足\begin{align}\lim\limits_{n\rightarrow \infty}F_{Y_n}(x)=\lim\limits_{n\rightarrow \infty}P\left\{ {\frac{\sum\limits_{i=1}^nX_i-n\mu} {\sqrt{n}\sigma}\leq x}\right\}=\int^x_{-\infty}\frac{1} {\sqrt{2\pi} } e^{-t^2/2}\text{d}t=\Phi(x)\end{align}即，$\lim\limits_{n\rightarrow \infty}\frac{\sum\nolimits_{i=1}^nX_i-n\mu} {\sqrt{n}\sigma}\sim \mathcal{N(0,1)}$。 证：$\underline{\text{step 1} } $：设$Z_i=X_i-\mu$，则$Z_i \ (i\in[n])$相互独立，且$\mathbb{E}[Z_i]=0$，$D(Z_i)=\sigma^2$。设$Z_i$的特征函数为$\phi_{Z_i}(t)$，根据特征函数的性质3，随机变量$\frac{1} {\sqrt{n}\sigma}Z_i$的特征函数为$\phi_{Z_i}(\frac{1} {\sqrt{n}\sigma}t)$。而\begin{align}Y_n=\frac{\sum\limits_{i=1}^nX_i-n\mu} {\sqrt{n}\sigma}=\sum\limits_{i=1}^n\left({\frac{Z_i} {\sqrt{n}\sigma} } \right)\end{align}根据，特征函数的性质2，得到$Y_n$的特征函数为$\prod\limits_{i=1}^n\left[{\phi_{Z_i}\left({\frac{1} {\sqrt{n}\sigma }t}\right)}\right]$。 $\underline{\text{step 2} } $：对$\phi_Z(t)$在$t=0$处，进行二阶泰勒展开，有\begin{align}\phi_{Z_i}(t)=\phi_{Z_i}(0)+\phi’_{Z_i}(t)|_{t=0}t+\phi’’_{Z_i}(t)(t)|_{t=0}t^2+o(t^2)\end{align}其中\begin{align}\phi_{Z_i}(0)&amp;=\int_{-\infty}^{+\infty}p_{Z_i}(z)\text{d}z=1\\\phi’_{Z_i}(t)|_{t=0}&amp;=\left[{\int_{-\infty}^{+\infty}jz e^{jtz}p_{Z_i}(z)\text{d}z}\right]_{t=0}=0\\\phi’’_{Z_i}(t)|_{t=0}&amp;=-\left[{\int_{-\infty}^{+\infty}z^2e^{jtz}p_{Z_i}(z)\text{d}z}\right]_{t=0}=-\sigma^2\end{align}故\begin{align}\phi_{Z_i}(t)=1-\frac{\sigma^2} {2}t^2+o(t^2)\end{align}相应地\begin{align}\phi_{Y_n}(t)=\prod\limits_{i=1}^n\left[{\phi_{Z_i}\left({\frac{1} {\sqrt{n}\sigma }t}\right)}\right]=\left[{1-\frac{1} {2n}t^2+o\left({\frac{t^2} {n\sigma^2} } \right)}\right]^n\end{align} $\underline{\text{step 3} } $：\begin{align}\lim\limits_{n\rightarrow \infty} \phi_{Y_n}(t)&amp;=\lim\limits_{n\rightarrow \infty} \left[{1-\frac{1} {2n}t^2+o\left({\frac{t^2} {n\sigma^2} } \right)}\right]^n\\&amp;=\lim\limits_{n\rightarrow \infty} \left({1-\frac{1} {2n}t^2}\right)^n\\&amp;=\lim\limits_{n\rightarrow \infty} \left({1-\frac{1} {2n}t^2}\right)^{\frac{2n} {t^2}\times \frac{t^2} {2} } \\&amp;=e^{-t^2/2}\end{align}其中，最后一个公式成立，根据极限公式$\lim\limits_{x\rightarrow \infty}\left({1+\frac{1} {x} } \right)^x=e$。因此，随机变量$Y_{n}=\lim\limits_{n\rightarrow \infty}\frac{\sum\nolimits_{i=1}^nX_i-n\mu} {\sqrt{n}\sigma}$的特征函数为$\phi_{Y_n}(t)=e^{-t^2/2}$。 $\underline{\text{step 4} } $：又因为标准正态分布的特征函数为$e^{-t^2/2}$【见附录C】，因此有\begin{align}Y_{n}=\lim\limits_{n\rightarrow \infty}\frac{\sum\nolimits_{i=1}^nX_i-n\mu} {\sqrt{n}\sigma}\sim \mathcal{N}(0,1)\end{align} Remarks： 本文所介绍的中心极限定理，是独立同分布的中心极限定理。这里假设$n$个相互独立的随机变量具有相同的均值和方差，因此该中心极限定理的条件相对较强，这中类型的中心极限定理，也称为独立同分布的中心极限定理 若假设$n$个相互独立的变量，具有不同的均值和方差，即$\mathbb{E}[X_i]=\mu_i$，$D(X_i)=\sigma_i^2,(i\in[n])$。该情况为独立同分布的中心极限定理的扩展，称为李亚普诺夫定理。 中心极限定理告诉我们，当相互独立的变量个数足够多，且每个个体对总体的贡献在$n\rightarrow \infty$时，均可忽略不计时，那么这些随机变量的算术平均，服从高斯分布，这也是为什么噪声服从高斯分布，这种假设的合理性解释。 附录A. 切比雪夫不等式设随机变量$X$具有数学期望$\mathbb{E}[X]=\mu$，方差$DX=\sigma^2$，则对于任意的正数$\varepsilon$，有\begin{align}P\left\{ {|X-\mu|\geq \varepsilon}\right\}\leq\frac{\sigma^2} {\varepsilon^2}\end{align} 证：设$X$的概率密度为$p_X(x)$，则有\begin{align}P\left\{ {|X-\mu|\geq \varepsilon}\right\}&amp;\leq \int_{|x-\mu|\geq \varepsilon} \frac{|x-\mu|^2} {\varepsilon^2}p_X(x)\text{d}x\\&amp;\leq \frac{1} {\varepsilon^2}\int_{-\infty}^{\infty} {(x-\mu)^2}p_X(x)\text{d}x\\&amp;=\frac{\sigma^2} {\varepsilon}\end{align} B. 特征函数性质的证明 若随机变量具有相同的特征函数，则它们具有相同的概率分布。证：设随机变量$X$，$Y$具有相同的特征函数，即$\phi_X(t)=\phi_Y(t)$，则有\begin{align}\phi_X(t)=\int_{-\infty}^{+\infty} p_X(x)e^{itx}\text{d}x=\int_{-\infty}^{+\infty} p_Y(y)e^{ity}\text{d}y\quad \Rightarrow \quad p_X(x)=p_Y(y)\end{align}反之，亦成立。 独立同分布随机变量和的特征函数，等于每个随机变量特征函数的乘积。证：设随机变量$X$，$Y$的特征函数分别为$\phi_X(t)$，$\phi_Y(t)$，令$Z=X+Y$，则随机变量$Z$的概率密度，可以由卷积公式得到\begin{align}p_Z(z)=p_X(x)*p_Y(y)=\int_{-\infty}^{+\infty}p_X(x)p_Y(z-x)\text{d}x\end{align}则随机变量$Z$的特征函数为\begin{align}\phi_Z(t)&amp;=\int_{-\infty}^{+\infty} p_Z(z)e^{itz}\text{d}z\\&amp;=\int_{-\infty}^{+\infty} \left({\int_{-\infty}^{+\infty}p_X(x)p_Y(z-x)\text{d}x}\right)e^{itz}\text{d}z\\&amp;=\int_{-\infty}^{+\infty} \left({\int_{-\infty}^{+\infty}p_X(x)p_Y(y)\text{d}x}\right)e^{it(x+y)}\text{d}(x+y)\\&amp;=\left({\int_{-\infty}^{+\infty}p_X(x)e^{jtx}\text{d}x}\right)\left({\int_{-\infty}^{+\infty}p_Y(y)e^{jty}\text{d}y}\right)\\&amp;=\phi_X(t)\phi_Y(t)\end{align} 设$Z=aX$，则有$\phi_Z(t)=\phi_X(at)$。证：设随机变量$X$的概率密度为$p_X(x)$，则随机变量$Z$的累积分布函数（CDF）可以表示为\begin{align}P(Z\leq z)=P\left({X\leq \frac{z} {a} } \right)=\int^{z/a}_{-\infty}p_X(x)\text{d}x\end{align}由于概率密度与累积分布函数互为导数关系，即\begin{align}p_Z(z)&amp;=\frac{\partial P(Z\leq z)} {\partial z}=\frac{\partial } {\partial z}\int^{z/a}_{-\infty}p_X(x)\text{d}x=\frac{1} {a}p_X(z/a)\end{align}因此，随机变量$Z=aX$的特征函数，表示为\begin{align}\phi_Z(t)&amp;=\int_{-\infty}^{+\infty} p_Z(z)e^{itz}\text{d}z\\&amp;=\int_{-\infty}^{+\infty}\frac{1} {a}p_X(x)e^{it(ax)}\text{d}(ax)\\&amp;=\int_{-\infty}^{+\infty}p_X(x)e^{i(at)x}\text{d}x\\&amp;=\phi_X(at)\end{align} C. 高斯分布的特征函数设随机变量$X\sim \mathcal{N}(a,A)$，则其特征函数为\begin{align}\phi_X(t)=e^{ita-\frac{At^2} {2} }\end{align}特别地，当$X\sim \mathcal{N}(0,1)$时，有$\phi_X(t)=e^{-\frac{t^2} {2} } $。证：随机变量$X$的特征函数为\begin{align}\phi_X(t)=\int_{-\infty}^{+\infty} e^{itx} \frac{1} {\sqrt{2\pi A} } \exp\left[-\frac{(x-a)^2} {2A}\right]\text{d}x\end{align}作变量替换$y=\frac{x-\mu} {\sqrt{A} } $，即$x=\sqrt{A}y+\mu$，则\begin{align}\phi_X(t)&amp;=\int_{-\infty}^{+\infty} e^{it(\sqrt{A}y+\mu)} \frac{1} {\sqrt{2\pi A} } \exp\left(-\frac{y^2} {2}\right)\text{d}y\cdot \sqrt{A}\\&amp;=\frac{1} {\sqrt{2\pi} } e^{it\mu} \cdot \int_{-\infty}^{+\infty} e^{it\sqrt{A}y-\frac{y^2} {2} } \text{d}y\\&amp;=\frac{1} {\sqrt{2\pi} } e^{it\mu-\frac{At^2} {2} } \underbrace{\int_{-\infty}^{+\infty}e^{-\frac{(y-it\sqrt{A})^2} {2} } \text{d}y}_{(\text{I})}\\&amp;=e^{it\mu-\frac{At^2} {2} }\end{align}其中，对于$\text{(I)}$的值，我们可以利用概率的归一性进行计算，即\begin{align}\int_{-\infty}^{+\infty} \frac{1} {\sqrt{2\pi} } e^{-(y-a)^2/2}\text{d}y=1\end{align}因此，可以得到\begin{align}\int_{-\infty}^{+\infty} e^{-(y-a)^2/2}\text{d}y=\sqrt{2\pi}\end{align}值得注意的是，$\text{(I)}$中的均值部分为$it\sqrt{A}$，是虚数，但是积分是对实数变量$y$积分，实际上，$\int_{-\infty}^{+\infty}e^{-\frac{(y-it\sqrt{A})^2} {2} } \text{d}y=\int_{-\infty}^{+\infty}e^{-\frac{y^2} {2} } \text{d}y$，具体我们可以由复高斯概率密度得到。]]></content>
      <categories>
        <category>统计信号处理</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[克拉美-罗下界]]></title>
    <url>%2F2018%2F10%2F28%2F%E5%85%8B%E6%8B%89%E7%BE%8E-%E7%BD%97%E4%B8%8B%E7%95%8C%2F</url>
    <content type="text"><![CDATA[估计量的衡量标准对于参数估计问题，目前存在着很多估计算法。那么如何去衡量一个估计器（estimator, 也称估计量或估计算法）的性能，我们主要考量以下三个方面 无偏性(unbiased)。对于参数估计问题，设未知参数$\theta$，估计器模型$\hat{\theta}$。则有$\mathbb{E}[\hat{\theta}]=\theta$。对于估计对象为随机变量，则有$\mathbb{E}[\hat{\theta}]=\mathbb{E}[\theta]$。我们称满足这个条件的估计量为无偏估计量。 有效性(availability)。有效性刻画估计量到真实值的偏离程度，$D(\hat{\theta})=\mathbb{E}[(\hat{\theta}-\mathbb{E}[\hat{\theta}])^2]$，即若存在多种无偏估计器，我们称估计量方差最小的估计器是有效的。 一致性(consistency)。设$\hat{\theta}$为未知参数$\theta$的估计量，若当样本数$N\rightarrow \infty$时，对于任意$\epsilon&gt;0$，有$\lim\limits_{N\rightarrow \infty} P\left\{ {|\hat{\theta}-\theta|&lt;\epsilon}\right\}=1$。我们称$\hat{\theta}$与$\theta$是一致的。一致性所体现的是，当样本总数逐渐增加时，估计量逐渐收敛于真实值。 基于这三点考量，那么很自然我们会问，如何衡量一个无偏估计器是否是有效的。统计信号处理理论中的克拉美罗下界（Cramer-Rao Lower Bound，CRLB）就是衡量一个无偏估计器的有力工具。 克拉美-罗下界（Scale Parameter 标量参数）对于估计参数$\theta$为标量时，假定PDF满足“正则”条件\begin{align}\mathbb{E}\left[{\frac{\partial \ln p(\boldsymbol{x};\theta)}{\partial \theta} }\right]=0\quad (\ \text{for any }\theta \ )\end{align}其中数学期望对$p(\boldsymbol{x};\theta)$取。那么无偏估计量$\hat{\theta}$的方差必然满足\begin{align}D(\hat{\theta}) \geq \frac{1}{-\mathbb{E}\left[{ \frac{\partial ^2\ln p(\boldsymbol{x};\theta)}{\partial \theta^2} }\right]}=\frac{1}{\mathbb{E}\left[{ \left(\frac{\partial \ln p(\boldsymbol{x};\theta)}{\partial \theta}\right)^2}\right]}\end{align}其中导数是在$\theta$的真实值处求，数学期望是对$p(\boldsymbol{x};\theta)$取。因此，我们可以说一个无偏估计量$g(\boldsymbol{x})$达到CRLB，当且仅当，该估计量满足\begin{align}\frac{\partial \ln p(\boldsymbol{x};\theta)}{\partial \theta} =\mathbf{I}(\theta)(g(\boldsymbol{x})-\theta)\end{align}其中，$\mathbf{I}(\theta)=-\mathbb{E}\left[{ \frac{\partial ^2\ln p(\boldsymbol{x};\theta)}{\partial \theta^2} }\right]$，称为Fisher information。证明见博客第四部分。 Remarks: CRLB是衡量一个无偏估计器是否有效的重要工具，也就是说，给定一个无偏估计器，我们可以利用克拉美-罗下界去判断这个估计器是否是最优的。 Example：线性高斯模型（Linear Gaussian model）\begin{align}\boldsymbol{x}=\boldsymbol{h}\theta+\boldsymbol{w}, \quad \boldsymbol{w}\sim \mathcal{N}(\boldsymbol{0},\boldsymbol{C}_{\boldsymbol{w} })\end{align}其中$\theta$是未知参数，$\boldsymbol{x}\in \mathbb{R}^p$是观测值（observed signal），$\boldsymbol{w}$是均值为$\boldsymbol{0}$，协方差矩阵为$\boldsymbol{C}_{\boldsymbol{w} }$的高斯噪声。 我们考虑如下估计器\begin{align}\hat{\theta}=(\boldsymbol{h}^T\boldsymbol{C}_{\boldsymbol{w} }^{-1}\boldsymbol{h})^{-1}\boldsymbol{h}^T\boldsymbol{C}_{\boldsymbol{w} }^{-1}\boldsymbol{x}\end{align}对于该模型，其似然函数$p(\boldsymbol{x};\theta)$为\begin{align}p(\boldsymbol{x};\theta)=\frac{1}{(2\pi)^{p/2}|\boldsymbol{C}_{\boldsymbol{w} }|^{1/2} } \exp \left[{-\frac{1}{2}(\boldsymbol{x}-\boldsymbol{h}\theta)^T\boldsymbol{C}_{\boldsymbol{w} }^{-1}(\boldsymbol{x}-\boldsymbol{h}\theta)}\right]\end{align}因此 无偏性\begin{align}\mathbb{E}[\hat{\theta}]=\int_{\boldsymbol{x} } \hat{\theta} p(\boldsymbol{x};\theta)\text{d}\boldsymbol{x}\end{align}我们可以将$p(\boldsymbol{x};\theta)$看作为自变量为$\boldsymbol{x}$均值为$\boldsymbol{h}\theta$，协方差矩阵为$\boldsymbol{C}_{\boldsymbol{w} }$的高斯PDF，即$\int_{\boldsymbol{x} }\boldsymbol{x}p(\boldsymbol{x};\theta)\text{d}\boldsymbol{x}=\boldsymbol{h}\theta$。因此$\mathbb{E}[\hat{\theta}]=(\boldsymbol{h}^T\boldsymbol{C}_{\boldsymbol{w} }^{-1}\boldsymbol{h})^{-1}\boldsymbol{h}^T\boldsymbol{C}_{\boldsymbol{w} }^{-1}\boldsymbol{h}\theta=\theta$，即$\hat{\theta}$为无偏估计量。 有效性\begin{align}\frac{\partial \ln p(\boldsymbol{x};\theta)}{\partial \theta}&amp;=(\boldsymbol{x}-\boldsymbol{h}\theta)^T\boldsymbol{C}_{\boldsymbol{w} }^{-1}\boldsymbol{h}\\\frac{\partial^2 \ln p(\boldsymbol{x};\theta)}{\partial \theta^2}&amp;=-\boldsymbol{h}^T\boldsymbol{C}_{\boldsymbol{w} }^{-1}\boldsymbol{h}\end{align}关于矩阵求导不太熟悉的朋友可以看下这个网站：https://en.wikipedia.org/wiki/Matrix_calculus。基于上述表述，该系统模型的CRLB为\begin{align}-\frac{1}{-\mathbb{E}\left[{ \frac{\partial ^2\ln p(\boldsymbol{x};\theta)}{\partial \theta^2} }\right]}=\frac{1}{\boldsymbol{h}^T\boldsymbol{C}_{\boldsymbol{w} }^{-1}\boldsymbol{h} }\end{align}而估计器$\hat{\theta}$的方差为\begin{align}D(\hat{\theta})=\left({(\boldsymbol{h}^T\boldsymbol{C}_{\boldsymbol{w} }^{-1}\boldsymbol{h})^{-1}\boldsymbol{h}^T\boldsymbol{C}_{\boldsymbol{w} }^{-1} }\right) \boldsymbol{C}_{\boldsymbol{w} } \left({(\boldsymbol{h}^T\boldsymbol{C}_{\boldsymbol{w} }^{-1}\boldsymbol{h})^{-1}\boldsymbol{h}^T\boldsymbol{C}_{\boldsymbol{w} }^{-1} }\right)^T=(\boldsymbol{h}^T\boldsymbol{C}_{\boldsymbol{w} }^{-1}\boldsymbol{h})^{-1}\end{align}由于$\boldsymbol{h}^T\boldsymbol{C}_{\boldsymbol{w} }^{-1}\boldsymbol{h}$是一维的，有$(\boldsymbol{h}^T\boldsymbol{C}_{\boldsymbol{w} }^{-1}\boldsymbol{h})^{-1}=\frac{1}{\boldsymbol{h}^T\boldsymbol{C}_{\boldsymbol{w} }^{-1}\boldsymbol{h} }$，因此，该估计量是有效的，即该无偏估计量$\theta$的方差可以达到CRLB。 一致性将系统模型$\boldsymbol{x}=\boldsymbol{h}\theta+\boldsymbol{w}$代入估计器中，有\begin{align}\hat{\theta}&amp;=(\boldsymbol{h}^T\boldsymbol{C}_{\boldsymbol{w} }^{-1}\boldsymbol{h})^{-1}\boldsymbol{h}^T\boldsymbol{C}_{\boldsymbol{w} }^{-1}(\boldsymbol{h}\theta+\boldsymbol{w})\\&amp;=\theta+(\boldsymbol{h}^T\boldsymbol{C}_{\boldsymbol{w} }^{-1}\boldsymbol{h})^{-1}\boldsymbol{h}^T\boldsymbol{C}_{\boldsymbol{w} }^{-1}\boldsymbol{w}\end{align}若假设噪声能量一定，即$\boldsymbol{C}_{\boldsymbol{w} }$元素值固定，随着观测样本$p\rightarrow\infty$，则噪声的方差\begin{align}D((\boldsymbol{h}^T\boldsymbol{C}_{\boldsymbol{w} }^{-1}\boldsymbol{h})^{-1}\boldsymbol{h}^T\boldsymbol{C}_{\boldsymbol{w} }^{-1}\boldsymbol{w})=\frac{1}{\boldsymbol{h}^T\boldsymbol{c}_{\boldsymbol{w} }^{-1}\boldsymbol{h} }\end{align}从公式可以看出，假设噪声$\boldsymbol{w}$的每个元素具有相同的方差，则必然$\lim\limits_{p\rightarrow \infty}\boldsymbol{h}^T\boldsymbol{c}_{\boldsymbol{w} }^{-1}\boldsymbol{h}\rightarrow \infty$。因此，当$p\rightarrow \infty$时，我们可以将估计量$\hat{\theta}$看作\begin{align}\hat{\theta}=\theta+n,\quad n\sim\mathcal{N}(0,(\boldsymbol{h}^T\boldsymbol{C}_\boldsymbol{w}^{-1}\boldsymbol{h})^{-1}) \ \ \text{and} \ \lim\limits_{p\rightarrow \infty}\boldsymbol{h}^T\boldsymbol{c}_{\boldsymbol{w} }^{-1}\boldsymbol{h}\rightarrow \infty\end{align}因此，对于任意$\epsilon&gt;0$，有\begin{align}\lim\limits_{N\rightarrow \infty} P\left\{ {|\hat{\theta}-\theta|&lt;\epsilon}\right\}=1\end{align}即，该估计量满足一致性。 CRLB证明由于$\hat{\theta}$是无偏估计，即\begin{align}&amp;\int_{\boldsymbol{x} } (\hat{\theta}-\theta)p(\boldsymbol{x};\theta)\text{d}\boldsymbol{x}=0\\\end{align}等式两边对$\theta$求偏导有\begin{align}&amp;\int (\hat{\theta}-\theta)\frac{\partial p(\boldsymbol{x};\theta)}{\partial \theta}\text{d}\boldsymbol{x}=1\\\Rightarrow&amp; \int (\hat{\theta}-\theta)\frac{\partial \ln p(\boldsymbol{x};\theta)}{\partial \theta}p(\boldsymbol{x};\theta)\text{d}\boldsymbol{x}=1\\\Rightarrow &amp; \int (\hat{\theta}-\theta)\sqrt{p(\boldsymbol{x};\theta)}\frac{\partial \ln p(\boldsymbol{x};\theta)}{\partial \theta}\sqrt{p(\boldsymbol{x};\theta)}\text{d}\boldsymbol{x}=1\end{align}由于柯西-施瓦茨不等式\begin{align}\int f^2(x)\text{d}x \int g^2(x)\text{d}x \geq\left({\int f(x)g(x)\text{d}x}\right)^2\end{align}当且仅当$f(x)=g(x)$时，取等号。 根据柯西-施瓦茨不等式（Cauchy-Schwarz inequality），有\begin{align}&amp;\left({ \int (\hat{\theta}-\theta)^2{p(\boldsymbol{x};\theta)}\text{d}\boldsymbol{x} }\right)\left({\int \left(\frac{\partial \ln p(\boldsymbol{x};\theta)}{\partial \theta}\right)^2p(\boldsymbol{x};\theta)\text{d}\boldsymbol{x} }\right)\geq 1\\\Rightarrow &amp;\int (\hat{\theta}-\theta)^2{p(\boldsymbol{x};\theta)}\text{d}\boldsymbol{x}\geq \frac{1}{\left({\int \left(\frac{\partial \ln p(\boldsymbol{x};\theta)}{\partial \theta}\right)^2p(\boldsymbol{x};\theta)\text{d}\boldsymbol{x} }\right)}\end{align}即\begin{align}D(\hat{\theta})\geq \frac{1}{\mathbb{E}\left[{\left(\frac{\partial \ln p(\boldsymbol{x};\theta)}{\partial \theta}\right)^2}\right]}\end{align}现在只需证明\begin{align}\mathbb{E}\left[{\left(\frac{\partial \ln p(\boldsymbol{x};\theta)}{\partial \theta}\right)^2}\right]=-\mathbb{E}\left[{ \frac{\partial ^2\ln p(\boldsymbol{x};\theta)}{\partial \theta^2} }\right]\end{align}证：由正则条件$\mathbb{E}\left[{\frac{\partial \ln p(\boldsymbol{x};\theta)}{\partial \theta} }\right]=0$，等式两边对$\theta$求偏导，有\begin{align}&amp;\frac{\partial }{\partial \theta} \int \frac{\partial \ln p(\boldsymbol{x};\theta)}{\partial \theta} p(\boldsymbol{x};\theta)\text{d}\boldsymbol{x}=0\\\Rightarrow&amp; \int \left[{\frac{\partial^2 \ln p(\boldsymbol{x};\theta)}{\partial \theta^2}p(\boldsymbol{x};\theta)+\frac{\partial \ln p(\boldsymbol{x};\theta)}{\partial \theta}\frac{\partial p(\boldsymbol{x};\theta)}{\partial \theta} }\right]\text{d}\boldsymbol{x}=0\\\Rightarrow &amp; \int \frac{\partial^2 \ln p(\boldsymbol{x};\theta)}{\partial \theta^2}p(\boldsymbol{x};\theta)\text{d}\boldsymbol{x}=-\int \left({\frac{\partial \ln p(\boldsymbol{x};\theta)}{\partial \theta} }\right)^2p(\boldsymbol{x};\theta)\text{d}\boldsymbol{x}\end{align} 现在证明，若估计量$\hat{\theta}=\text{g}(\boldsymbol{x})$可以达到CRLB，则有\begin{align}\frac{\partial \ln p(\boldsymbol{x};\theta)}{\partial \theta} =\mathbf{I}(\theta)(g(\boldsymbol{x})-\theta)\end{align}其中，$\mathbf{I}(\theta)=-\mathbb{E}\left[{ \frac{\partial ^2\ln p(\boldsymbol{x};\theta)}{\partial \theta^2} }\right]$。 证：等式两边同时对$\theta$求偏导，有\begin{align}\frac{\partial^2 \ln p(\boldsymbol{x};\theta)}{\partial \theta^2} =\frac{\partial \mathbf{I}(\theta)}{\partial \theta}(g(\boldsymbol{x})-\theta)-\mathbf{I}(\theta)\end{align}等式两边同时对乘上$p(\boldsymbol{x};\theta)$，并对$\boldsymbol{x}$积分，得\begin{align}\mathbb{E}\left[{\frac{\partial^2 \ln p(\boldsymbol{x};\theta)}{\partial \theta^2} }\right]=-\mathbf{I}(\theta)\end{align}证毕。]]></content>
      <categories>
        <category>统计信号处理</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[开通个人blog]]></title>
    <url>%2F2018%2F10%2F27%2F%E5%BC%80%E9%80%9A%E4%B8%AA%E4%BA%BAblog%2F</url>
    <content type="text"><![CDATA[学习的方法方式有很多种，做笔记是其中一种比较好的选择。我本人从大学本科阶段就有写一些学习笔记的习惯。从一开始是用word来记录，那时候还不会latex，所以用的是mathtype。这个工具比较容易上手，所见即所得嘛。那时候写写笔记，装装逼发给别人看看也确实会满足个人虚荣心，不过却实有帮助一些人。 到了研究生阶段，开始写blog。这一点主要是因为这时候我已经掌握了latex的代码编写规则。此外，刚好那时候，国内开始有一些博客论坛开始支持latex公式，所以在研一的时候，我开通了一个CSDN账号。CSDN那个账号到现在浏览量不是很多，但是所写的博客却也是我在学习过程中的积累。CSDN博客，我一直在用。不过近段时间，CSDN博客改版了。有一些功能，在下实在不敢苟同，比如：不支持公式组环境以及公式没有之前版本美观等。由于我所研究的领域，较多的是公式推导等工作。公式写下来，通常一大堆，多行公式也是家常便饭。无奈之下，我便心生开通个人博客的想法。经过一番努力，所创建的博客网站及其功能也大致能满足在下之需求。今后我会尽可能地利用到这个博客网站记录工作、学习、生活的点滴。O(∩_∩)O]]></content>
      <categories>
        <category>随笔</category>
      </categories>
  </entry>
</search>
