<?xml version="1.0" encoding="utf-8"?>
<search>
  <entry>
    <title><![CDATA[线性高斯模型的估计方法]]></title>
    <url>%2F2018%2F11%2F21%2F%E7%BA%BF%E6%80%A7%E9%AB%98%E6%96%AF%E6%A8%A1%E5%9E%8B%E7%9A%84%E4%BC%B0%E8%AE%A1%E6%96%B9%E6%B3%95%2F</url>
    <content type="text"><![CDATA[对于线性高斯模型\begin{align}\boldsymbol{y}=\boldsymbol{Hx}+\boldsymbol{w}\end{align}其中$\boldsymbol{x}\in \mathbb{R}^N$为待估计变量，其概率密度为$p(\boldsymbol{x})$。$\boldsymbol{w}$是高斯白噪声，即$\boldsymbol{w}\sim \mathcal{N}(\boldsymbol{w}|\boldsymbol{a},\boldsymbol{C}_{\boldsymbol{w} })$。信号估计的目标是根据已知的模型信息，从观测向量$\boldsymbol{y}\in \mathbb{R}^M$中恢复出原始信号$\boldsymbol{x}$。为了得到确定解，一般$\boldsymbol{y}$的维度大于$\boldsymbol{x}$的维度，即模型为超定方程组。 最小二乘法 （Least Square, LS）$\boldsymbol{x}$的最小二乘估计，通过最小化如下损失函数得到\begin{align}J=||\boldsymbol{y}-\boldsymbol{Hx}||^2\end{align}由于该损失函数是凸函数，因此我们通过计算损失函数对$\boldsymbol{x}$的导数\begin{align}\frac{\partial J}{\partial \boldsymbol{x} }=-2\boldsymbol{H}^T\boldsymbol{y}+2\boldsymbol{H}^T\boldsymbol{H}\boldsymbol{x}\end{align}并令导数为零，得到该模型的最小二乘估计\begin{align}\hat{\boldsymbol{x} }_{\text{LS} }=(\boldsymbol{H}^T\boldsymbol{H})^{-1}\boldsymbol{H}^T\boldsymbol{y}\end{align} 几何解释: 如图所示，由于$\boldsymbol{H}$所构成的超平面用$\mathcal{C}$表示，最小化$J=||\boldsymbol{y}-\boldsymbol{Hx}||^2$所描述的是，找到$\boldsymbol{y}$在超平面$\mathcal{C}$上的正交投影。 Remarks: 最小二乘的优势在于算法结构简单，其缺陷在于，由于忽略了噪声的存在，因此当噪声很大的时候，其估计性能极差。 最大似然估计（Maximum likelihood, ML）似然函数的定义（摘自Wiki Pedia）： In frequentist inference, a likelihood function (often simply the likelihood) is a function of the parameters of a statistical model, given specific observed data. Likelihood functions play a key role in frequentist inference, especially methods of estimating a parameter from a set of statistics. In informal contexts, “likelihood” is often used as a synonym for “probability”. In mathematical statistics, the two terms have different meanings. Probability in this mathematical context describes the plausibility of a random outcome, given a model parameter value, without reference to any observed data. Likelihood describes the plausibility of a model parameter value, given specific observed data.在概率推论中，一个似然函数（简称似然）是给定明确的观测数据，关于一个统计模型的参数的函数。似然函数在概率推论中扮演着重要的角色，尤其是从一组统计数据中估计参数。在非正式的文献中，似然函数通常被认为是“概率”。在统计数学中，这两者有不同的含义。在数学文献中，概率描述的是给定模型参数值下一个随机输出的可能性，没有参考任何观测数据。似然函数描述的是给定具体观测数据，模型参数值得可能性。 Following Bayes’ Rule, the likelihood when seen as a conditional density can be multiplied by the prior probability density of the parameter and then normalized, to give a posterior probability density.根据贝叶斯公式，似然函数被看作是条件概率，可以乘上先验概率然后归一化得到后验概率。 对于线性高斯模型$\boldsymbol{y}=\boldsymbol{Hx}+\boldsymbol{w}$，为了方便计算，这里我们设$\boldsymbol{w}\sim \mathcal{N}(\boldsymbol{0},\sigma^2\mathbf{I})$，则该模型的其似然函数为\begin{align}L(\boldsymbol{x})&amp;=p(\boldsymbol{y}|\boldsymbol{x})=\mathcal{N}(\boldsymbol{y}|\boldsymbol{Hx},\sigma^2\mathbf{I})\\&amp;=(2\pi\sigma^2)^{-\frac{M}{2} }\exp \left(-\frac{1}{2\sigma^2}(\boldsymbol{y}-\boldsymbol{Hx})^T(\boldsymbol{y}-\boldsymbol{Hx})\right)\end{align}等式两边取对数，有\begin{align}\ell(\boldsymbol{x})=\ln L(\boldsymbol{x})=-\frac{1}{2\sigma^2}(\boldsymbol{y}-\boldsymbol{Hx})^T(\boldsymbol{y}-\boldsymbol{Hx})-\frac{M}{2}\ln (2\pi\sigma^2)\end{align}计算对数似然函数关于$\boldsymbol{x}$的偏导数，有\begin{align}\frac{\partial \ell(\boldsymbol{x})}{\partial \boldsymbol{x} }=-\frac{1}{2\sigma^2}(2\boldsymbol{H}^T\boldsymbol{y}-2\boldsymbol{H}^T\boldsymbol{H}\boldsymbol{x})=0 \ \Rightarrow \hat{\boldsymbol{x} }_{\text{ML} }=(\boldsymbol{H}^T\boldsymbol{H})^{-1}\boldsymbol{H}^T\boldsymbol{y}\end{align}因此，我们发现，线性高斯模型的最大似然解和最小二乘解一致。 最小均方误差估计（Minimum mean square error, MMSE）定义如下贝叶斯均方误差（Bayesian mean square error, Bmse）\begin{align}\text{Bmse}(\hat{\boldsymbol{x} })=\mathbb{E}\left\{||\boldsymbol{x}-\hat{\boldsymbol{x} }||^2\right\}=\int ||\boldsymbol{x}-\hat{\boldsymbol{x} }||^2p(\boldsymbol{x},\boldsymbol{y})\text{d}\boldsymbol{x}\text{d}\boldsymbol{y}\end{align}最小均方误差估计量，即寻找使得贝叶斯均方误差最小的$\boldsymbol{x}$\begin{align}\hat{\boldsymbol{x} }&amp;=\underset{\boldsymbol{x} }{\arg \min} \int \left[\int ||\boldsymbol{x}-\hat{\boldsymbol{x} }||^2p(\boldsymbol{x}|\boldsymbol{y})\text{d}\boldsymbol{x}\right]p(\boldsymbol{y})\text{d}\boldsymbol{y}\\&amp;=\underset{\boldsymbol{x} }{\arg \min}\int ||\boldsymbol{x}-\hat{\boldsymbol{x} }||^2p(\boldsymbol{x}|\boldsymbol{y})\text{d}\boldsymbol{x}\end{align}计算其导数\begin{align}\frac{\partial }{\partial \boldsymbol{x} }\int ||\boldsymbol{x}-\hat{\boldsymbol{x} }||^2p(\boldsymbol{x}|\boldsymbol{y})\text{d}\boldsymbol{x}&amp;=2\int (\boldsymbol{x}-\hat{\boldsymbol{x} })p(\boldsymbol{x}|\boldsymbol{y})\text{d}\boldsymbol{x}\\&amp;=2\int \boldsymbol{x}p(\boldsymbol{x}|\boldsymbol{y})\text{d}\boldsymbol{x}-2\hat{\boldsymbol{x} }\end{align}注意$\hat{\boldsymbol{x} }$是关于$\boldsymbol{y}$的函数。令导数为0，有\begin{align}\hat{\boldsymbol{x} }_{\text{MMSE} }=\int \boldsymbol{x} p(\boldsymbol{x}|\boldsymbol{y})\text{d}\boldsymbol{x}=\mathbb{E}\left[\boldsymbol{x}|\boldsymbol{y}\right]\end{align} Remarks: 最小均方误差估计器，被称为后验均值估计，也就是选取后验概率的均值作为$\boldsymbol{x}$的估计值。因此，最小均方误差估计器最为核心之处，在于计算后验概率$p(\boldsymbol{x}|\boldsymbol{y})$。根据贝叶斯公式\begin{align}p(\boldsymbol{x}|\boldsymbol{y})=\frac{p(\boldsymbol{x},\boldsymbol{y})}{p(\boldsymbol{y})}=\frac{p(\boldsymbol{y}|\boldsymbol{x})p(\boldsymbol{x})}{p(\boldsymbol{y})}\end{align}这里我们仅需要求$p(\boldsymbol{y}|\boldsymbol{x})p(\boldsymbol{x})$，而$p(\boldsymbol{y})$可以通过归一化来实现。\begin{align}\hat{\boldsymbol{x} }=\left[\begin{matrix}\hat{x}_1\\\vdots\\\hat{x}_N\end{matrix}\right]=\left[\begin{matrix}\int x_1p(x_1|\boldsymbol{y})\text{d}x_1\\\vdots\\\int x_1p(x_N|\boldsymbol{y})\text{d}x_N\end{matrix}\right]\end{align}因此，我们可以知道，最小均方误差真正的难点在于，求边缘后验概率\begin{align}p(x_i|\boldsymbol{y})=\int_{\boldsymbol{x}_{\backslash i} } p(\boldsymbol{x}|\boldsymbol{y})\text{d}\boldsymbol{x}_{\backslash i}\end{align}其中$\boldsymbol{x}_{\backslash i}$表示除了第$i$个元素外，$\boldsymbol{x}$中其余元素所构成的向量。 最小均方误差估计器是贝叶斯最优的，因为，最小均方误差估计器选取使得贝叶斯均方误差最小的$\boldsymbol{x}$作为估计器。 当先验概率是高斯的时候，根据高斯相乘引理，我们可以写出线性高斯模型的MMSE估计器的解析表达式。 通常先验概率是非高斯的，此时，我们不能写出MMSE估计器的解析表达式。一种方法是，退而求其次，通过限制待估计量与观测值呈线性关系，即LMMSE估计器；另一种方法是通过因子图的角度出发，利用近似消息传递（approximate message passing, AMP）[1][2]类算法或者期望传播（Expectation propagation, EP）[3]类算法，来迭代得到估计量的MMSE解。注意，不管是AMP族算法还是EP族算法，其本质上是计算边缘后验概率。 线性最小均方误差估计 (Linear minmum mean square error, LMMSE)线性最小均方误差估计，通过假设估计器的模型为$\boldsymbol{y}$的线性模型，并使得贝叶斯均方误差最小，来得到估计器的表达式\begin{align}\hat{\boldsymbol{x} }=\boldsymbol{A}\boldsymbol{y}+\boldsymbol{b}\end{align}为了得到$\boldsymbol{x}$的表达式，我们需要进一步确定$\boldsymbol{A}$和$\boldsymbol{b}$。定义如下贝叶斯均方误差（Bayesian mean square error, BMSE）\begin{align}\text{Bmse}(\hat{\boldsymbol{x} })=\mathbb{E}\left\{||\boldsymbol{x}-\hat{\boldsymbol{x} }||^2\right\}\end{align}这里的期望是对联合概率$p(\boldsymbol{x},\boldsymbol{y})$求。 $\underline{\text{Step 1} }$：为求$\hat{\boldsymbol{x} }=[\hat{x}_1,\cdots,\hat{x}_N]^T$，我们首先考虑一维的情况，即\begin{align}\hat{x}=\boldsymbol{a}^T\boldsymbol{y}+b\end{align}其对应的贝叶斯均方误差为\begin{align}\text{Bmse}(\hat{x })=\mathbb{E}\left\{(x-\hat{x})^2\right\}\end{align}其中期望对$p(x,\boldsymbol{y})$取。 $\underline{\text{Step 2} }$： 求$b$。计算贝叶斯均方误差对$b$的偏导，有\begin{align}\frac{\partial }{\partial b}\mathbb{E}\left\{(x-\boldsymbol{a}^T\boldsymbol{y}-b)^2\right\}=-2\mathbb{E}\left\{x-\boldsymbol{a}^T\boldsymbol{y}-b\right\}\end{align}令偏导为0，得到\begin{align}b=\mathbb{E}[x]-\boldsymbol{a}^T\mathbb{E}[\boldsymbol{y}]\end{align}$\underline{\text{Step 3} }$：计算$\boldsymbol{a}$。计算贝叶斯均方误差如下\begin{align}\text{Bmse}(\hat{x})&amp;=\mathbb{E}\left\{(x-\boldsymbol{a}^T\boldsymbol{y}-\mathbb{E}[x]+\boldsymbol{a}^T\mathbb{E}[\boldsymbol{y}])^2\right\}\\&amp;=\mathbb{E}\left\{\left[\boldsymbol{a}^T(\boldsymbol{y}-\mathbb{E}[\boldsymbol{y}])-(x-\mathbb{E}[x])\right]^2\right\}\\&amp;=\mathbb{E}\left\{\boldsymbol{a}^T(\boldsymbol{y}-\mathbb{E}[\boldsymbol{y}])(\boldsymbol{y}-\mathbb{E}[\boldsymbol{y}])^T\boldsymbol{a}\right\}-\mathbb{E}\left\{\boldsymbol{a}^T(\boldsymbol{y}-\mathbb{E}[\boldsymbol{y}])(x-\mathbb{E}[x])\right\}\\&amp;\quad -\mathbb{E}\left\{(x-\mathbb{E}[x])(\boldsymbol{y}-\mathbb{E}[\boldsymbol{y}])^T\boldsymbol{a}\right\}+\mathbb{E}\left\{(x-\mathbb{E}[x])^2\right\}\\&amp;=\boldsymbol{a}^T\boldsymbol{C}_{\boldsymbol{yy} }\boldsymbol{a}-\boldsymbol{a}^T\boldsymbol{C}_{\boldsymbol{y}x}-\boldsymbol{C}_{x\boldsymbol{y} }\boldsymbol{a}+C_{xx}\end{align}其中$\boldsymbol{C}_{\boldsymbol{yy} }$是$\boldsymbol{y}$的协方差矩阵，$\boldsymbol{C}_{x\boldsymbol{y} }$是$1\times N$的互协方差矢量，且$\boldsymbol{C}_{x\boldsymbol{y} }=\boldsymbol{C}_{\boldsymbol{y}x}^T$。$C_{xx}$是$x$的方差。计算贝叶斯均方误差对$\boldsymbol{a}$的偏导，并令偏导为0，有\begin{align}\frac{\partial \text{Bmse}(\hat{\boldsymbol{x} })}{\partial \boldsymbol{a} }=2\boldsymbol{C}_{\boldsymbol{yy} }\boldsymbol{a}-2\boldsymbol{C}_{\boldsymbol{y}x}=0 \quad \Rightarrow \boldsymbol{a}=C_{\boldsymbol{yy} }^{-1}\boldsymbol{C}_{\boldsymbol{y}x}\end{align}因此，得到\begin{align}\hat{x}&amp;=\boldsymbol{C}_{x\boldsymbol{y} }\boldsymbol{C}_{\boldsymbol{yy} }^{-1}\boldsymbol{y}+\mathbb{E}[x]-\boldsymbol{C}_{x\boldsymbol{y} }\boldsymbol{C}_{\boldsymbol{yy} }^{-1}\mathbb{E}[\boldsymbol{y}]\\&amp;=\boldsymbol{C}_{x\boldsymbol{y} }\boldsymbol{C}_{\boldsymbol{yy} }^{-1}(\boldsymbol{y}-\mathbb{E}[\boldsymbol{y}])+\mathbb{E}[x]\end{align} $\underline{\text{Step 4} }$：扩展到矢量$\hat{\boldsymbol{x} }$。\begin{align}\hat{\boldsymbol{x} }&amp;=\left[\begin{matrix}\mathbb{E}[x_1]\\\mathbb{E}[x_2]\\\vdots\\\mathbb{E}[x_N]\\\end{matrix}\right]+\left[\begin{matrix}\boldsymbol{C}_{x_1\boldsymbol{y} }\boldsymbol{C}_{\boldsymbol{yy} }^{-1}(\boldsymbol{y}-\mathbb{E}[\boldsymbol{y}])\\\boldsymbol{C}_{x_2\boldsymbol{y} }\boldsymbol{C}_{\boldsymbol{yy} }^{-1}(\boldsymbol{y}-\mathbb{E}[\boldsymbol{y}])\\\vdots\\\boldsymbol{C}_{x_N\boldsymbol{y} }\boldsymbol{C}_{\boldsymbol{yy} }^{-1}(\boldsymbol{y}-\mathbb{E}[\boldsymbol{y}])\\\end{matrix}\right]\\&amp;=\mathbb{E}[\boldsymbol{x}]+\boldsymbol{C}_{\boldsymbol{xy} }\boldsymbol{C}_{\boldsymbol{yy} }^{-1}(\boldsymbol{y}-\mathbb{E}[\boldsymbol{y}])\end{align}其中\begin{align}\boldsymbol{C}_{\boldsymbol{yy} }&amp;=\boldsymbol{H}\boldsymbol{C}_{\boldsymbol{xx} }\boldsymbol{H}^T+\boldsymbol{C}_{\boldsymbol{w} }\\\boldsymbol{C}_{\boldsymbol{xy} }&amp;=\boldsymbol{C}_{\boldsymbol{xx} }\boldsymbol{H}^T\end{align}因此\begin{align}\hat{\boldsymbol{x} }_{\text{LMMSE} }&amp;=\mathbb{E}[\boldsymbol{x}]+\boldsymbol{C}_{\boldsymbol{xx} }\boldsymbol{H}^T(\boldsymbol{H}\boldsymbol{C}_{\boldsymbol{xx} }\boldsymbol{H}^T+\boldsymbol{C}_{\boldsymbol{w} })^{-1}(\boldsymbol{y}-\boldsymbol{H}\mathbb{E}[\boldsymbol{x}])\\&amp;=\mathbb{E}[\boldsymbol{x}]+(\boldsymbol{C}_{\boldsymbol{xx} }^{-1}+\boldsymbol{H}^T\boldsymbol{C}_{\boldsymbol{w} }^{-1}\boldsymbol{H})^{-1}\boldsymbol{H}^T\boldsymbol{C}_{\boldsymbol{w} }^{-1}(\boldsymbol{y}-\boldsymbol{H}\mathbb{E}[\boldsymbol{x}])\end{align} Remarks: 通常我们所遇到的模型中，经过功率归一化后，$\boldsymbol{x}$的均值为0，方差为1，以及噪声方差为$\sigma^2$。因此，进一步将其LMMSE估计器简化为\begin{align}\hat{\boldsymbol{x} }_{\text{LMMSE} }=(\boldsymbol{H}^T\boldsymbol{H}+\sigma^2\mathbf{I})^{-1}\boldsymbol{H}^T\boldsymbol{y}\end{align}我们可以看到，相对于LS而言 $\left(\hat{\boldsymbol{x} }=(\boldsymbol{H}^T\boldsymbol{H})^{-1}\boldsymbol{H}^T\boldsymbol{y}\right)$，LMMSE加入了噪声修正项$\sigma^2\mathbf{I}$。 对于简化后的LMMSE估计器模型$\hat{\boldsymbol{x} }=(\boldsymbol{H}^T\boldsymbol{H}+\sigma^2\mathbf{I})^{-1}\boldsymbol{H}^T\boldsymbol{y}$，我们可以将其视为，假设$\boldsymbol{x}\sim \mathcal{N}(\boldsymbol{x}|\boldsymbol{0},\mathbf{I})$的MMSE结果。证明如下\begin{align}p(\boldsymbol{x}|\boldsymbol{y})&amp;=\frac{p(\boldsymbol{x})p(\boldsymbol{y}|\boldsymbol{x})}{p(\boldsymbol{y})}\\&amp;\propto p(\boldsymbol{x})p(\boldsymbol{y}|\boldsymbol{x})\end{align}根据高斯相乘引理：\begin{align}p(\boldsymbol{x})p(\boldsymbol{y}|\boldsymbol{x})&amp;=\mathcal{N}(\boldsymbol{x}|\boldsymbol{0},\mathbf{I})\mathcal{N}(\boldsymbol{y}|\boldsymbol{Hx},\sigma^2\mathbf{I})\\&amp;\propto \mathcal{N}(\boldsymbol{x}|\boldsymbol{0},\mathbf{I})\mathcal{N}(\boldsymbol{x}|(\boldsymbol{H}^T\boldsymbol{H})^{-1}\boldsymbol{H}^T\boldsymbol{y},(\sigma^{-2}\boldsymbol{H}^T\boldsymbol{H})^{-1})\\&amp;\propto \mathcal{N}(\boldsymbol{x}|\boldsymbol{c},\boldsymbol{C})\end{align}其中\begin{align}\boldsymbol{C}&amp;=(\sigma^{-2}\boldsymbol{H}^T\boldsymbol{H}+\mathbf{I})^{-1}\\\boldsymbol{c}&amp;=\boldsymbol{C}\cdot (\sigma^{-2}\boldsymbol{H}^T\boldsymbol{y})=(\boldsymbol{H}^T\boldsymbol{H}+\sigma^2\mathbf{I})^{-1}\boldsymbol{H}^T\boldsymbol{y}\end{align}由于$p(\boldsymbol{x}|\boldsymbol{y})$为高斯分布，因此，该模型的MMSE估计为其后验概率均值，即高斯的均值$\boldsymbol{c}=(\boldsymbol{H}^T\boldsymbol{H}+\sigma^2\mathbf{I})^{-1}\boldsymbol{H}^T\boldsymbol{y}$。我们可以看到，这与LMMSE解一致。 最大后验概率估计（Maximum a posterior, MAP）最大后验概率估计，顾名思义，即选择后验概率最大值所处的$\boldsymbol{x}$作为估计器。\begin{align}\hat{\boldsymbol{x} }_{\text{MAP} }&amp;=\underset{\boldsymbol{x} }{\arg \max} \ p(\boldsymbol{x}|\boldsymbol{y})\\\end{align}估计器$\hat{\boldsymbol{x} }$的元素表示为\begin{align}\hat{x}_i&amp;=\underset{x_i}{\arg \max} \left\{\max_{\boldsymbol{x}_{\backslash i} }\ p(\boldsymbol{x}|\boldsymbol{y})\right\}\\&amp;=\underset{x_i}{\arg \max} \left\{\max_{\boldsymbol{x}_{\backslash i} }\ \log p(\boldsymbol{x}|\boldsymbol{y})\right\}\end{align} Remarks: 特别地，当先验概率为高斯时候，利用高斯相乘引理，我们可以得到后验概率$p(\boldsymbol{x}|\boldsymbol{y})$是关于$\boldsymbol{x}$的高斯分布。此时，最大后验概率估计，为该高斯分布的均值点，相应地，这种情况下的MMSE估计和MAP估计是一致的。然而，通常情况下先验概率为非高斯的，这种情况下，我们可以利用AMP算法或者EP算法来迭代计算边缘后验概率。 References[1] Donoho D L, Maleki A, Montanari A. How to design message passing algorithms for compressed sensing[J]. preprint, 2011.[2] Meng X, Wu S, Kuang L, et al. Concise derivation of complex Bayesian approximate message passing via expectation propagation[J]. arXiv preprint arXiv:1509.08658, 2015.[3] Minka T P. A family of algorithms for approximate Bayesian inference[D]. Massachusetts Institute of Technology, 2001.]]></content>
      <categories>
        <category>统计信号处理</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[回归问题初步之线性回归]]></title>
    <url>%2F2018%2F11%2F09%2F%E5%9B%9E%E5%BD%92%E9%97%AE%E9%A2%98%E5%88%9D%E6%AD%A5%E4%B9%8B%E7%BA%BF%E6%80%A7%E5%9B%9E%E5%BD%92%2F</url>
    <content type="text"><![CDATA[回归的线性模型如图1所示，机器学习根据数据是否带标签分为：有监督学习（supervised learning）、无监督学习（unsupervised learning）、半监督学习/强化学习（seimi-supervised learning）。所谓有监督学习，即训练样本中包含输入矢量$\boldsymbol{x}$以及其对应的目标矢量$t$。进一步地，有监督学习主要完成回归和分类两大任务。 回归（regression）：回归问题的目标，是在给定输入$\boldsymbol{x}$，预测一个或者多个连续目标（target）变量$t$的值。多项式曲线拟合就是一个经典的回归问题。 分类（classification）：分类的目标是将输入变量$\boldsymbol{x}$划分到$K$个离散的类别$\left\{\mathcal{C}_k\right\}_{k=1}^K$中的某一类。 给定数据集$\mathcal{D}=\left\{(\boldsymbol{x}_1,t_1),\cdots,(\boldsymbol{x}_n,t_n)\right\}$，我们的目标是预测对于给定新的$\boldsymbol{x}$所定义的$t$值。为此，我们首先要建立模型。直观的方法是，基于训练数据集$\mathcal{D}$，建立函数$y(\boldsymbol{x},\boldsymbol{w})$，对给定新值$\boldsymbol{x}$，预测其对应的目标$t$。广义上，从概率的角度，我们是对概率$p(t|\boldsymbol{x})$进行建模，因为它表达了对于任意新的输入$\boldsymbol{x}$，其所对应的$t$的可能性。这种方法等同于最小化一个恰当的损失函数的期望值，如若选择均方误差函数，则$t$的估计值，由条件概率$p(t|\boldsymbol{x},\boldsymbol{t})$的均值给出。注意，这里$\boldsymbol{t}$是训练集中的目标变量，$t$为新值$\boldsymbol{x}$所对应的目标。 一元线性回归线性回归模型是回归问题中的一个相对简单的特例。线性回归假设模型的输出和输入是线性关系\begin{align}y(\boldsymbol{x}_i,\boldsymbol{w})=\boldsymbol{w}^T\boldsymbol{x}_i+b\end{align}为了方便推导，我们假设$\boldsymbol{x}$的数据维度$d=1$。因此，对应的线性回归模型为\begin{align}f(x_i,w)=wx_i+b\end{align}我们的目标是让$f(x_i)$去近似$t_i$。我们选择使得均方误差最小的参数作为模型的参数\begin{align}(w,b)^{\ast}&amp;=\underset{(w,b)}{\arg \min} \frac{1}{n}\sum_{i=1}^n\left(f(x_i)-t_i\right)^2\\&amp;=\underset{(w,b)}{\arg \min} \sum_{i=1}^n\left(f(x_i)-t_i\right)^2\end{align}由于$y(x_i,w)$是$x_i$的线性函数，因此该问题是个凸问题，我们利用导数工具进行求解。定义误差函数$J=\sum_{i=1}^n\left(y(x_i,w)-t_i\right)^2$，我们首先求$J$对$b$的偏导数，并令偏导数为$0$\begin{align}\frac{\partial J}{\partial b}=0 \quad \Rightarrow \ b=\frac{1}{n}\sum_{i=1}^n(t_i-wx_i)=\overline{t}-w\overline{x}\end{align}其中$\overline{t}\overset{\triangle}{=}\frac{1}{n}\sum_{i=1}^nt_i$，$\overset{\triangle}{=}\frac{1}{n}\sum_{i=1}^nx_i$。从此处，我们可以看出，偏置$b$补偿了目标的平均值与输入的加权和之间的差。 将$b$代入误差函数$J$，求$J$对$w$的偏导\begin{align}\frac{\partial J}{\partial w}&amp;=2w\sum_{i=1}^nx_i^2-2\sum_{i=1}^n(t_i-b)x_i\\&amp;=2w\sum_{i=1}^nx_i^2-2\sum_{i=1}^n(t_i-\overline{t})x_i-2w\overline{x}^2\end{align}令偏导为$0$，得\begin{align}w=\frac{\sum_{i=1}^nx_i(t_i-\overline{t})}{\sum_{i=1}^nx_i^2-\overline{x}^2}\end{align} 多元线性回归设置输入和输出呈线性关系，给定数据集合$\mathcal{D}=\left\{(\boldsymbol{x}_1,t_1),(\boldsymbol{x}_n,t_n)\right\}$，若假设样本中$\boldsymbol{x}$的维度$d&gt;1$，这就是多元线性回归。为了简化计算步骤，这里我们设置$b=0$，即$y(\boldsymbol{x}_i,\boldsymbol{w})=\boldsymbol{w}^T\boldsymbol{x}_i$。因此，我们有\begin{align}\boldsymbol{y}=\boldsymbol{X}^T\boldsymbol{w}\end{align}其中$\boldsymbol{y}=[y(\boldsymbol{x}_1,\boldsymbol{w}),\cdots,y(\boldsymbol{x}_n,\boldsymbol{w})]^T$，$\boldsymbol{X}=[\boldsymbol{x}_1,\cdots,\boldsymbol{x}_n]$。定义误差函数$J$\begin{align}J=||\boldsymbol{t}-\boldsymbol{X}^T\boldsymbol{w}||^2\end{align}求$J$对$\boldsymbol{w}$的导数，并令偏导为零，得到\begin{align}\frac{\partial J}{\partial \boldsymbol{w} }=0 \quad \Rightarrow \boldsymbol{w}=(\boldsymbol{X}\boldsymbol{X}^T)^{-1}\boldsymbol{Xt}\end{align} 线性基函数模型上述的例子之中，我们假设输入$\boldsymbol{x}$与输出$f(\boldsymbol{x})$是线性关系，这给模型带来了很大的局限性。为此，我们设定，输出$y(\boldsymbol{x},\boldsymbol{w})$与输入的函数$\phi(\boldsymbol{x})$呈线性关系。（注：对于为什么引入基函数，这一点，可以从分类问题类比过来，在样本的原始空间中，样本线性不可分，引入基函数，将样本空间映射到更高维的特征空间，达到线性可分的目的。对应的，就是样本的线性拟合，效果不好。）\begin{align}y(\boldsymbol{x},\boldsymbol{w})=\sum\limits_{i=1}^nw_j\phi_j(\boldsymbol{x})+b\end{align}其中$\phi_i(\boldsymbol{x})$称为基函数（basis function），$b$称偏置参数。基函数的选择有很多种，如 高斯基函数\begin{align}\phi_j(x)=\exp \left[-\frac{(x-\mu_j)^2}{2s^2}\right]\end{align} Sigmoid函数\begin{align}\phi_j(x)&amp;=\sigma\left(\frac{x-\mu_j}{s}\right)\\\sigma(a)&amp;=\frac{1}{1+\exp(-a)}\end{align} 傅里叶函数、小波基函数，等。 最小二乘与最大似然前面说到，设置输出与基函数呈线性关系，通过最小化均方误差函数，可以得到模型。这里我们从最大似然的角度出发，通过高斯噪声的假设，对概率密度$p(t|\boldsymbol{x})$进行建模，同样能够得到相同解。这里我们假设模型输出$y(\boldsymbol{x},\boldsymbol{w})$与目标$t$的差值服从高斯分布。\begin{align}t=y(\boldsymbol{x},\boldsymbol{w})+\epsilon\end{align}其中$\epsilon\sim \mathcal{N}(\epsilon|0,\beta^{-1})$，因此给模型的似然函数为\begin{align}p(t|\boldsymbol{x},\boldsymbol{w},\beta)=\mathcal{N}\left(t|y(\boldsymbol{x},\boldsymbol{w}),\beta^{-1}\right)\end{align}对于给定一个新值$\boldsymbol{x}$，目标预测，由条件均值$y(\boldsymbol{x},\boldsymbol{w})$给出，即$\mathbb{E}[t|\boldsymbol{x}]=\int tp(t|\boldsymbol{x})\text{d}t=y(\boldsymbol{x},\boldsymbol{w})$。这个例子中$t$的分布是单峰的，实际中，$t$的条件分布，可以由多个高斯的线性加权和表示（近似），即混合高斯。 给定数据集$\left\{(\boldsymbol{x}_1,t_1),\cdots,(\boldsymbol{x}_N,t_N)\right\}$，设置模型输入与输出关系为$y(\boldsymbol{x},\boldsymbol{w})=\boldsymbol{w}^T\boldsymbol{\phi}(\boldsymbol{x})$，进一步假设$t$与$y(\boldsymbol{x},\boldsymbol{w})$之间存在一个高斯误差项$\epsilon\sim \mathcal{N}(\epsilon|0,\beta^{-1})$。记$\boldsymbol{t}\overset{\triangle}{=}[t_1,\cdots,t_N]^{T}$，$\boldsymbol{X}\overset{\triangle}{=}[\boldsymbol{x}_1,\cdots,\boldsymbol{x}_N]$，则有\begin{align}p(\boldsymbol{t}|\boldsymbol{X},\boldsymbol{w},\beta)=\prod\limits_{n=1}^N\mathcal{N}(t_n|\boldsymbol{w}^T\boldsymbol{\phi}(\boldsymbol{x}_n),\beta^{-1})\end{align}我们对似然函数取对数，有\begin{align}\ln p(\boldsymbol{t}|\boldsymbol{X},\boldsymbol{w},\beta)&amp;=\sum_{n=1}^N\ln \mathcal{N}(t_n|\boldsymbol{w}^T\boldsymbol{\phi}(\boldsymbol{x}_n),\beta^{-1})\\&amp;=\frac{N}{2}\ln \beta-\frac{N}{2}\ln (2\pi)-\beta \left(\frac{1}{2}\sum_{n=1}^N(t_n-\boldsymbol{w}^T\boldsymbol{\phi}(\boldsymbol{x}_n))^2\right)\\&amp;=\frac{N}{2}\ln \beta-\frac{N}{2}\ln (2\pi)-\beta E_D(\boldsymbol{w})\end{align}其中$E_D(\boldsymbol{w})\overset{\triangle}{=}\frac{1}{2}\sum_{n=1}^N(t_n-\boldsymbol{w}^T\boldsymbol{\phi}(\boldsymbol{x}_n))^2$。 若假设$\beta$与$\boldsymbol{w}$无关，求对数似然关于$\boldsymbol{w}$的梯度\begin{align}\nabla \ln p(\boldsymbol{t}|\boldsymbol{w},\beta)=\beta \sum_{n=1}^N (t_n-\boldsymbol{w}^T\boldsymbol{\phi}(\boldsymbol{w}))\boldsymbol{\phi}(\boldsymbol{x}_n)^T\end{align}令梯度为0，得到\begin{align}\boldsymbol{w}_{\text{ML} }=\left(\sum_{n=1}^N\boldsymbol{\phi}(\boldsymbol{x}_n)\boldsymbol{\phi}(\boldsymbol{x}_n)^T\right)^{-1}\left(\sum_{n=1}^N\boldsymbol{\phi}(\boldsymbol{x}_n)t_n\right)\end{align}定义\begin{align}\boldsymbol{\Phi}=\left(\begin{matrix}\phi_0(\boldsymbol{x}_1) &amp;\cdots &amp;\phi_{M-1}(\boldsymbol{x}_1)\\\vdots &amp;\ddots &amp;\vdots\\\phi_0(\boldsymbol{x}_N) &amp;\cdots &amp;\phi_{M-1}(\boldsymbol{x}_N)\end{matrix}\right)\end{align}因此\begin{align}\boldsymbol{w}_{\text{ML} }=\left(\mathbf{\Phi}^T\mathbf{\Phi}\right)^{-1}\mathbf{\Phi}^T\boldsymbol{t}\end{align}很明显，这是最小二乘解（least square, LS）。 关于最小二乘，更为直观的解释，可以通过图-2表示。基函数$\left\{\boldsymbol{\phi}(\boldsymbol{x}_1),\cdots,\boldsymbol{\phi}(\boldsymbol{x}_n)\right\}$，张成空间$\mathcal{C}$（图中，考虑更为简单的特例，二维平面）。最小二乘的集合解释，就是在基函数所张成的几何空间$\mathcal{C}$上，找到$\boldsymbol{t}$的正交投影$\boldsymbol{y}$，此时所得到的误差$e$最小。 将$\boldsymbol{w}_{\text{ML} }$代回对数似然函数，求对数似然函数对$\beta$的偏导，并令其为0，得到\begin{align}\beta_{\text{ML} }^{-1}=\frac{1}{N}\sum_{n=1}^N\left(t_n-\boldsymbol{w}_{\text{ML} }^T\boldsymbol{\phi}(\boldsymbol{x}_n)\right)^2\end{align}因此，我们看到噪声方差的倒数由目标值与回归函数的加权和的残差（residual variance）给出。 正则化最小二乘（Least Square）为了控制过拟合，我们给误差函数添加正则化项，相应地，目标函数变为\begin{align}J=E_D(\boldsymbol{w})+\lambda E_W(\boldsymbol{w})\end{align}其中$\lambda$是正则化系数，用于控制数据相对误差$E_D(\boldsymbol{w})$和正则化项$E_W(\boldsymbol{w})$的比例。正则化项的选择，可以是一个简单的权向量$\boldsymbol{w}$的二范数$E_W(\boldsymbol{w})=\frac{1}{2}||\boldsymbol{w}||^2$，若考虑平方和误差\begin{align}E_D(\boldsymbol{w})=\frac{1}{2}\sum_{n=1}^N(t_n-\boldsymbol{w}^T\boldsymbol{\phi}(\boldsymbol{x}_n))^2\end{align}则，相应的目标向量为\begin{align}J=\frac{1}{2}\sum_{n=1}^N(t_n-\boldsymbol{w}^T\boldsymbol{\phi}(\boldsymbol{x}_n))^2+\frac{\lambda}{2}||\boldsymbol{w}||^2\end{align}利用导数工具，我们可以到权值$\boldsymbol{w}$的解\begin{align}\boldsymbol{w}=(\lambda \mathbf{I}+\mathbf{\Phi}^T\mathbf{\Phi})^{-1}\mathbf{\Phi}^T\boldsymbol{t}\end{align}这是线性最小均方误差（linear minimum mean square error, LMMSE）解，这类似于给$\boldsymbol{w}$加了一个高斯先验分布。 更为一般地，考虑正则化项为$p$-范数\begin{align}J=E_D(\boldsymbol{w})+\lambda ||\boldsymbol{w}||^p\end{align}而，实际中，二范数正则化项应用较多。 贝叶斯线性回归在讨论使用最大似然方法寻找线性回归模型时，我们已经看到，模型的复杂度由基函数$\phi(\boldsymbol{x})$的数量及其具体形式所决定。最大似然方法本身的缺陷在于，需要大量的样本（渐进最优），并且可能造成过拟合的现象。 过拟合：模型在训练集上可以达到很好的效果（太依赖训练集），但是在测试集上，效果奇差。 欠拟合：模型不够复杂，或者数据训练太少，导致无法预测数据。 这里，我们考虑线性回归的贝叶斯方法，为了简单起见，我们考虑单一目标变量$t$的情形。对于多个变量的推广，可以类比于线性回归的最大似然法。 参数的先验分布关于线性回归的贝叶斯方法的讨论，我们首先引入权值矢量的先验概率分布$p(\boldsymbol{w})$。这里，我们假设其先验分布（prior distribution）为\begin{align}p(\boldsymbol{w})=\mathcal{N}\left(\boldsymbol{w}|\boldsymbol{0},\alpha^{-1}\mathbf{I}\right)\end{align}计算$\boldsymbol{w}$的后验分布（posterior distribution）如下\begin{align}p(\boldsymbol{w}|\boldsymbol{t})&amp;\propto \frac{p(\boldsymbol{t}|\boldsymbol{w})p(\boldsymbol{w})}{p(\boldsymbol{t})}\\&amp;\propto p(\boldsymbol{t}|\boldsymbol{w})p(\boldsymbol{w})\end{align}其中$p(\boldsymbol{t}|\boldsymbol{w})=\mathcal{N}(\boldsymbol{t}|\boldsymbol{\Phi}\boldsymbol{w},\beta^{-1}\mathbf{I})$为模型的似然函数\begin{align}\mathcal{N}(\boldsymbol{t}|\boldsymbol{\Phi w},\beta^{-1}\mathbf{I})\propto \mathcal{N}(\boldsymbol{w}|(\mathbf{\Phi}^T\mathbf{\Phi})^{-1}\mathbf{\Phi}^T\boldsymbol{t},(\beta\boldsymbol{\Phi}^T\boldsymbol{\Phi})^{-1})\end{align} 高斯相乘引理（Guassian product lemma）：\begin{align}\mathcal{N}(\boldsymbol{x}|\boldsymbol{a},\boldsymbol{A})\mathcal{N}(\boldsymbol{x}|\boldsymbol{b},\boldsymbol{B})=\mathcal{N}(\boldsymbol{0}|\boldsymbol{a}-\boldsymbol{b},\boldsymbol{A}+\boldsymbol{B})\mathcal{N}(\boldsymbol{x}|\boldsymbol{c},\boldsymbol{C})\end{align}其中\begin{align}\boldsymbol{C}&amp;=\left(\boldsymbol{A}^{-1}+\boldsymbol{B}^{-1}\right)^{-1}\\\boldsymbol{c}&amp;=\boldsymbol{C}\cdot\left(\boldsymbol{A}^{-1}\boldsymbol{a}+\boldsymbol{B}^{-1}\boldsymbol{b}\right)\end{align} 利用高斯相乘引理，我们有\begin{align}p(\boldsymbol{w}|\boldsymbol{t})&amp;=\mathcal{N}\left(\boldsymbol{w}|\boldsymbol{\mu},\boldsymbol{\Sigma}\right)\\\boldsymbol{\Sigma}&amp;\overset{\triangle}{=}\left(\alpha \mathbf{I}+\eta\boldsymbol{\Phi}^T\boldsymbol{\Phi}\right)^{-1}\\\boldsymbol{\mu}&amp;\overset{\triangle}{=}\beta\boldsymbol{\Sigma}\mathbf{\Phi}^T\boldsymbol{t}\end{align} 预测分布在实际应用中，我们通常对新值$\boldsymbol{x}$所对应的$t$感兴趣，这需要我们计算出预测$t$分布，定义\begin{align}p(t|\boldsymbol{t},\alpha,\beta)=\int p(t|\boldsymbol{w},\beta)p(\boldsymbol{w}|\boldsymbol{t},\alpha,\beta)\text{d}\boldsymbol{w}\end{align}注意，这里$\boldsymbol{t}$是训练集中样本目标，$t$是测试集目标。为计算该分布，我们进行维度扩充，计算\begin{align}p(\boldsymbol{t}’|\boldsymbol{t},\alpha,\beta)=\int p(\boldsymbol{t}’|\boldsymbol{w},\beta)p(\boldsymbol{w}|\boldsymbol{t},\alpha,\beta)\text{d}\boldsymbol{w}\end{align}其中\begin{align}p(\boldsymbol{t}’|\boldsymbol{w},\beta)=\mathcal{N}(\boldsymbol{t}|\boldsymbol{\Phi}\boldsymbol{w},\beta^{-1}\mathbf{I})\propto \mathcal{N}(\boldsymbol{w}|(\mathbf{\Phi}^T\mathbf{\Phi})^{-1}\mathbf{\Phi}^T\boldsymbol{t}’,(\beta\boldsymbol{\Phi}^T\boldsymbol{\Phi})^{-1})\end{align}为了简化计算步骤，定义\begin{align}\mathbf{H}_1&amp;=(\mathbf{\Phi}^T\mathbf{\Phi})^{-1}\mathbf{\Phi}^T=\mathbf{\Phi}^{\dagger}\\\mathbf{H}_2&amp;=(\beta\boldsymbol{\Phi}^T\boldsymbol{\Phi})^{-1}\end{align}利用高斯相乘引理，有\begin{align}p(\boldsymbol{t}’|\boldsymbol{t},\alpha,\beta)&amp;\propto \mathcal{N}\left(\boldsymbol{H}_1\boldsymbol{t}’|\boldsymbol{\mu},\boldsymbol{H}_2+\boldsymbol{\Sigma}\right)\\&amp;\propto \mathcal{N}\left(\boldsymbol{t}’|\boldsymbol{\mu}_1,\boldsymbol{\Sigma}_1\right)\end{align}其中\begin{align}\boldsymbol{\Sigma}_1&amp;=\left(\boldsymbol{H}_1^T(\boldsymbol{H}_2+\boldsymbol{\Sigma})^{-1}\boldsymbol{H}_1\right)^{-1}\\\boldsymbol{\mu}_1&amp;=\boldsymbol{\Sigma}_1\boldsymbol{H}_1^T(\boldsymbol{H}_2+\boldsymbol{\Sigma})^{-1}\boldsymbol{\mu}\end{align}由于这里，我们要求的是$t$的分布，其维度为1，因此，我们重新设置$\boldsymbol{\Phi}=[\boldsymbol{\phi}(\boldsymbol{x}),\cdots,\boldsymbol{\phi}(\boldsymbol{x})]$，计算$t$的均值和方差如下\begin{align}\text{方差：}&amp;\frac{1}{N}\text{tr}\left\{\left(\boldsymbol{H}_1^T(\boldsymbol{H}_2+\boldsymbol{\Sigma})^{-1}\boldsymbol{H}_1\right)^{-1}\right\}=\frac{1}{N}\text{tr}\left\{\beta^{-1}\mathbf{I}+\boldsymbol{\Phi}^T\boldsymbol{\Sigma}\boldsymbol{\Phi}\right\}=\beta^{-1}+\boldsymbol{\phi}(\boldsymbol{x})^T\boldsymbol{\Sigma}\boldsymbol{\phi}(\boldsymbol{x})\\\text{均值：}&amp; \left(\boldsymbol{H}_1^T(\boldsymbol{H}_2+\boldsymbol{\Sigma})^{-1}\boldsymbol{H}_1\right)^{-1}\boldsymbol{H}_1^T(\boldsymbol{H}_1+\boldsymbol{\Sigma})^{-1}\boldsymbol{\mu}=\boldsymbol{\Phi}\boldsymbol{\mu}\end{align}因此\begin{align}p(t|\boldsymbol{t},\alpha,\beta)=\mathcal{N}(t|\boldsymbol{\phi}(\boldsymbol{x})^T\boldsymbol{\mu},\beta^{-1}+\boldsymbol{\phi}(\boldsymbol{x})^T\boldsymbol{\Sigma}\boldsymbol{\phi}(\boldsymbol{x}))\end{align}]]></content>
      <categories>
        <category>PRML</category>
      </categories>
      <tags>
        <tag>PRML</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[高斯相乘引理]]></title>
    <url>%2F2018%2F11%2F07%2F%E9%AB%98%E6%96%AF%E7%9B%B8%E4%B9%98%E5%BC%95%E7%90%86%2F</url>
    <content type="text"><![CDATA[高斯PDF 标量实高斯分布\begin{align}\mathcal{N}(x|a,A)=\frac{1}{\sqrt{2\pi A} }\exp \left[{-\frac{(x-a)^2}{2A} }\right]\end{align} 标量复高斯分布\begin{align}\mathcal{N}_c(x|a,A)=\frac{1}{\pi A}\exp \left[-\frac{||x-a||^2}{A}\right]\end{align} 矢量实高斯分布\begin{align}\mathcal{N}(\boldsymbol{x}|\boldsymbol{a},\boldsymbol{A})=(2\pi)^{-\frac{N}{2} }\det(\boldsymbol{A})^{-\frac{1}{2} }\exp \left({-\frac{1}{2}(\boldsymbol{x}-\boldsymbol{a})^T\boldsymbol{A}^{-1}(\boldsymbol{x}-\boldsymbol{a})}\right)\end{align}其中$N$表示$\boldsymbol{x}$的维度。 矢量复高斯分布\begin{align}\mathcal{N}_c(\boldsymbol{x}|\boldsymbol{a},\boldsymbol{A})=\frac{1}{\det(\pi \boldsymbol{A})}\exp \left[{-(\boldsymbol{x}-\boldsymbol{a})^H\boldsymbol{A}^{-1}(\boldsymbol{x}-\boldsymbol{a})}\right]\end{align} 标量实高斯相乘引理给定高斯概率分布$\mathcal{N}(x|a,A)$和$\mathcal{N}(x|b,B)$，存在\begin{equation}\mathcal{N}(x|a,A)\mathcal{N}(x|b,B)=\mathcal{N}(0|a-b,A+B)\mathcal{N} \left({x\left|\frac{\frac{a}{A}+\frac{b}{B} }{\frac{1}{A}+\frac{1}{B} },\frac{1}{\frac{1}{A}+\frac{1}{B} }\right.}\right)\end{equation}其中$\mathcal{N}(x|a,A)$表示以均值为$a$，方差为$A$，自变量为$x$的高斯概率密度函数。证： 指数部分\begin{eqnarray}\mathcal{N}(x|a,A)\mathcal{N}(x|b,B)&amp;\propto&amp; \exp\left[{-\frac{(x-a)^2}{2A}-\frac{(x-b)^2}{2B} }\right]\\&amp;\propto&amp;\exp{\left[{-x^2\left({\frac{1}{2A}+\frac{1}{2B} }\right)+x\left({\frac{a}{A}+\frac{b}{B} }\right)}\right]}\\&amp;\propto&amp;\exp{\left[{-(\frac{1}{2A}+\frac{1}{2B})\left({x-\frac{\frac{a}{A}+\frac{b}{B} }{\frac{1}{A}+\frac{1}{B} }}\right)^2}\right]}\\&amp;\propto&amp;\mathcal{N} \left({x\left|\frac{\frac{a}{A}+\frac{b}{B} }{\frac{1}{A}+\frac{1}{B} },\frac{1}{\frac{1}{A}+\frac{1}{B} }\right.}\right)\end{eqnarray}其中$\propto$表示正比于。 系数部分（显然）\begin{align}\frac{1}{\sqrt{2\pi A} }\frac{1}{\sqrt{2\pi B} }=\frac{1}{\sqrt{2\pi (A+B)} }\frac{1}{\sqrt{2\pi \frac{AB}{A+B} }}\end{align}因此\begin{equation}\mathcal{N}(x;a,A)\mathcal{N}(x;b,B)=\mathcal{N}(0;a-b,A+B)\mathcal{N} \left({x;\frac{\frac{a}{A}+\frac{b}{B} }{\frac{1}{A}+\frac{1}{B} },\frac{1}{\frac{1}{A}+\frac{1}{B} }}\right)\end{equation}从高斯相乘引理，我们可以得到以下两个结论1. 两个高斯PDF相乘正比于一个新的高斯PDF。2. 两个Gaussian PDF相乘，其实是在降方差，$\left({\frac{1}{A}+\frac{1}{B} }\right)^{-1} \leq \min (A,B)$ 矢量实高斯相乘引理给定矢量实高斯分布$\mathcal{N}(\boldsymbol{x}|\boldsymbol{a},\boldsymbol{A})$，$\mathcal{N}(\boldsymbol{x}|\boldsymbol{b},\boldsymbol{B})$\begin{align}\mathcal{N}(\boldsymbol{x}|\boldsymbol{a},\boldsymbol{A})\mathcal{N}(\boldsymbol{x}|\boldsymbol{b},\boldsymbol{B})=\mathcal{N}(\boldsymbol{0}|\boldsymbol{a}-\boldsymbol{b},\boldsymbol{A}+\boldsymbol{B})\mathcal{N}(\boldsymbol{x}|\boldsymbol{c},\boldsymbol{C})\end{align}其中\begin{align}\boldsymbol{C}&amp;=\left({\boldsymbol{A}^{-1}+\boldsymbol{B}^{-1} }\right)^{-1}\\\boldsymbol{c}&amp;=\boldsymbol{C}\cdot \left(\boldsymbol{A}^{-1}\boldsymbol{a}+\boldsymbol{B}^{-1}\boldsymbol{b}\right)\end{align}证： 指数部分\begin{align}\mathcal{N}(\boldsymbol{x}|\boldsymbol{a},\boldsymbol{A})\mathcal{N}(\boldsymbol{x}|\boldsymbol{b},\boldsymbol{B})&amp;\propto \exp \left[{-\frac{1}{2}(\boldsymbol{x}-\boldsymbol{a})^T\boldsymbol{A}^{-1}(\boldsymbol{x}-\boldsymbol{a})-\frac{1}{2}(\boldsymbol{x}-\boldsymbol{b})^T\boldsymbol{B}^{-1}(\boldsymbol{x}-\boldsymbol{b})}\right]\\&amp;\propto \exp \left[{-\frac{1}{2}\boldsymbol{x}^T(\boldsymbol{A}^{-1}+\boldsymbol{B}^{-1})\boldsymbol{x}-2\boldsymbol{x}^T(\boldsymbol{A}^{-1}\boldsymbol{a}+\boldsymbol{B}^{-1}\boldsymbol{b})}\right]\\&amp;\propto \mathcal{N}\left({\boldsymbol{x}|\boldsymbol{c},\boldsymbol{C} }\right)\end{align} 系数部分\begin{align}&amp;|\boldsymbol{A}| |\boldsymbol{B}|=|\boldsymbol{A}+\boldsymbol{B}| |(\boldsymbol{A}^{-1}+\boldsymbol{B}^{-1})^{-1}|\\\Rightarrow \ &amp;|\boldsymbol{AB}(\boldsymbol{A}^{-1}+\boldsymbol{B}^{-1})|=|\boldsymbol{A}+\boldsymbol{B}|\\\Rightarrow \ &amp; |\boldsymbol{A}+\boldsymbol{B}|=|\boldsymbol{A}+\boldsymbol{B}|\end{align} 复高斯相乘引理 标量复高斯相乘引理\begin{align}\mathcal{N}_c(x|a,A)\cdot\mathcal{N}_c(x|b,B)=\mathcal{N}_c(0|a-b,A+B)\mathcal{N}_c\left({x\left|\frac{\frac{a}{A}+\frac{b}{B} }{\frac{1}{A}+\frac{1}{B} },\frac{1}{\frac{1}{A}+\frac{1}{B} }\right.}\right)\end{align}证明过程可以参考标量实高斯相乘引理。 矢量复高斯相乘引理\begin{align}\mathcal{N}_c(\boldsymbol{x}|\boldsymbol{a},\boldsymbol{A})\cdot\mathcal{N}_c(\boldsymbol{x}|\boldsymbol{b},\boldsymbol{B})=\mathcal{N}_c(0|a-b,A+B)\cdot\mathcal{N}_c(\boldsymbol{x}|\boldsymbol{c},\boldsymbol{C})\end{align}其中\begin{align}\boldsymbol{C}&amp;=\left({\boldsymbol{A}^{-1}+\boldsymbol{B}^{-1} }\right)^{-1}\\\boldsymbol{c}&amp;=\boldsymbol{C}\cdot \left(\boldsymbol{A}^{-1}\boldsymbol{a}+\boldsymbol{B}^{-1}\boldsymbol{b}\right)\end{align}证明过程可以参考矢量实高斯相乘引理。]]></content>
      <categories>
        <category>基础数学</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[条件概率与条件均值]]></title>
    <url>%2F2018%2F11%2F07%2F%E6%9D%A1%E4%BB%B6%E6%A6%82%E7%8E%87%E4%B8%8E%E6%9D%A1%E4%BB%B6%E5%9D%87%E5%80%BC%2F</url>
    <content type="text"><![CDATA[笔者在研究室内定位算法的过程中，有一些论文出现了条件均值。比如$x\sim f(x)$，那么该变量的均值为\begin{align}\mathbb{E}[X]=\int_{-\infty }^{+\infty }xf(x)\text{d}x\end{align}现在需要求解$E\left[ X|x&gt;a \right]$。我们将条件均值进行展开\begin{align}\mathbb{E}\left[ X|x&gt;a \right]=\int_{-\infty }^{+\infty }xf(x|x&gt;a)\text{d}x\end{align}从公式中可以看出，欲求条件均值，需要先得到条件概率密度。我们可以通过如下公式，得到条件概率\begin{align}f(x|x&gt;a)=\frac{f(x)}{1-F(a)}, (x&gt;a)\end{align}其实，也很好理解，$f(x|x&gt;a)$就是对$f(x)\ (x&gt;a)$进行了比例放大，而这个放大系数就是$\frac{1}{1-F(a)}$。其中$F(x)=\int_{-\infty}^x f(t)\text{d}t$表达累积分布函数（cumulative distribution function, CDF） 以下，给出公式具体证明\begin{align}f(x|x&gt;a)=\frac{\text{d} F(x|x&gt;a)}{\text{d}x}\end{align}其中\begin{align}F(x|x&gt;a)=\frac{F(x,x&gt;a)}{F(x&gt;a)}=\frac{F(x)}{1-F(a)}, \ x&gt;a\end{align}因此，有\begin{align}f(x|x&gt;a)=\frac{1}{1-F(a)}\frac{\text{d} F(x) }{\text{d} x}=\frac{f(x)}{1-F(a)},\ x&gt;a\end{align}]]></content>
      <categories>
        <category>基础数学</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[信号系统笔记之连续时间傅里叶变换]]></title>
    <url>%2F2018%2F11%2F07%2F%E4%BF%A1%E5%8F%B7%E7%B3%BB%E7%BB%9F%E7%AC%94%E8%AE%B0%E4%B9%8B%E8%BF%9E%E7%BB%AD%E6%97%B6%E9%97%B4%E5%82%85%E9%87%8C%E5%8F%B6%E5%8F%98%E6%8D%A2%2F</url>
    <content type="text"><![CDATA[引言 学习傅里叶级数之后，我们得到一个结论，任何满足狄利克雷条件（Dirichlet Conditions）的周期信号$f(t)$可以分解为一串虚指数信号的线性加权和，即傅里叶级数。然而实际上，我们需要处理的信号大多为非周期信号。因此，要想对非周期信号进行频域分析，我们需要得到一个属于非周期信号的“傅里叶级数”。在周期信号的分解中，我们选择信号的分解区间为$(a-T/2,a+T/2)$。当周期信号的周期$T\to\infty$时，周期信号就转换为非周期信号（周期$T\to\infty$），此时分解区间为$(-\infty,+\infty)$。 为了能够透彻整个傅里叶级数到傅里叶变换的过程，笔者先从黎曼积分讲起。然后再推导非周期信号的傅里叶变换公式。 黎曼积分黎曼是德国数学家，数学分析大师，物理学家，被后人誉为定积分之父。对数学分析和微分几何做出了重要贡献，其中一些为广义相对论的发展铺平了道路。他的名字出现在黎曼ζ函数，黎曼积分，黎曼几何，黎曼引理，黎曼流形，黎曼映照定理，黎曼-希尔伯特问题，黎曼思路回环矩阵和黎曼曲面中。 如何求函数$f(t)$在区间$[a,b]$上的面积呢？于是，黎曼想到将区间$[a,b]$划分为无数个子区间，设第$i$个区间的宽度为$\Delta x_i$，然后在该区间上任取一点$\xi_i\left(\xi_i\in [x_{i-1},x_i]\right)$，用$f(\xi_i)\triangle x_i$来表示该小柱条的面积。令$\lambda=\max \left\{\triangle x_i\right\}$，当$\lambda \to 0$时，函数$f(t)$在区间$[a,b]$上的面积可以表示为\begin{align}S=\lim_{\lambda\rightarrow 0} \sum_{i=1}^n f(\xi_i)\triangle x_i\end{align}通常采用等分切割处理，并选择区间最右端的函数值为小柱条的高，因此\begin{align}S=\lim_{n\rightarrow \infty} \sum_{i=1}^n f\left(a-\frac{b-a}{n}i\right)\frac{b-a}{n}\end{align}为了定义这个运算，黎曼翻阅书籍，由于该运算是极限求和，因此选取了求和单词Sum的首字母，并对其进行拉长也就是现在的积分符号$\int$。因此上式写为\begin{align}S=\int_a^b f(x)\text{d}x\end{align}心细的朋友应该会发现，即使$\lambda \to 0$，但还是存在误差，设每一个小柱条与该区间实际面积之差为$\triangle s_i$，那么总体误差为\begin{align}\triangle S=\sum_{i\rightarrow \infty}\triangle s_i\end{align}无穷个无穷小之和可能不为无穷小，因此式子的$S=\int_a^bf(x)\text{d}x$的成立还需证明$\Delta {S}\to 0$。这种工作一般需要数学家去完成，这里不进行扩展。至此，我们有了极限求和的思想。 再谈傅里叶级数如图2所示，周期性方波信号，其周期为$T$，单周期内，方波持续时间为$2\tau $，讨论周期$T$对傅里叶级数$F_n$的影响。 该方波信号的傅里叶级数$F_n$\begin{align}F_n =\frac{\tau}{T}\text{Sa}\left({\frac{nw_0\tau}{2} }\right)\end{align}设$\tau=1/2$，讨论周期$T$对$F_n$的影响【实验程序】1234567891011121314clear allT=2; %信号周期tau=1/2; %方波持续时间t=-20*pi:0.01:20*pi; %包络显示范围wo=2*pi/T; %角频率nwo=-20*pi:wo:20*pi; %Fn=(tau/T).*sinc(nwo*tau/(2*pi)); %傅里叶级数谱线f=(tau/T).*sinc(t*tau/(2*pi)); %包络stem(nwo,Fn) %绘制傅里叶级数谱线hold onplot(t,f,&apos;--r&apos;); %绘制保罗谱线hold ontitle(strcat(&apos;T=&apos;,num2str(T)));hold on $T=2$，$\omega_o=\pi$ $T=4$，$\omega_o=\frac{\pi}{2}$ $T=8$，$\omega_o=\frac{\pi}{4}$ $T=16$，$\omega_o=\frac{\pi}{8}$ $T=32$，$\omega_o=\frac{\pi}{16}$ 从上述实验可以看出，随着周期$T$的增大，频率谱线之间的间距逐渐减小，谱线的幅度逐渐减小。当$T\to \infty$时，频率谱线趋于连续谱线，谱线的幅度趋于0。然而，研究幅度为0的频率谱线是没有意义的，这又要如何处理呢？ 从傅里叶级数到傅里叶变换对于周期$T\to \infty$的周期信号$f(t)$，其傅里叶级数为\begin{align}F_n=\lim_{T\rightarrow \infty}\frac{1}{T} \int_T f(t)e^{-jnw_0t}\text{d}t\end{align}实际信号处理中，$f(t)$为有限长信号，因此$\int_T f(t)e^{-jnw_0t}\text{d}t$可以看做是一个有界常量，那么\begin{align}F_n=\lim_{T\rightarrow \infty} \frac{1}{T}\int_T f(t)e^{-jnw_0t}\text{d}t\end{align}就是一个无穷小量（无穷小乘以有界常量仍为无穷小）。因此，在等式两端同时乘以$T$，有\begin{align}TF_n=\lim_{T\rightarrow \infty}\int_T f(t)e^{-jnw_0t}\text{d}t\end{align}记$X(jw)=TF_n$，当$T\to \infty $时，$nw_0\rightarrow w$，因此\begin{align}X(jw)=\int_{-\infty}^{+\infty}f(t)e^{-jwt}\text{d}t\end{align}【注】这里为什么要记作$X(jw)$，主要是为了和傅里叶级数$F_n$相区别，$F_n$是离散谱线，而$X(jw)$是连续谱线。另外，傅里叶变换是拉普拉斯变换的特殊形式，即$s=\left( \sigma +j\omega \right)\left| _{\sigma =0} \right.$时，拉普拉斯变换就转换成了傅里叶变换。 傅里叶逆变换\begin{align}f(t)&amp;=\sum\limits_{n=-\infty}^{+\infty}F_ne^{jnw_0t}\\&amp;=\sum_{n=-\infty}^{+\infty}X(jw)\frac{1}{T}e^{jnw_0t}\\&amp;=\frac{1}{2\pi}\left(\sum_{n=-{\infty} }^{+\infty}X(jw)e^{jnw_0t}\right)\frac{2\pi}{T}\end{align}由于$T\to \infty $，因此$\frac{2\pi}{T}=w_o\to \text{d}w$（这里的$w_0$就是小柱条的宽），$w_0\to w$，由此前黎曼积分的知识，此时求和变成了积分\begin{align}f(t)&amp;=\frac{1}{2\pi}\int_{-\infty}^{+\infty}X(jw)e^{jwt}\text{d}w\end{align}因此，信号$f(t)$的傅里叶变换对为\begin{align}f(t)&amp;=\frac{1}{2\pi }\int_{-\infty }^{+\infty }X(jw ){ {e}^{jw t} }dw \\X(jw )&amp;=\int_{-\infty }^{+\infty }{f(t){ {e}^{-jw t} }dt} \\\end{align}至此，连续时间频域分析得到了统一，我们可以用频域分析法来分析信号。我们称傅里叶级数为频谱，称傅里叶变换为频谱密度，两者统称为频谱。 周期信号的傅里叶变换一个周期为$T$的周期函数$f(t)$，可以展开成傅里叶级数\begin{align}f(t)=\sum_{n=-\infty}^{+\infty}F_ne^{-jnw_0t}\end{align}对两边取傅里叶变换\begin{align}\mathscr{F}[f(t)]=\mathscr{F}\left[\sum_{n=-\infty}^{+\infty}F_ne^{-jnw_0t} \right]=\sum_{n=-\infty}^{+\infty} F_n\mathscr{F}\left[e^{-jnw_0t}\right]\end{align}由于$\mathscr{F}\left[e^{-jnw_0t}\right]=2\pi \delta(w-w_0)$，因此\begin{align}\mathscr{F}[f(t)]=2\pi \sum_{n=-\infty}^{+\infty}F_n\delta(w-w_0)\end{align}]]></content>
      <categories>
        <category>信号系统</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[支持向量机]]></title>
    <url>%2F2018%2F11%2F07%2F%E6%94%AF%E6%8C%81%E5%90%91%E9%87%8F%E6%9C%BA%2F</url>
    <content type="text"><![CDATA[支持向量机定义的引出给定训练样本集$D=\left\{ {(\boldsymbol{x}_1,y_1),\cdots,(\boldsymbol{x}_m,y_m)}\right\},y_i\in \left\{ {-1,+1}\right\}$。分类最基本的出发点是找到一个超平面来区分训练样本集中的不同类别。事实上，可能存在很多这样的超平面。我们需要制定衡量标准（如：欧式距离）来确定最合适的超平面。如图1所示的二维平面，直观上看，红色直线那个相对于其他更为适合，因为该直线对训练样本局部扰动的“容忍性”最好。现在，从数学角度来确定该超平面。 在样本空间中，超平面通过如下线性方程组来描述\begin{align}\boldsymbol{w}^T\boldsymbol{x}+b=0\end{align}其中$\boldsymbol{w}=\left\{ {w_1,\cdots,w_d}\right\}^T$为超平面法向量，决定超平面的方向；$b$为位移项，决定了超平面与原点之间的距离。样本空间中任意点$\boldsymbol{x}$到超平面$\boldsymbol{w}^T\boldsymbol{x}+b=0$的距离为\begin{align}r=\frac{|\boldsymbol{w}^T\boldsymbol{x}+b|}{||\boldsymbol{w}||}\end{align}空间任意点到超平面的距离，可以参考点到直线距离，类比得到。 假设超平面$(\boldsymbol{w},b)$能够将样本正确分类，即对于任意的$(\boldsymbol{x},y_i)\in D$，若$y_i=+1$，则有$\boldsymbol{w}^T\boldsymbol{x}_i+b&gt;0$；若$y_i=-1$，则有$\boldsymbol{w}^T\boldsymbol{x}_i+b&lt;0$。给定超平面，定义如下两个平面\begin{align}\left\{\begin{matrix}\boldsymbol{w}^T\boldsymbol{x}_i+b\geq +1,\ y_i=+1\\\boldsymbol{w}^T\boldsymbol{x}_i+b\leq -1,\ y_i=+1\end{matrix}\right.\end{align}样本中，距离超平面距离最小的两个或者（在一类样本点中可能存在到超平面距离相同的点）训练样本点，我们称其为支持向量（support vector），两异类支持向量到超平面的距离之和，即两平面之间的距离，称之为间隔（margin）。间隔的代数表达为$\gamma=\frac{2}{||\boldsymbol{w}||}$，如图2所示。 为了增加超平面的鲁棒性，因此需要找到最大间隔的超平面，即\begin{align}\max_{(\boldsymbol{w},b)} \ &amp;\frac{1}{||\boldsymbol{w}||}\\\text{s.t.} \ &amp;y_i(\boldsymbol{w}^T\boldsymbol{x}_i+b)\geq 1 \quad (i=1,\cdots,m)\end{align}该问题等价于\begin{align}\min_{(\boldsymbol{w},b)} \ &amp; \frac{1}{2}||\boldsymbol{w}||^2\\\text{s.t.} \ &amp;y_i(\boldsymbol{w}^T\boldsymbol{x}_i+b)\geq 1 \quad (i=1,\cdots,m)\end{align}这就是支持向量机（support vector machine, SVM）的基本型。 对偶（Dual）问题的引出我们希望找到最大化间隔的超平面\begin{align}f(\boldsymbol{x})=\boldsymbol{w}^T\boldsymbol{x}+b\end{align}这里我们假设训练样本是线性可分的（上一节中，我们假设样本是两类的）。我们注意到，求解参数$(\boldsymbol{w},b)$本身就是一个凸优化问题，可以通过凸优化工具箱进行计算。另外，我们还可以通过解该问题的对偶问题，来得到最优解。这样所带来的好处是，将一种最优化（最小化）问题转化为另一种最优化（最大化）问题，而后者相对于前者更容易计算。 对偶问题对于求解标准的优化问题\begin{align}\min \ &amp;f_0(\boldsymbol{x})\\\text{s.t.} \ &amp;f_i(\boldsymbol{x})\leq 0, i=1\cdots,m\\&amp;h_j(\boldsymbol{x})=0, j=1,\cdots,p\end{align}其中$\boldsymbol{x}\in \mathbb{R}^n$。我们假设条件$f_i(\boldsymbol{x})=0$与$h_j(\boldsymbol{x})=0$所构成的定义域集合$\mathcal{D}$是非空的。并且假设$p^{\star}$是最优值。 拉个朗日对偶的基本思想，是将该优化问题增广成目标函数$L(\boldsymbol{x},\boldsymbol{\lambda},\boldsymbol{v})$，其中$L$表示映射$L:\mathbb{R}^n\times \mathbb{R}^m\times \mathbb{R}^p\rightarrow \mathbb{R}$。\begin{align}L(\boldsymbol{x},\boldsymbol{\lambda},\boldsymbol{v})=f_0(\boldsymbol{x})+\sum\limits_{i=1}^m\lambda_if_i(\boldsymbol{x})+\sum\limits_{i=1}^pv_ih_i(\boldsymbol{x})\end{align}定义拉格朗日对偶函数$g:\mathbb{R}^m\times \mathbb{R}^p\rightarrow \mathbb{R}$表示目标函数$L(\boldsymbol{x},\boldsymbol{\lambda},\boldsymbol{v})$关于$\boldsymbol{x}$的下确界\begin{align}g(\boldsymbol{\lambda},\boldsymbol{v})=\inf_{\boldsymbol{x}\in \mathcal{D} } L(\boldsymbol{x},\boldsymbol{\lambda},\boldsymbol{v})\end{align}此时，通过计算可知$g(\boldsymbol{\lambda},\boldsymbol{v})$是最优值$p^{\star}$的下界\begin{align}g(\boldsymbol{\lambda},\boldsymbol{v})\leq p^{\star}\end{align}这个结论很容易得到，假设$\tilde{\boldsymbol{x} }\in \mathcal{D}$，对于$\boldsymbol{\lambda}&gt;0$，有\begin{align}\sum\limits_{i=1}^m\lambda_if_i(\tilde{\boldsymbol{x} })+\sum\limits_{i=1}^pv_ih_i(\tilde{\boldsymbol{x} })\leq 0\end{align}因此，我们可以得到\begin{align}L(\tilde{\boldsymbol{x} },\boldsymbol{\lambda},\boldsymbol{v})=f_0(\tilde{\boldsymbol{x} })+\sum\limits_{i=1}^m\lambda_if_i(\tilde{\boldsymbol{x} })+\sum\limits_{i=1}^pv_ih_i(\tilde{\boldsymbol{x} })\leq f_0(\tilde{\boldsymbol{x} })\end{align}即\begin{align}g(\boldsymbol{\lambda},\boldsymbol{v})=\inf_{\boldsymbol{x}\in \mathcal{D} } L(\boldsymbol{x},\boldsymbol{\lambda},\boldsymbol{v})\leq L(\tilde{\boldsymbol{x} },\boldsymbol{\lambda},\boldsymbol{v})\leq f_0(\tilde{\boldsymbol{x} })\end{align}我们通过图3来进行理解。图中，实曲线表示$f_0(\boldsymbol{x})$曲线，虚曲线表示$f_1(\boldsymbol{x})$，由于限制条件$f_1(\boldsymbol{x})\leq 0$，因此$\boldsymbol{x}$的区间为$[-0.46,+0.46]$。显然该定义区间上的最优解为$\boldsymbol{x}^{\star}=-0.46,p^{\star}=1.54$。图中，带点的虚线族表达$L(\lambda,v)$，$\lambda=0.1,0.2,\cdots,1.0$。我们可以看到，$L(\lambda,v)$相对于$p^{\star}$更小。我们需要调整$\lambda$找到使得$L(\lambda,v)$最大的$p^{\star}$的下确界。 超平面的对偶问题利用拉格朗日乘子法，得到目标函数\begin{align}L(\boldsymbol{w},b,\boldsymbol{\alpha})=\frac{1}{2}||\boldsymbol{w}||^2+\sum\limits_{i=1}^m\alpha_i(1-y_i(\boldsymbol{w}^T\boldsymbol{x}_i+b))\end{align}其中$\alpha_i\geq 0$。令$L(\boldsymbol{w},b,\boldsymbol{\alpha})$对$\boldsymbol{w}$和$b$的偏导数为$0$可得\begin{align}\boldsymbol{w}&amp;=\sum\limits_{i=1}^m \alpha_iy_i\boldsymbol{x}_i\\0&amp;=\sum\limits_{i=1}^m\alpha_iy_i\end{align}代入$L(\boldsymbol{w},b,\boldsymbol{\alpha})$中，再考虑约束条件，有\begin{align}\max_{\boldsymbol{\alpha} }\ &amp;\sum\limits_{i=1}^m\alpha_i-\frac{1}{2}\sum\limits_{i=1}^m\sum\limits_{j=1}^m \alpha_i\alpha_jy_iy_j\boldsymbol{x}_i^T\boldsymbol{x}_j\\\text{s.t.}\ &amp; \sum\limits_{i=1}^m\alpha_iy_i=0\\&amp;\alpha_i\geq 0, \ i=1,\cdots,m\end{align}解出$\boldsymbol{\alpha}$之后，即可得到模型\begin{align}f(\boldsymbol{x})&amp;=\boldsymbol{w}^T\boldsymbol{x}+b\\&amp;=\sum\limits_{i=1}^m\alpha_iy_i\boldsymbol{x}_i^T\boldsymbol{x}+b\end{align}由于该过程为拉格朗日对偶得出的解，因此，所得的解，还需满足KKT条件（found in boyd’s convex optimization）。\begin{align}\left\{\begin{matrix}\alpha_i\geq 0\\y_if(\boldsymbol{x}_i)-1\geq 0\\\alpha_i(y_if(\boldsymbol{x}_i)-1)=0\end{matrix}\right.\end{align} 具体求解$\boldsymbol{\alpha}$的过程，是一个二次规划问题，具体方法如SMO（sequential minimal optimization）。SMO方法，每次更新$\boldsymbol{\alpha}$向量中的两个元素(如，$\alpha_i$和$\alpha_j$)，固定其余参数。利用$\boldsymbol{\alpha}^T\boldsymbol{y}=0$得到$\alpha_i$和$\alpha_j$的更新。 核函数在之前的内容中，我们假设训练样本是线性可分的。然而实际中，在原始样本空间，可能并不存在这样一个能够完全正确划分两类样本的超平面，比如图4所示“异或”问题，通过将二维平面映射到三维空间，我们可以能够切分样本点的超平面。其中，三维平面按照紫色箭头的方向投影，可以得到原始的异或点。 对于这样的问题，通过将样本原始空间映射到高维空间，使得样本在高维空间中线性可分。如果样本空间是有限维的，那么一定存在一个高维空间使得样本可分。我们用$\phi:\mathbb{R}^n\rightarrow \mathbb{R}^p\ (p\gg n)$来表示这样映射，则映射之后的样本表示为$\phi(\boldsymbol{x})$。于是，我们设超平面方程为\begin{align}f(\boldsymbol{x})=\boldsymbol{w}^T\phi(\boldsymbol{x})+b\end{align}其中$(\boldsymbol{w},b)$是模型参数，类似于上一节知识，参数$(\boldsymbol{w},b)$是如下凸优化问题\begin{align}\min\limits_{(\boldsymbol{w},b)}\ &amp;\frac{1}{2}||\boldsymbol{w}||^2\\\text{s.t.} \ &amp; y_i(\boldsymbol{w}^T\phi(\boldsymbol{x}_i)+b\geq 1)\ i=1,\cdots,m\end{align}其对偶问题，为\begin{align}\max_{\boldsymbol{\alpha} } \ &amp;\sum\limits_{i=1}^m\alpha_i-\frac{1}{2}\sum\limits_{i=1}\sum\limits_{j=1}\alpha_i\alpha_jy_iy_j\phi(\boldsymbol{x}_i)^T\phi(\boldsymbol{x}_j)\\\text{s.t.}\ &amp;\sum\limits_{i=1}^m\alpha_iy_i=0\\&amp;\alpha_i\geq0, \ i=1,\cdots,m\end{align}其中$\phi(\boldsymbol{x}_i)^T\phi(\boldsymbol{x}_j)$的计算是非常复杂的，因此我们定义\begin{align}\kappa(\boldsymbol{x}_i\boldsymbol{x}_j)=\phi(\boldsymbol{x}_i)^T\phi(\boldsymbol{x}_j)\end{align}这里我们称$\kappa(\boldsymbol{x}_i\boldsymbol{x}_j)$为“核函数”。类似于上一节的知识，我们可以得到\begin{align}f(\boldsymbol{x})&amp;=\boldsymbol{w}^T\phi(\boldsymbol{x})+b\\&amp;=\sum_{i=1}^m\alpha_iy_i\kappa(\boldsymbol{x},\boldsymbol{x}_i)+b\end{align}该式子表明，模型的最优解可以通过训练样本的核函数展开，这一展开式称为“支持向量展式”（support vecot expansion） 那如何确定核函数或者选择合适的核函数？需要注意的是，在不知道特征映射的形式时，我们并不知道什么样的核函数是合适的，而核函数也仅是隐式地定义了这个特征空间。因此，核函数的选择成为支持向量机最大的变数。常见的核函数有以下几种在定义核函数的基础上，人们发展了一系列基于核函数的学习方法，统称为“核方法”（kernel methods）。最常见的，是通过“核化”（即引入核函数）来将线性学习器扩展为非线性学习器，如核线性判别分析（kernelized linear discriminant analysis, KlDA）。 软间隔在前面的讨论中，我们假设样本是线性可分的。然而，在现实任务中往往很难确定合适的核函数将训练样本再特征空间中线性可分退一步讲，即使恰好找到某个核函数使训练集在特征空间中线性可分，也很难断定这个貌似线性可分的结果，会不会造成过拟合（即，训练集效果好，测试集效果差）。 我们通过放松样本线性可分的条件，允许一些支持向量在少数的样本上出错。为此，引入软间隔的概念。如图5所示，这里不做赘述。 支持向量回归支持向量机所完成的工作是样本分类问题，即，将样本分成两个类，或者三个类。其目的是寻找一个超平面使得样本到超平面的间隔最大。而，支持向量回归，则是通过函数$f(\boldsymbol{x})=\boldsymbol{w}^T\boldsymbol{x}+b$来对样本进行拟合，即我们希望$f(\boldsymbol{x})$与$y$尽可能的靠近。 在回归问题中，我们通常采用模型输出$f(\boldsymbol{x})$和真实输出$y$之间的欧式距离来衡量回归的好坏。当且仅当$f(\boldsymbol{x})$与$y$完全相同，损失才为零。而，支持向量回归（support vector regression, SVR）放松了这个条件，SVR允许$f(\boldsymbol{x})$与$y$之间存在最多为$\epsilon$的误差，这就相当于以$f(\boldsymbol{x})$为中心，构建了如图6的一个宽度为$2\epsilon$的间隔带，若样本落入此间隔带中，则SVR认为是被预测正确的。 于是SVR问题形式化为\begin{align}\min_{\boldsymbol{w},b} \ &amp;\frac{1}{2}||\boldsymbol{w}||^2+C\sum\limits_{i=1}^m\ell_{\epsilon} (f(\boldsymbol{x}_i)-y_i) \\\text{s.t.}\ &amp;f(\boldsymbol{x}_i)-y_i\leq \epsilon\\&amp;y_i-f(\boldsymbol{x}_i)\leq \epsilon\end{align}其中$C(C&gt;0)$是常数，$\ell_{\epsilon}$是不敏感损失函数（$\epsilon$-insensitive loss function）。\begin{align}\ell_{\epsilon}=\left\{\begin{matrix}0 &amp; |z|\leq \epsilon\\|z|-\epsilon &amp;\text{otherwise}\end{matrix}\right.\end{align}通过引入松弛变量(或者称，惩罚因子)$\xi_i$和$\hat{\xi}_i$，则该SVR问题重写为\begin{align}\min_{\boldsymbol{w},b,\xi_i,\hat{\xi}_i} \ &amp;\frac{1}{2}||\boldsymbol{w}||^2+C\sum_{i=1}^m(\xi_i+\hat{\xi}_i)\\\text{s.t.}\ &amp;f(\boldsymbol{x}_i)-y_i\leq \epsilon+\xi_i,\\&amp;y_i-f(\boldsymbol{x}_i)\leq \epsilon+\hat{\xi}_i,\\&amp;\xi\geq 0,\hat{\xi}_i\geq 0,\ i=1\cdots,m\end{align}类似地，我们通过拉格朗日法找到其对偶问题。通过引入拉格朗日乘子$\mu_i\geq 0,\hat{\mu}_i\geq 0,\alpha_i\geq 0,\hat{\alpha}_\geq 0$，由拉格朗日乘子法，可以得到拉格朗日函数\begin{align}L(\boldsymbol{w},b,\boldsymbol{\alpha},\hat{\boldsymbol{\alpha} },\boldsymbol{\xi},\hat{\boldsymbol{\xi} },\boldsymbol{\mu},)&amp;=\frac{1}{2}||\boldsymbol{w}||^2+C\sum\limits_{i=1}^m(\xi_i+\hat{\xi}_i)-\sum\limits_{i=1}^m\mu_i\xi_i-\sum\limits_{i=1}^m\hat{\mu}\hat{\xi}_i\\&amp;\quad +\sum\limits_{i=1}^m \alpha_i(f(\boldsymbol{x}_i)-y_i-\epsilon-\xi_i)+\sum\limits_{i=1}^m\hat{\alpha}_i (y_i-f(\boldsymbol{x}_i)-\epsilon-\hat{\xi}_i)\end{align}令$L(\boldsymbol{w},b,\boldsymbol{\alpha},\hat{\boldsymbol{\alpha} },\boldsymbol{\xi},\hat{\boldsymbol{\xi} },\boldsymbol{\mu},)$对$\boldsymbol{w},b,\xi$和$\hat{\xi}_i$的偏导为$0$，可得\begin{align}\boldsymbol{w}&amp;=\sum\limits_{i=1}^m(\hat{\alpha}_i-\alpha_i)\boldsymbol{x}_i\\0&amp;=\sum\limits_{i=1}^m (\hat{\alpha}_i-\alpha_i)\\C&amp;=\alpha_i+\mu_i\\C&amp;=\hat{\alpha}_i+\hat{\mu}_i\end{align}代入，可得SVR的对偶问题\begin{align}\max_{\boldsymbol{\alpha},\hat{\boldsymbol{\alpha} }} \ &amp; \sum\limits_{i=1}^m y_i(\hat{\alpha}_i-\alpha_i)-\epsilon (\hat{\alpha}_i+\alpha_i)\\&amp;-\frac{1}{2}\sum_{i=1}^m\sum_{j=1}^m(\hat{\alpha}_i-\alpha_i)(\hat{\alpha}_j-\alpha_j)\boldsymbol{x}_i^T\boldsymbol{x}_j\\\text{s.t.} \ &amp;\sum_{i=1}^m (\hat{\alpha}_i-\alpha_i)=0,\\&amp;0\leq \alpha_i,\alpha_j\leq C.\end{align}上述过程还需满足KKT条件，即\begin{align}\left\{\begin{matrix}\alpha_i(f(\boldsymbol{x}_i)-y_i-\epsilon-\xi_i)=0\\\hat{\alpha}_i(y_i-f(\boldsymbol{x}_i)-\epsilon-\hat{\xi}_i)=0\\\alpha_i\hat{\alpha}_0=0,\xi_i\hat{\xi}_i=0\\(C-\alpha_i)\xi_i=0,\ (C-\hat{\alpha}_i)\hat{\xi}_i=0\end{matrix}\right.\end{align}若解出$\boldsymbol{\alpha}和\hat{\boldsymbol{\alpha} }$，最终可得\begin{align}f(\boldsymbol{x})=\sum_{i=1}^m (\hat{\alpha}_i-\alpha_i)\boldsymbol{x}_i^T\boldsymbol{x}+b\end{align}其中$b$可以由KKT条件以及$\boldsymbol{\alpha}$和$\hat{\boldsymbol{\alpha} }$得到。 若考虑特征映射$\phi$，即将样本空间映射到更高维的空间，则相应的形式为\begin{align}f(\boldsymbol{x})=\sum_{i=1}^m(\hat{\alpha}_i-\alpha_i)\kappa(\boldsymbol{x},\boldsymbol{x}_i)+b\end{align}其中$\kappa(\boldsymbol{x}_i,\boldsymbol{x}_j)=\phi(\boldsymbol{x}_i)^T\phi(\boldsymbol{x}_j)$为核函数。]]></content>
      <categories>
        <category>PRML</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[信号系统笔记之连续时间傅里叶级数]]></title>
    <url>%2F2018%2F11%2F06%2F%E4%BF%A1%E5%8F%B7%E7%B3%BB%E7%BB%9F%E7%AC%94%E8%AE%B0%E4%B9%8B%E8%BF%9E%E7%BB%AD%E6%97%B6%E9%97%B4%E5%82%85%E9%87%8C%E5%8F%B6%E7%BA%A7%E6%95%B0%2F</url>
    <content type="text"><![CDATA[信号的正交分解矢量的几何分解为了理解对信号进行分解的目的，我们先从几何学的角度，回味下平面矢量以及空间矢量的分解。如图1所示，(a)$\overrightarrow{A}=c_1\overrightarrow{v_x}+c_2\overrightarrow{v_y}$，即将平面矢量分解成正交的$x$轴和$y$轴的单位矢量；（b）$\overrightarrow{A}=c_1\overrightarrow{v_x}+c_2\overrightarrow{v_y}+c_3\overrightarrow{v_z}$，即将空间矢量分解成正交的$x$轴、$y$轴和$z$轴的单位矢量。 为此，我们先理清楚什么是矢量正交？ 矢量正交的定义若有$\overrightarrow{A}\cdot \overrightarrow{B}=0$，则矢量$\overrightarrow{A}$与$\overrightarrow{B}$正交。 为什么要对空间矢量进行正交分解？很明显，方便分析！简化计算！为了方便理解，我们举一个高中牛顿力学的一个例子，如图2所示，固定斜面上有一小球，小球与斜面的接触面绝对光滑，现在要研究小球沿斜面的加速度这样求得的斜面加速度为\begin{align}g=\sin \theta\end{align}可以看到通过矢量的正交分解，我们可以很轻松的求出小球沿斜面的加速度。将空间矢量正交分解的概念推广到信号空间，在信号空间中找到若干个相互正交的信号为基本信号，使得任意信号可以表示成这组正交的信号集合的线性组合。 正交信号定义：在区间$(t_1,t_2)$上的两个信号$\phi_1(t)$和$\phi_2(t)$，若满足\begin{align}\int_{t_1}^{t_2}\phi_1(t)\phi_2(t)=0\end{align}则称$\phi_1(t)$和$\phi_2(t)$在区间$(t_1,t_2)$上正交。 正交函数集：如果有$n$个函数$\left\{\phi_1(t),\cdots,\phi_n(t)\right\}$构成一个函数集合，若函数集合在区间$(t_1,t_2)$上满足\begin{align}\int_{t_1}^{t_2}\phi_1(t)\phi_2(t)\text{d}t=\left\{\begin{matrix}0 &amp;i\ne j\\K &amp;i=j\end{matrix}\right.\end{align}其中$K$为常数，则称此函数集为正交函数集。 完备正交函数集：定义集合$\mathcal{S}=\left\{\phi_1(t),\cdots,\phi_n(t)\right\}$是区间$(t_1,t_2)$上的正交函数集，如果除$\mathcal{S}$外，不存在函数$\phi(t)$满足等式\begin{align}\int_{t_1}^{t_2} \phi_i(t)\phi(t)=0\quad \forall i=\left\{1,\cdots,n\right\}\end{align}则称此函数集为完备正交函数集。我们称该完备集合中的函数$\phi_j(t)$为基或者基底。常见的完备正交函数集合有三角函数集、虚指数函数集。 Example: 证明三角函数集$\left\{1,\cos(nw_0t),\sin(nw_0t)\right\},(n=1,2,\cdots)$是正交函数集合证：\begin{align}\int_{t_0}^{t_0+T}\cos(nw_0t)\cos(mw_0t)\text{d}t&amp;=\left\{\begin{matrix}0 &amp;m\ne n\\\frac{T}{2} &amp;m=n\ne 0\\T &amp;m=n=0\end{matrix}\right.\\\int_{t_0}^{t_0+T}\sin(nw_0t)\sin(mw_0t)\text{d}t&amp;=\left\{\begin{matrix}0 &amp;m\ne n\\\frac{T}{2} &amp;m=n\ne 0\end{matrix}\right.\\\int_{t_0}^{t_0+T}\sin(nw_0t)\cos(mw_0t)\text{d}t&amp;=0\end{align}因此三角函数集为正交函数集。【注】如果函数$f(t)$是周期为$T$的周期信号，则有$\int_{a}^{T+a}f(t)\text{d}t=\int_{0}^{T}f(t)\text{d}t$。 信号的正交分解设$n$个函数$\phi_1(t),\cdots,\phi_n(t)$在区间$(t_1,t_2)$构成一个正交函数集$\mathcal{S}$。将任意信号$f(t)$表示成这$n$个函数的线性组合来近似，可表示为\begin{align}f(t)\approx a_1\phi_1(t)+\cdots a_n\phi_n(t)=\sum\limits_{i=1}^na_i\phi_i(t)\end{align}为此，我们需要确定$a_i (i=1,\cdots n)$的具体取值，来确保$\sum\limits_{i=1}^na_i\phi_i(t)$是对$f(t)$的最佳近似。我们选取均方误差（mean square error, MSE）来衡量这个近似的效果\begin{align}\text{MSE}=\frac{1}{t_2-t_1}\int_{t_1}^{t_2}\left(f(t)-\sum\limits_{i=1}^na_i\phi_i(t)\right)^2\text{d}t\end{align}为使得MSE最小，计算MSE对$a_j$的偏导如下\begin{align}\frac{\partial \text{MSE} }{\partial a_j}&amp;=\frac{1}{t_2-t_1}\frac{\partial }{\partial a_j}\int_{t_1}^{t_2}\left(f(t)-\sum\limits_{i=1}^na_i\phi_i(t)\right)^2\text{d}t\\&amp;\overset{(a)}{=}\frac{1}{t_2-t_1}\int_{t_1}^{t_2}\frac{\partial }{\partial a_j}\left(f(t)-\sum\limits_{i=1}^na_i\phi_i(t)\right)^2\text{d}t\\&amp;=-\frac{1}{t_2-t_1}\int_{t_1}^{t_2}2\left(f(t)-\sum\limits_{i=1}^na_i\phi_i(t)\right)\phi_j(t)\text{d}t\\&amp;\overset{(b)}{=}\frac{2a_j}{t_2-t_1}\int_{t_1}^{t_2}\phi_j^2(t)\text{d}t-\frac{2}{t_2-t_1}\int_{t_1}^{t_2}f(t)\phi_j(t)\text{d}t\end{align}其中步骤$(a)$成立，假设被积函数性质足够好，使得积分和偏导顺序可以交换；步骤$(b)$成立，利用完备正交函数集合，函数正交的性质。令偏导数为零，得到\begin{align}a_j=\frac{\int_{t_1}^{t_2}f(t)\phi_j(t)\text{d}t}{\int_{t_1}^{t_2}\phi_j^2(t)\text{d}t} \quad i=(1,\cdots,n)\end{align}定义$K_j=\int_{t_1}^{t_2}\phi_j^2(t)\text{d}t$，则参数$a_j$可以表示为\begin{align}a_j=\frac{1}{K_j}\int_{t_1}^{t_2}f(t)\phi_j(t)\text{d}t\end{align}到这里，我们会发现，信号的正交分解，跟矢量投影很相似。将矢量$\overrightarrow{A}$投影到矢量$\overrightarrow{a}$上，其投影长度$\frac{\overrightarrow{A}\cdot \overrightarrow{a} }{|\overrightarrow{a}|}$。注意这里所得到的$a_j$表达式是基于均方误差最小准则得到的，均方误差刻画的是真实值和近似值的欧式距离，根据不同的规则，可以得到不同的解。 连续时间傅里叶级数的三角级数形式根据上一节知识，我们尝试将信号$f(t)$分解到三角函数集$\mathcal{S}=\left\{1,\cos(nw_0t),\sin(nw_0t)\right\},(n=1,\cdots)$上，即将$f(t)$表示成该集合中基的线性组合，如下\begin{align}f(t)=\frac{a_0}{2}+\sum\limits_{n=1}^{+\infty}a_n\cos(nw_0t)+\sum\limits_{n=1}^{+\infty}b_n\sin (nw_0t)\end{align}为了计算$a_n$ 利用信号正交特性。利用信号正交特性，对等式两边同时乘上$\cos(nw_0t)$项，并在一个周期内进行积分，得到\begin{align}\int_T f(t)\cos(nw_0t)\text{d}t&amp;=\int_T a_n\cos(nw_0t)\cos(nw_0t)\text{d}t\\&amp;=a_n\int _T\frac{1+\cos(2nw_0t)}{2}\text{d}t\\&amp;=\frac{a_nT}{2}\end{align}因此，得到\begin{align}a_n=\frac{2}{T}\int_T f(t)\cos(nw_0t)\text{d}t\end{align}同理，在等式两边乘上$\sin (nw_0t)$项，可以到$b_n$的表达式如下\begin{align}b_n=\frac{2}{T}\int_T f(t)\sin(nw_0t)\text{d}t\end{align} 利用从均方误差最小得到的公式：$a_j=\frac{\int_{t_1}^{t_2}f(t)\phi_j(t)\text{d}t}{\int_{t_1}^{t_2}\phi_j^2(t)\text{d}t}$，来进行求解\begin{align}a_n&amp;=\frac{\int_{t_1}^{t_2}f(t)\cos(nw_0t)\text{d}t}{\int \cos^2(nw_0t)\text{d}t}=\frac{2}{T}\int_Tf(t)\cos(nw_0t)\text{d}t\\b_n&amp;=\frac{\int_{t_1}^{t_2}f(t)\sin(nw_0t)\text{d}t}{\int \sin^2(nw_0t)\text{d}t}=\frac{2}{T}\int_Tf(t)\sin(nw_0t)\text{d}t\\\end{align}我们惊喜的发现：最佳近似估计的系数和傅里叶三角系数是相等的，也就是说傅里叶级数是一种对周期信号的最佳近似！ 合并同频率的$\cos(nw_0t),\sin(nw_0t)$，如下\begin{align}f(t)=\frac{A_0}{2}+\sum\limits_{n=1}^{+\infty}A_n \cos(nw_0t+\psi_n)\end{align}其中\begin{align}\left\{\begin{matrix}A_0=a_0\qquad \quad \quad \quad\\A_n=\sqrt{a_n^2+b_n^2}\quad \quad\\\psi_n=-\arctan (\frac{b_n}{a_n})\end{matrix}\right.\end{align} 连续时间傅里叶级数的虚指数形式傅里叶级数是对周期信号的最佳近似，做这种近似的目的就是为了方便计算和分析信号，但是在实际信号分析中，使用三角级数计算比较麻烦。因此通过欧拉公式将傅里叶级数的三角形式转换成指数形式。\begin{align}\cos(t)=\frac{e^{jt}+e^{-jt} }{2}\end{align}应用欧拉公式，信号$f(t)$表示为\begin{align}f(t)&amp;=\frac{A_0}{2}+\sum\limits_{n=1}^{+\infty}\frac{A_n}{2}\left(e^{j(nw_0t+\psi_n)}+e^{-j(nw_0t+\psi_n)}\right)\\&amp;=\frac{A_0}{2}+\sum\limits_{n=1}^{+\infty}\frac{A_n}{2}e^{j(nw_0t+\psi_n)}+\sum\limits_{n=1}^{+\infty}\frac{A_n}{2}e^{-j(nw_0t+\psi_n)}\\&amp;=\frac{A_0}{2}+\sum\limits_{n=1}^{+\infty}\frac{A_n}{2}e^{j(nw_0t+\psi_n)}+\sum\limits_{n=-1}^{-\infty}\frac{A_{-n} }{2}e^{jnw_0t}e^{-j\psi_{-n} }\end{align}由于$A_n=\sqrt{a_n^2+b_n^2}$是关于$n$的偶函数，$\psi_n$是关于$n$的奇函数，因此有\begin{align}f(t)=\frac{A_0}{2}+\sum\limits_{n=1}^{+\infty}\frac{A_n}{2}e^{j(nw_0t+\psi_n)}+\sum\limits_{n=-1}^{-\infty}\frac{A_n}{2}e^{j(nw_0t+\psi_n)}\end{align}令$A_n|_{n=0}=A_0$，因此有\begin{align}f(t)=\sum\limits_{n=-\infty}^{+\infty}\frac{A_n}{2}e^{j\psi_n}e^{jnw_0t}\end{align}令$F_n=\frac{A_n}{2}e^{j\psi_n}$，即得到信号的傅里叶级数的虚指数形式的表达式\begin{align}f(t)=\sum\limits_{n=-\infty}^{+\infty}F_ne^{jnw_0t}\end{align}类似地，这里$F_n$的表达式也可以利用信号正交或者均方误差最小的方式来求解，得到\begin{align}F_n=\frac{1}{T}\int_T f(t)e^{-jnw_0t}\text{d}t\end{align}]]></content>
      <categories>
        <category>信号系统</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[深度学习笔记之人工智能、机器学习、深度学习之间的关系]]></title>
    <url>%2F2018%2F11%2F03%2F%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0%E4%B9%8B%E4%BA%BA%E5%B7%A5%E6%99%BA%E8%83%BD%E3%80%81%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E3%80%81%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E4%B9%8B%E9%97%B4%E7%9A%84%E5%85%B3%E7%B3%BB%2F</url>
    <content type="text"><![CDATA[这两年创业圈、技术圈、互联网圈都在热烈讨论人工智能、机器学习、深度学习。那么到底什么是人工智能（AI）、机器学习（ML）和深度学习（DL），这几个概念之间又有什么样的联系呢？首先，我们通过图来理解这三者之间的关系 机器学习是实现人工智能的方法；深度学习是实现机器学习的一种技术。 人工智能（Artificial intelligence, AI）: 是指人工制造出来的系统所表现出来的智能。通常人工智能是指通过普通电脑实现的智能。人工智能的研究可以分为几个技术问题。其分支领域主要集中在解决具体问题，其中之一是，如何利用各种不同的工具完成特定的应用程序。AI的核心问题包括推理、知识、规划、学习、交流、感知、移动和操作物体的能力等。 目前有大量的工具应用了人工智能，其中包括搜索和数学优化、逻辑推演。而基于仿生学、认知心理学，以及基于概率论和经济学的算法等等也在逐步探索当中。 机器学习（Machine Learning, ML）：是人工智能的一个分支。人工智能的研究是从以“推理”为重点到“知识”为重点，再到以“学习”为重点，一条自然、清晰的脉络。显然，机器学习是实现人工智能的一个途径，即以机器学习为手段解决人工智能重的问题。机器学习在近30年已经发展为一门多领域交叉学科，设计概率论、统计学、逼近轮、凸优化、计算复杂性理论等多门学科。机器学习理论主要是设计和分析一些让计算机可以自动“学习”的算法。机器学习算法是一类从数据中自动分析获得规律，并利用规律对未知数据进行预测的算法。因为学习算法中设计到了大量的统计学理论，机器学习与腿短统计学联系尤为密切，也被成为统计学习理论。算法设计方面，机器学习理论关注可以实现的，行之有效的学习算法。很多推论问题属于无程序难处理，所以部分机器学习研究是开发容易处理的近似算法。 机器学习有以下几种定义：（1）机器学习是一门人工智能的科学，该领域的主要研究对象是人工智能，特别是处理在经验学习中改善具体算法的性能。（2）机器学习是对能通过经验自动改进的计算机算法的研究。（3）机器学习是用数据或以往的经验，以此优化计算机程序的性能标准。 机器学习已广泛应用于数据挖掘、计算机视觉、自然语言处理、生物特征识别、搜索引擎、医学诊断、检测信号卡欺诈、证券市场分析、DNA序列测序、语音和手写识别、战略游戏和机器人等领域。 深度学习（Deep Learning）是机器学习拉出的分支，它试图使用包含复杂结构或由多重非线性变换构成的多个处理层对数据进行高层抽象的算法。 深度学习是机器学习中一种基于对数据进行表征学习的方法。观测值（例如：一幅画）可以使用多种方法来表示，如每个像素强度值得向量，或者更抽象地表示成一系列边、特定形状的区域等。而使用某些特定的表示方法更容易从实例中学习任务（例如：人脸识别或面部表情识别）。深度学习的好处是用无监督式或半监督的特征学习和分层特征提取搞笑算法来替代手工获取特征。]]></content>
      <categories>
        <category>深度学习</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[多元实高斯分布对协方差的偏导]]></title>
    <url>%2F2018%2F10%2F30%2F%E5%A4%9A%E5%85%83%E5%AE%9E%E9%AB%98%E6%96%AF%E5%88%86%E5%B8%83%E5%AF%B9%E5%8D%8F%E6%96%B9%E5%B7%AE%E7%9F%A9%E9%98%B5%E7%9A%84%E5%81%8F%E5%AF%BC%2F</url>
    <content type="text"><![CDATA[假设$N$维实随机矢量$\mathbf{x}$服从均值为$\mathbf{a}$，协方差为$\mathbf{A}$的高斯分布，记作$\mathbf{x}\sim \mathcal{N}(\mathbf{x}|\mathbf{a},\mathbf{A})$\begin{align}\mathcal{N}\left({\mathbf{z}|\mathbf{a},\mathbf{A} }\right)=(2\pi)^{-\frac{N}{2} }|\mathbf{A}|^{-\frac{1}{2} }\exp \left({-\frac{1}{2}(\mathbf{x}-\mathbf{a})^T\mathbf{A}^{-1}(\mathbf{x}-\mathbf{a})}\right)\end{align} 其对协方差矩阵的偏导为\begin{align}\frac{\partial \mathcal{N}(\mathbf{x}|\mathbf{a},\mathbf{A})}{\partial \mathbf{A} }&amp;=(2\pi)^{-\frac{N}{2} }\frac{\partial |\mathbf{A}|^{-\frac{1}{2} } }{\partial \mathbf{A} }\exp \left(-\frac{1}{2}(\mathbf{x}-\mathbf{a})^T\mathbf{A}^{-1}(\mathbf{x}-\mathbf{a})\right)\\&amp;\quad +(2\pi)^{-\frac{N}{2} }|\mathbf{A}|^{-\frac{1}{2} }\frac{\partial }{\partial \mathbf{A} }\exp \left[-\frac{1}{2}(\mathbf{x}-\mathbf{a})^T\mathbf{A}^{-1}(\mathbf{x}-\mathbf{a})\right]\\&amp;\overset{(a)}{=}-\frac{1}{2}\mathbf{A}^{-1}\mathcal{N}(\mathbf{z}|\mathbf{a},\mathbf{A})+\frac{1}{2}\boldsymbol{A}^{-1}(\mathbf{x}-\mathbf{a})(\mathbf{x}-\mathbf{a})^T\mathbf{A}^{-1}\mathcal{N}(\mathbf{z}|\mathbf{a},\mathbf{A})\end{align}这里最为主要的是偏导$\frac{\partial |\mathbf{A}|^{-\frac{1}{2} }}{\partial \mathbf{A} }$和$\frac{\partial }{\partial \mathbf{A} }\exp \left({-\frac{1}{2}(\mathbf{x}-\mathbf{a})^T\mathbf{A}^{-1}(\mathbf{x}-\mathbf{a})}\right)$的计算。我们给出其详细计算过程如下\begin{align}\frac{\partial |\mathbf{A}|^{-\frac{1}{2} } }{\partial \mathbf{A} }=-\frac{1}{2}|\mathbf{A}|^{-\frac{3}{2} } (|\mathbf{A}|\mathbf{A}^{-1})=-\frac{1}{2}|\mathbf{A}|^{-\frac{1}{2} }\mathbf{A}^{-1}\end{align}这里利用到偏导数公式$\frac{\partial |\mathbf{A}|}{\partial \mathbf{A} }=|\mathbf{A}|\mathbf{A}^{-1}$。 另外\begin{align}&amp;\quad \frac{\partial }{\partial \mathbf{A} }\exp \left[-\frac{1}{2}(\mathbf{x}-\mathbf{a})^T\mathbf{A}^{-1}(\mathbf{x}-\mathbf{a})\right]\\&amp;=\exp\left(-\frac{1}{2}(\mathbf{x}-\mathbf{a})^T\mathbf{A}^{-1}(\mathbf{x}-\mathbf{a})\right)\left(-\frac{1}{2}\frac{\partial }{\partial \mathbf{A} }(\mathbf{x}-\mathbf{a})^T\mathbf{A}^{-1}(\mathbf{x}-\mathbf{a})\right)\end{align}其中\begin{align}\frac{\partial (\mathbf{x}-\mathbf{a})^T\mathbf{A}^{-1}(\mathbf{x}-\mathbf{a})}{\partial \mathbf{A} }=\left({\begin{matrix}\frac{\partial (\mathbf{x}-\mathbf{a})^T\mathbf{A}^{-1}(\mathbf{x}-\mathbf{a})}{\partial A_{11} }&amp; \cdots&amp; \frac{\partial (\mathbf{x}-\mathbf{a})^T\mathbf{A}^{-1}(\mathbf{x}-\mathbf{a})}{A_{1N} }\\\vdots&amp; \ddots&amp; \vdots\\\frac{\partial (\mathbf{x}-\mathbf{a})^T\mathbf{A}^{-1}(\mathbf{x}-\mathbf{a})}{\partial A_{N1} }&amp; \cdots&amp; \frac{\partial (\mathbf{x}-\mathbf{a})^T\mathbf{A}^{-1}(\mathbf{x}-\mathbf{a})}{\partial A_{NN} }\end{matrix}}\right)\end{align}计算该矩阵元素如下\begin{align}\frac{\partial (\mathbf{x}-\mathbf{a})^T\mathbf{A}^{-1}(\mathbf{x}-\mathbf{a})}{\partial A_{ij} }&amp;=\text{tr}\left\{ {\frac{\partial (\mathbf{x}-\mathbf{a})^T\mathbf{A}^{-1}(\mathbf{x}-\mathbf{a})}{\partial \mathbf{A}^{-1} }\frac{\partial \mathbf{A}^{-1} }{\partial A_{ij} } }\right\}\\&amp;\overset{(b)}{=}\text{tr}\left\{ {-(\mathbf{x}-\mathbf{a})(\mathbf{x}-\mathbf{a})^T\mathbf{A}^{-1}\frac{\partial \mathbf{A} }{\partial A_{ij} }\mathbf{A}^{-1} }\right\}\\&amp;=\text{tr}\left\{ {-(\mathbf{x}-\mathbf{a})(\mathbf{x}-\mathbf{a})^T\mathbf{A}^{-1}\boldsymbol{e}_i\boldsymbol{e}_j^T\mathbf{A}^{-1} }\right\}\\&amp;=-\boldsymbol{e}_j^T\mathbf{A}^{-1}(\mathbf{x}-\mathbf{a})(\mathbf{x}-\mathbf{a})^T\mathbf{A}^{-1}\boldsymbol{e}_i\end{align}这里步骤$(b)$利用到偏导公式$\frac{\partial g(\mathbf{U})}{\partial x}=\text{tr}\left\{ {\frac{\partial g(\mathbf{U})}{\partial \mathbf{U} }\frac{\partial \mathbf{U} }{\partial x} }\right\}$以及$\frac{\partial \mathbf{U}^{-1} }{\partial x}=-\mathbf{U}^{-1}\frac{\partial \mathbf{U} }{\partial x}\mathbf{U}^{-1}$。因此，可以得到\begin{align}\frac{\partial (\mathbf{x}-\mathbf{a})^T\mathbf{A}^{-1}(\mathbf{x}-\mathbf{a})}{\partial \mathbf{A} }=-(\boldsymbol{A}^{-1})^T(\mathbf{x}-\mathbf{a})(\mathbf{x}-\mathbf{a})^T(\boldsymbol{A}^{-1})^T\end{align}若假设$\mathbf{A}$是对称矩阵, 则有\begin{align}\frac{\partial (\mathbf{x}-\mathbf{a})^T\mathbf{A}^{-1}(\mathbf{x}-\mathbf{a})}{\partial \mathbf{A} }=-\boldsymbol{A}^{-1}(\mathbf{x}-\mathbf{a})(\mathbf{x}-\mathbf{a})^T\mathbf{A}^{-1}\end{align}]]></content>
      <categories>
        <category>基础数学</category>
      </categories>
      <tags>
        <tag>Gaussian</tag>
      </tags>
  </entry>
  <entry>
    <title></title>
    <url>%2F2018%2F10%2F29%2F%E7%9F%A9%E9%98%B5%E5%A5%87%E5%BC%82%E5%80%BC%E5%88%86%E8%A7%A3%2F</url>
    <content type="text"><![CDATA[对于线性高斯模型\begin{align}\boldsymbol{y}=\boldsymbol{Hx}+\boldsymbol{w}\end{align}其中$\boldsymbol{x}\in \mathbb{R}^N$为待估计变量，其概率密度为$\boldsymbol{X}\sim p_{\boldsymbol{X} }(\boldsymbol{x})$。$\boldsymbol{w}$是高斯白噪声，即$\boldsymbol{w}\sim \mathcal{N}(\boldsymbol{w}|\boldsymbol{a},\boldsymbol{C}_{\boldsymbol{w} })$。信号估计所要完成的是，根据已知的模型信息，从观测向量$\boldsymbol{y}\in \mathbb{R}^M$中恢复出原始信号$\boldsymbol{x}$。为了得到确定解，一般$\boldsymbol{y}$的维度大于$\boldsymbol{x}$的维度，即模型为超定方程。 最小二乘法 （Least Square, LS）$\boldsymbol{x}$的最小二乘估计，通过最小化如下损失函数得到\begin{align}J=|\boldsymbol{y}-\boldsymbol{Hx}|^2\end{align}由于该损失函数是凸函数，因此我们通过计算损失函数对$\boldsymbol{x}$的导数\begin{align}\frac{\partial J}{\partial \boldsymbol{x} }=-2\boldsymbol{y}^T\boldsymbol{H}+2\boldsymbol{x}^T\boldsymbol{H}^T\boldsymbol{H}\end{align}通过令导数为零，得到该模型的最小二乘估计\begin{align}\boldsymbol{x}_{\text{LS} }=(\boldsymbol{H}^T\boldsymbol{H})^{-1}\boldsymbol{H}^T\boldsymbol{y}\end{align} 几何解释: 如图所示，由于$\boldsymbol{H}$所构成的超平面用$\mathcal{C}$表示，最小化$J=|\boldsymbol{y}-\boldsymbol{Hx}|^2$所描述的是，找到$\boldsymbol{y}$在超平面$\mathcal{C}$上的正交投影。 Remarks: 最小二乘的优势在于，算法结构简单。其缺陷在于，由于忽略了噪声的存在，因此当噪声很大的时候，其估计性能极差。 最大似然估计（Maximum likelihood, ML）似然函数的定义（摘自Wiki Pedia）： In frequentist inference, a likelihood function (often simply the likelihood) is a function of the parameters of a statistical model, given specific observed data. Likelihood functions play a key role in frequentist inference, especially methods of estimating a parameter from a set of statistics. In informal contexts, “likelihood” is often used as a synonym for “probability”. In mathematical statistics, the two terms have different meanings. Probability in this mathematical context describes the plausibility of a random outcome, given a model parameter value, without reference to any observed data. Likelihood describes the plausibility of a model parameter value, given specific observed data.在概率推论中，一个似然函数（简称似然）是给定明确的观测数据，关于一个统计模型的参数的函数。似然函数在概率推论中扮演着重要的角色，尤其是从一组统计数据中估计参数。在非正式的文献中，似然函数通常被认为是“概率”。在统计数学中，这两者有不同的含义。在数学文献中，概率描述的是给定模型参数值下一个随机输出的可能性，没有参考任何观测数据。似然函数描述的是给定具体观测数据，模型参数值得可能性。 Following Bayes’ Rule, the likelihood when seen as a conditional density can be multiplied by the prior probability density of the parameter and then normalized, to give a posterior probability density.根据贝叶斯公式，似然函数被看作是条件概率，可以乘上先验概率然后归一化得到后验概率。 对于线性高斯模型$\boldsymbol{y}=\boldsymbol{Hx}+\boldsymbol{w}$，为了方便计算，这里我们设$\boldsymbol{w}\sim \mathcal{N}(\boldsymbol{0},\sigma^2\mathbf{I})$，则该模型的其似然函数为\begin{align}L(\boldsymbol{x})&amp;=p(\boldsymbol{y}|\boldsymbol{x})=\mathcal{N}(\boldsymbol{y}|\boldsymbol{Hx},\sigma^2\mathbf{I})\\&amp;=(2\pi\sigma^2)^{-\frac{M}{2} }\exp \left(-\frac{1}{2\sigma^2}(\boldsymbol{y}-\boldsymbol{Hx})^T(\boldsymbol{y}-\boldsymbol{Hx})\right)\end{align}等式两边取对数，有\begin{align}\ell(\boldsymbol{x})=\ln L(\boldsymbol{x})=-\frac{1}{2\sigma^2}(\boldsymbol{y}-\boldsymbol{Hx})^T(\boldsymbol{y}-\boldsymbol{Hx})-\frac{M}{2}\log (2\pi\sigma^2)\end{align}计算对数似然函数关于$\boldsymbol{x}$的偏导数，有\begin{align}\frac{\partial \ell(\boldsymbol{x})}{\partial \boldsymbol{x} }=-\frac{1}{2\sigma^2}(2\boldsymbol{y}^T\boldsymbol{H}-2\boldsymbol{x}^T\boldsymbol{H}^T\boldsymbol{H})=0 \ \Rightarrow \boldsymbol{x}_{\text{ML} }=(\boldsymbol{H}^T\boldsymbol{H})^{-1}\boldsymbol{H}\boldsymbol{y}\end{align}因此，我们发现，线性高斯模型的最大似然解和最小二乘解一致。 最小均方误差估计（Minimum mean square error, MMSE）定义如下贝叶斯均方误差（Bayesian mean square error, Bmse）\begin{align}\text{Bmse}(\hat{\boldsymbol{x} })=\mathbb{E}\left\{(\boldsymbol{x}-\hat{\boldsymbol{x} })^2\right\}=\int (\boldsymbol{x}-\hat{\boldsymbol{x} })^2p(\boldsymbol{x},\boldsymbol{y})\text{d}\boldsymbol{x}\text{d}\boldsymbol{y}\end{align}最小均方误差估计量，即寻找使得贝叶斯均方误差最小的$\boldsymbol{x}$\begin{align}\hat{\boldsymbol{x} }&amp;=\underset{\boldsymbol{x} }{\arg \min} \int \left[\int (\boldsymbol{x}-\hat{\boldsymbol{x} })^2p(\boldsymbol{x}|\boldsymbol{y})\text{d}\boldsymbol{x}\right]p(\boldsymbol{y})\text{d}\boldsymbol{y}\\&amp;=\underset{\boldsymbol{x} }{\arg \min}\int (\boldsymbol{x}-\hat{\boldsymbol{x} })^2p(\boldsymbol{x}|\boldsymbol{y})\text{d}\boldsymbol{x}\end{align}计算其导数\begin{align}\frac{\partial }{\partial \boldsymbol{x} }\int (\boldsymbol{x}-\hat{\boldsymbol{x} })^2p(\boldsymbol{x}|\boldsymbol{y})\text{d}\boldsymbol{x}&amp;=2\int (\boldsymbol{x}-\hat{\boldsymbol{x} })p(\boldsymbol{x}|\boldsymbol{y})\text{d}\boldsymbol{x}\\&amp;=2\int \boldsymbol{x}p(\boldsymbol{x}|\boldsymbol{y})\text{d}\boldsymbol{x}-\hat{\boldsymbol{x} }\end{align}注意$\hat{\boldsymbol{x} }$是关于$\boldsymbol{y}$的函数。令导数为0，有\begin{align}\hat{\boldsymbol{x} }_{\text{MMSE} }=\int \boldsymbol{x} p(\boldsymbol{x}|\boldsymbol{y})\text{d}\boldsymbol{x}=\mathbb{E}\left[\boldsymbol{x}|\boldsymbol{y}\right]\end{align} Remarks: 最小均方误差估计器，被称为后验均值估计，也就是选取后验概率的均值作为$\boldsymbol{x}$的估计值。因此，最小均方误差估计器最为核心之处，在于计算后验概率$p(\boldsymbol{x}|\boldsymbol{y})$。根据贝叶斯公式\begin{align}p(\boldsymbol{x}|\boldsymbol{y})=\frac{p(\boldsymbol{x},\boldsymbol{y})}{p(\boldsymbol{y})}=\frac{p(\boldsymbol{y}|\boldsymbol{x})p(\boldsymbol{x})}{p(\boldsymbol{y})}\end{align}这里我们仅需要求$p(\boldsymbol{y}|\boldsymbol{x})p(\boldsymbol{x})$，而$p(\boldsymbol{y})$可以通过归一化来实现。\begin{align}\hat{\boldsymbol{x} }=\left[\begin{matrix}\hat{x}_1\\\vdots\\\hat{x}_N\end{matrix}\right]=\left[\begin{matrix}\int x_1p(x_1|\boldsymbol{y})\text{d}x_1\\\vdots\\\int x_1p(x_N|\boldsymbol{y})\text{d}x_N\end{matrix}\right]\end{align}因此，我们可以知道，最小均方误差真正的难点在于，求边缘后验概率\begin{align}p(x_i|\boldsymbol{y})=\int_{\boldsymbol{x}_{\backslash i} } p(\boldsymbol{x}|\boldsymbol{y})\text{d}\boldsymbol{x}_{\backslash i}\end{align}其中$\boldsymbol{x}_{\backslash i}$表示除了第$i$个元素外，$\boldsymbol{x}$中其余元素所构成的向量。 最小均方误差估计器，是贝叶斯最优的。因为，最小均方误差估计器选取使得贝叶斯均方误差最小的$\boldsymbol{x}$作为估计器。 当先验概率是高斯的时候，根据高斯相乘引理，我们可以写出线性高斯模型的MMSE估计器的解析表达式。 通常先验概率是非高斯的，此时，我们不能写出MMSE估计器的解析表达式。一种方法是，退而求其次，通过限定待估计量与观测值呈线性关系，即LMMSE估计器；另一种方法是通过因子图的角度出发，利用近似消息传递（approximate message passing, AMP）[1][2]类算法或者期望传播（Expectation propagation, EP）[3]类算法，来迭代得到估计量的MMSE解。注意，不管是AMP族算法还是EP族算法，其本质上是计算边缘后验概率。 线性最小均方误差估计 (Linear minmum mean square error, LMMSE)线性最小均方误差估计，通过假设估计器的模型为$\boldsymbol{y}$的线性模型，并使得均方误差最小，来得到估计器的表达式\begin{align}\hat{\boldsymbol{x} }=\boldsymbol{A}\boldsymbol{y}+\boldsymbol{b}\end{align}为了得到$\boldsymbol{x}$的表达式，我们需要进一步确定$\boldsymbol{A}$和$\boldsymbol{b}$。定义如下贝叶斯均方误差（Bayesian mean square error, BMSE）\begin{align}\text{Bmse}(\hat{\boldsymbol{x} })=\mathbb{E}\left\{||\boldsymbol{x}-\hat{\boldsymbol{x} }||^2\right\}\end{align}这里的期望是对联合概率$p(\boldsymbol{x},\boldsymbol{y})$求。 $\underline{\text{Step 1} }$：为求$\hat{\boldsymbol{x} }=[\hat{x}_1,\cdots,\hat{x}_N]^T$，我们首先考虑一维的情况，即\begin{align}\hat{x}=\boldsymbol{a}^T\boldsymbol{y}+b\end{align}其对应的贝叶斯均方误差为\begin{align}\text{Bmse}(\hat{\boldsymbol{x} })=\mathbb{E}\left\{(x-\hat{x})^2\right\}\end{align}其中期望对$p(x,\boldsymbol{y})$取。 $\underline{\text{Step 2} }$: 求$b$。计算贝叶斯均方误差对$b$的偏导，有\begin{align}\frac{\partial }{\partial b}\mathbb{E}\left\{(x-\boldsymbol{a}^T\boldsymbol{y}-b)^2\right\}=-2\mathbb{E}\left\{x-\boldsymbol{a}^T\boldsymbol{y}-b\right\}\end{align}令偏导为0，得到\begin{align}b=\mathbb{E}[x]-\boldsymbol{a}^T\mathbb{E}[\boldsymbol{y}]\end{align} $\underline{\text{Step 3} }$：计算$\boldsymbol{a}$。计算贝叶斯均方误差如下\begin{align}\text{Bmse}(\hat{x})&amp;=\mathbb{E}\left\{(x-\boldsymbol{a}^T\boldsymbol{y}-\mathbb{E}[x]+\boldsymbol{a}^T\mathbb{E}[\boldsymbol{y}])^2\right\}\\&amp;=\mathbb{E}\left\{\left[\boldsymbol{a}^T(\boldsymbol{y}-\mathbb{E}[\boldsymbol{y}])-(x-\mathbb{E}[x])\right]^2\right\}\\&amp;=\mathbb{E}\left\{\boldsymbol{a}^T(\boldsymbol{y}-\mathbb{E}[\boldsymbol{y}])(\boldsymbol{y}-\mathbb{E}[\boldsymbol{y}])^T\boldsymbol{a}\right\}-\mathbb{E}\left\{\boldsymbol{a}^T(\boldsymbol{y}-\mathbb{E}[\boldsymbol{y}])(x-\mathbb{E}[x])\right\}\\&amp;\quad -\mathbb{E}\left\{(x-\mathbb{E}[x])(\boldsymbol{y}-\mathbb{E}[\boldsymbol{y}])^T\boldsymbol{a}\right\}+\mathbb{E}\left\{(x-\mathbb{E}[x])^2\right\}\\&amp;=\boldsymbol{a}^T\boldsymbol{C}_{\boldsymbol{yy} }\boldsymbol{a}-\boldsymbol{a}^T\boldsymbol{C}_{\boldsymbol{y}x}-\boldsymbol{C}_{x\boldsymbol{y} }+C_{xx}\end{align}其中$\boldsymbol{C}_{\boldsymbol{yy} }$是$\boldsymbol{y}$的协方差矩阵，$\boldsymbol{C}_{x\boldsymbol{y} }$是$1\times N$的互协方差矢量，且$\boldsymbol{C}_{x\boldsymbol{y} }=\boldsymbol{C}_{\boldsymbol{y}x}^T$。$C_{xx}$是$x$的方差。计算贝叶斯均方误差对$\boldsymbol{a}$的偏导，并令偏导为0，有\begin{align}\frac{\partial \text{Bmse}(\hat{\boldsymbol{x} })}{\partial \boldsymbol{a} }=2C_{xx}\boldsymbol{a}-2\boldsymbol{C}_{\boldsymbol{y}x}=0 \quad \Rightarrow \boldsymbol{a}=C_{\boldsymbol{yy} }^{-1}\boldsymbol{C}_{\boldsymbol{y}x}\end{align}因此，得到\begin{align}\hat{x}&amp;=\boldsymbol{C}_{x\boldsymbol{y} }\boldsymbol{C}_{\boldsymbol{yy} }^{-1}\boldsymbol{y}+\mathbb{E}[x]-\boldsymbol{C}_{x\boldsymbol{y} }\boldsymbol{C}_{\boldsymbol{yy} }^{-1}\mathbb{E}[\boldsymbol{y}]\\&amp;=\boldsymbol{C}_{x\boldsymbol{y} }\boldsymbol{C}_{\boldsymbol{yy} }^{-1}(\boldsymbol{y}-\mathbb{E}[\boldsymbol{y}])+\mathbb{E}[x]\end{align} $\underline{\text{Step 4} }$：扩展到矢量$\hat{\boldsymbol{x} }$。\begin{align}\hat{\boldsymbol{x} }&amp;=\left[\begin{matrix}\mathbb{E}[x_1]\\\mathbb{E}[x_2]\\\vdots\\\mathbb{E}[x_N]\\\end{matrix}\right]+\left[\begin{matrix}\boldsymbol{C}_{x_1\boldsymbol{y} }\boldsymbol{C}_{\boldsymbol{yy} }^{-1}(\boldsymbol{y}-\mathbb{E}[\boldsymbol{y}])\\\boldsymbol{C}_{x_2\boldsymbol{y} }\boldsymbol{C}_{\boldsymbol{yy} }^{-1}(\boldsymbol{y}-\mathbb{E}[\boldsymbol{y}])\\\vdots\\\boldsymbol{C}_{x_N\boldsymbol{y} }\boldsymbol{C}_{\boldsymbol{yy} }^{-1}(\boldsymbol{y}-\mathbb{E}[\boldsymbol{y}])\\\end{matrix}\right]\\&amp;=\mathbb{E}[\boldsymbol{x}]+\boldsymbol{C}_{\boldsymbol{xy} }\boldsymbol{C}_{\boldsymbol{yy} }^{-1}(\boldsymbol{y}-\mathbb{E}[\boldsymbol{y}])\end{align}其中\begin{align}\boldsymbol{C}_{\boldsymbol{yy} }&amp;=\boldsymbol{H}\boldsymbol{C}_{\boldsymbol{xx} }\boldsymbol{H}^T+\boldsymbol{C}_{\boldsymbol{w} }\\\boldsymbol{C}_{\boldsymbol{xy} }&amp;=\boldsymbol{C}_{\boldsymbol{xx} }\boldsymbol{H}^T\end{align}因此\begin{align}\hat{\boldsymbol{x} }_{\text{LMMSE} }&amp;=\mathbb{E}[\boldsymbol{x}]+\boldsymbol{C}_{\boldsymbol{xx} }\boldsymbol{H}^T(\boldsymbol{H}\boldsymbol{C}_{\boldsymbol{xx} }\boldsymbol{H}^T+\boldsymbol{C}_{\boldsymbol{w} })^{-1}(\boldsymbol{y}-\boldsymbol{H}\mathbb{E}[\boldsymbol{x}])\\&amp;=\mathbb{E}[\boldsymbol{x}]+(\boldsymbol{C}_{\boldsymbol{xx} }^{-1}+\boldsymbol{H}^T\boldsymbol{C}_{\boldsymbol{w} }^{-1}\boldsymbol{H})^{-1}\boldsymbol{H}^T\boldsymbol{C}_{\boldsymbol{w} }^{-1}(\boldsymbol{y}-\boldsymbol{H}\mathbb{E}[\boldsymbol{x}])\end{align} Remarks: 通常我们所遇到的模型中，经过功率归一化后，$\boldsymbol{x}$的均值为0，方差为1，以及噪声方差为$\sigma^2$。因此，进一步将其LMMSE估计器简化为\begin{align}\hat{\boldsymbol{x} }_{\text{LMMSE} }=(\boldsymbol{H}^T\boldsymbol{H}+\sigma^2\mathbf{I})^{-1}\boldsymbol{H}^T\boldsymbol{y}\end{align}我们可以看到，相对于LS而言 $\left(\hat{\boldsymbol{x} }=(\boldsymbol{H}^T\boldsymbol{H})^{-1}\boldsymbol{H}^T\boldsymbol{y}\right)$，LMMSE加入了噪声修正项$\sigma^2\mathbf{I}$。 对于简化后的LMMSE估计器模型$\hat{\boldsymbol{x} }=(\boldsymbol{H}^T\boldsymbol{H}+\sigma^2\mathbf{I})^{-1}\boldsymbol{H}^T\boldsymbol{y}$，我们可以将其视为，假设$\boldsymbol{x}\sim \mathcal{N}(\boldsymbol{x}|\boldsymbol{0},\mathbf{I})$的MMSE结果。证明如下\begin{align}p(\boldsymbol{x}|\boldsymbol{y})&amp;=\frac{p(\boldsymbol{x})p(\boldsymbol{y}|\boldsymbol{x})}{p(\boldsymbol{y})}\\&amp;\propto p(\boldsymbol{x})p(\boldsymbol{y}|\boldsymbol{x})\end{align}根据高斯相乘引理：\begin{align}p(\boldsymbol{x})p(\boldsymbol{y}|\boldsymbol{x})&amp;=\mathcal{N}(\boldsymbol{x}|\boldsymbol{0},\mathbf{I})\mathcal{N}(\boldsymbol{y}|\boldsymbol{Hx},\sigma^2\mathbf{I})\\&amp;\propto \mathcal{N}(\boldsymbol{x}|\boldsymbol{0},\mathbf{I})\mathcal{N}(\boldsymbol{x}|(\boldsymbol{H}^T\boldsymbol{H})^{-1}\boldsymbol{H}^T\boldsymbol{y},(\sigma^{-2}\boldsymbol{H}^T\boldsymbol{H})^{-1})\\&amp;\propto \mathcal{N}(\boldsymbol{x}|\boldsymbol{c},\boldsymbol{C})\end{align}其中\begin{align}\boldsymbol{C}&amp;=(\sigma^{-2}\boldsymbol{H}^T\boldsymbol{H}+\mathbf{I})^{-1}\\\boldsymbol{c}&amp;=\boldsymbol{C}\cdot (\sigma^{-2}\boldsymbol{H}^T\boldsymbol{y})=(\boldsymbol{H}^T\boldsymbol{H}+\sigma^2\mathbf{I})^{-1}\boldsymbol{H}^T\boldsymbol{y}\end{align}由于$p(\boldsymbol{x}|\boldsymbol{y})$为高斯分布，因此，该模型的MMSE估计为其后验概率均值，即高斯的均值$\boldsymbol{c}=(\boldsymbol{H}^T\boldsymbol{H}+\sigma^2\mathbf{I})^{-1}\boldsymbol{H}^T\boldsymbol{y}$。我们可以看到，这与LMMSE解一致。 最大后验概率估计（Maximum a posterior, MAP）最大后验概率估计，顾名思义，即选择后验概率最大值所处的$\boldsymbol{x}$作为估计器。\begin{align}\hat{\boldsymbol{x} }_{\text{MAP} }&amp;=\underset{\boldsymbol{x} }{\arg \max} \ p(\boldsymbol{x}|\boldsymbol{y})\\\end{align}估计器$\hat{\boldsymbol{x} }$的元素表示为\begin{align}\hat{x}_i&amp;=\underset{x_i}{\arg \max} \left\{\max_{\boldsymbol{x}_{\backslash i} }\ p(\boldsymbol{x}|\boldsymbol{y})\right\}\\&amp;=\underset{x_i}{\arg \max} \left\{\max_{\boldsymbol{x}_{\backslash i} }\ \log p(\boldsymbol{x}|\boldsymbol{y})\right\}\end{align} Remarks: 特别地，当先验概率为高斯时候，利用高斯相乘引理，我们可以得到后验概率$p(\boldsymbol{x}|\boldsymbol{y})$是关于$\boldsymbol{x}$的高斯分布。此时，最大后验概率估计，为该高斯分布的均值点，相应地，这种情况下的MMSE估计和MAP估计是一致的。然而，通常情况下先验概率为非高斯的，这种情况下，我们可以利用AMP算法或者EP算法来迭代计算边缘后验概率。 References[1] Donoho D L, Maleki A, Montanari A. How to design message passing algorithms for compressed sensing[J]. preprint, 2011.[2] Meng X, Wu S, Kuang L, et al. Concise derivation of complex Bayesian approximate message passing via expectation propagation[J]. arXiv preprint arXiv:1509.08658, 2015.[3] Minka T P. A family of algorithms for approximate Bayesian inference[D]. Massachusetts Institute of Technology, 2001.]]></content>
  </entry>
  <entry>
    <title><![CDATA[矩阵求逆引理及其证明]]></title>
    <url>%2F2018%2F10%2F29%2F%E7%9F%A9%E9%98%B5%E6%B1%82%E9%80%86%E5%BC%95%E7%90%86%E5%8F%8A%E5%85%B6%E8%AF%81%E6%98%8E%2F</url>
    <content type="text"><![CDATA[矩阵求逆引理，或者称Sherman-Woodbury-Morrison公式\begin{align}(\boldsymbol{A}+\boldsymbol{BC})^{-1}=\boldsymbol{A}^{-1}-\boldsymbol{A}^{-1}\boldsymbol{B}(\mathbf{I}+\boldsymbol{C}\boldsymbol{A}^{-1}\boldsymbol{B})^{-1}\boldsymbol{CA}^{-1}\end{align}其中$\boldsymbol{A}\in\mathbb{R}^{n\times n}$是非奇异矩阵，$\boldsymbol{B}\in \mathbb{R}^{n\times p}$， $C\in \mathbb{R}^{p\times n}$。证明：考虑线性等式\begin{align}(\boldsymbol{A}+\boldsymbol{BC})\boldsymbol{x}=\boldsymbol{b}\end{align}其中$\boldsymbol{A}\in\mathbb{R}^{n\times n}$是非奇异矩阵，$\boldsymbol{B}\in \mathbb{R}^{n\times p}$， $C\in \mathbb{R}^{p\times n}$。定义$\boldsymbol{y}=\boldsymbol{Cx}$，则有\begin{align}\left\{ {\begin{matrix}{\boldsymbol{Ax}+\boldsymbol{By}=\boldsymbol{b} } \\{\boldsymbol{y}=\boldsymbol{Cx} }\end{matrix} } \right.\end{align}该方程组可以写成块矩阵的形式\begin{align}\left[ {\begin{matrix}{\boldsymbol{A} } &amp; {\boldsymbol{B} }\\{\boldsymbol{C} } &amp; {-\mathbf{I} }\end{matrix} } \right]\left[ {\begin{matrix}{ \boldsymbol{x} } \\{ \boldsymbol{y} }\end{matrix} } \right]{\rm{ = } }\left[ {\begin{matrix}{\boldsymbol{b} }\\{\boldsymbol{0} }\end{matrix} } \right]\end{align}根据方程组（1）式，有$\boldsymbol{x}=\boldsymbol{A}^{-1}(\boldsymbol{b}-\boldsymbol{By})$，代入方程组（2）式中有\begin{align}\boldsymbol{y}=\boldsymbol{C}\boldsymbol{A}^{-1}(\boldsymbol{b}-\boldsymbol{By})\end{align}合并同类项，有\begin{align}\boldsymbol{y}=(\mathbf{I}+\boldsymbol{CA}^{-1}\boldsymbol{B})^{-1}\boldsymbol{A}^{-1}\boldsymbol{b}\end{align}代入$\boldsymbol{x}=\boldsymbol{A}^{-1}(\boldsymbol{b}-\boldsymbol{B}\boldsymbol{y})$中，得到\begin{align}\boldsymbol{x}=(\boldsymbol{A}^{-1}-\boldsymbol{A}^{-1}\boldsymbol{B}(\mathbf{I}+\boldsymbol{C}\boldsymbol{A}^{-1}\boldsymbol{B})^{-1}\boldsymbol{CA}^{-1})\boldsymbol{b}\end{align}因此，结合$(\boldsymbol{A}+\boldsymbol{BC})\boldsymbol{x}=\boldsymbol{b}$，得到\begin{align}(\boldsymbol{A}+\boldsymbol{BC})^{-1}=\boldsymbol{A}^{-1}-\boldsymbol{A}^{-1}\boldsymbol{B}(\mathbf{I}+\boldsymbol{C}\boldsymbol{A}^{-1}\boldsymbol{B})^{-1}\boldsymbol{CA}^{-1}\end{align}特别地，$\boldsymbol{B},\boldsymbol{C}$为矢量时，有\begin{align}(\boldsymbol{A}+\boldsymbol{uv}^{T})^{-1}=\boldsymbol{A}^{-1}-\frac{\boldsymbol{A}^{-1}\boldsymbol{uv}^T\boldsymbol{A}^{-1} }{1+\boldsymbol{v}^T\boldsymbol{A}^{-1}\boldsymbol{u} }\end{align} 说明：该证明过程是翻译body的书 《Convex Optimization》 p-678的内容。 Remake：单纯的应用矩阵求逆引理并不能降低计算量，当一个矩阵$\boldsymbol{D}$可以分解成$\boldsymbol{A}+\boldsymbol{BC}$，并且已知$\boldsymbol{A}^{-1}$已知，利用矩阵求逆引理，可以得到$\boldsymbol{D}$的逆。]]></content>
      <categories>
        <category>基础数学</category>
      </categories>
      <tags>
        <tag>常用矩阵公式</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[矩阵正态分布]]></title>
    <url>%2F2018%2F10%2F29%2F%E7%9F%A9%E9%98%B5%E6%AD%A3%E6%80%81%E5%88%86%E5%B8%83%2F</url>
    <content type="text"><![CDATA[正态分布关于正态分布的由来，在文章《正态分布的前世今生》中写的很清楚，正态分布是由二项分布而来。正态分布的密度形式首次发现是在棣莫弗-拉普拉斯中心极限定理中。读者可以通过以下几个链接深入了解正态分布的含义正态分布的前世今生（上）正态分布的前世今生（下）为什么正态分布如此常见棣莫弗-拉普拉斯中心极限定理设随机变量$X_n(n=1,2,\cdots)$服从参数为$n,p（0&lt;p&lt;1）$的二项分布，则对于任意$x$， 有\begin{align}\underset{n\rightarrow \infty}{\lim} \text{P}\left\{ {\frac{X_n-np}{\sqrt{np(1-p)} }\leq x}\right\}=\int_{-\infty}^x \frac{1}{\sqrt{2\pi} }\exp \left({-\frac{t^2}{2} }\right)\text{d}t=\Phi(x)\end{align}从该定理中可以看出，当$n\rightarrow \infty$时候，可以用二项分布趋于高斯分布。我们可以通过棣莫弗-拉普拉斯中心极限定理来计算二项分布的概率。 设随机变量随机变量$X$服从正态分布（高斯分布）$X\sim \mathcal{N}(\mu,\sigma_2^2)$，则其概率密度函数表示为\begin{align}p(x)=\frac{1}{\sqrt{2\pi\sigma^2} }\exp\left[{-\frac{(x-\mu)^2}{2\sigma} }\right]\end{align} 多元正态分布设$X,Y$为独立同分布的随机变量，且$X\sim \mathcal{N}(0,1)$。则$X,Y$的联合分布为\begin{align}p(x,y)=p(x)p(y)=\frac{1}{2\pi}\exp(-\frac{1}{2}(x^2+y^2))\end{align}设$\mathbf{z}=[X,Y]^T$，则有\begin{align}p(\boldsymbol{z})=\frac{1}{\pi}\exp (-\frac{1}{2}\boldsymbol{z}^T\boldsymbol{z})=\frac{1}{\pi}\exp (-\frac{1}{2}\text{tr}(\boldsymbol{z}\boldsymbol{z}^T))\end{align}令$\mathbf{z}=\boldsymbol{A}(\mathbf{x}-\boldsymbol{\mu})$，该线性变换的雅可比行列式为\begin{align}J=|\boldsymbol{A}|\end{align}代入$\mathbf{z}$的概率公式中有,\begin{align}p(\boldsymbol{x})=\frac{|\boldsymbol{A}|}{\pi}\exp \left[{-\frac{1}{2}(\boldsymbol{x}-\boldsymbol{\mu})^T\boldsymbol{A}^T\boldsymbol{A}(\boldsymbol{x}-\boldsymbol{\mu})}\right]\end{align}令$\boldsymbol{\Sigma}^{-1}=\boldsymbol{A}^T\boldsymbol{A}$，则\begin{align}p(\boldsymbol{x})=\frac{1}{\sqrt{|\boldsymbol{\Sigma}|}2\pi}\exp \left[{-\frac{1}{2}(\boldsymbol{x}-\boldsymbol{\mu})^T\boldsymbol{\Sigma}^{-1}(\boldsymbol{x}-\boldsymbol{\mu})}\right]\end{align}若$\mathbf{x}$的维数是$n$，则有\begin{align}p(\boldsymbol{x})=\frac{1}{\sqrt{|\boldsymbol{\Sigma}|}(2\pi)^{n/2} }\exp \left[{-\frac{1}{2}(\boldsymbol{x}-\boldsymbol{\mu})^T\boldsymbol{\Sigma}^{-1}(\boldsymbol{x}-\boldsymbol{\mu})}\right]\end{align} 矩阵正态分布设随机矢量$\mathbf{x}\in \mathbb{R}^n$服从多元高斯分布$\mathbf{x}\sim \mathcal{N}(\boldsymbol{0},\mathbf{I})$，随机矢量$\mathbf{y}$与$\mathbf{x}$独立同分布，则$\mathbf{x},\mathbf{y}$的联合概率密度为\begin{align}p(\mathbf{x},\mathbf{y})=\frac{1}{(2\pi)^n}\exp\left[{-\frac{1}{2}(\mathbf{x}^T\mathbf{x}+\mathbf{y}^T\mathbf{y})}\right]\end{align}令$\mathbf{Z}=[\mathbf{x},\mathbf{y}]$，则有\begin{align}p(\mathbf{Z})=\frac{1}{(2\pi)^n}\exp\left[{-\frac{1}{2}\text{tr}(\mathbf{Z}\mathbf{Z}^T)}\right]\end{align}设$\mathbf{Z}=\boldsymbol{A}(\mathbf{X}-\boldsymbol{M})\boldsymbol{B}$, 其中$\boldsymbol{A}\in \mathbb{R}^{n\times n}, \boldsymbol{B}\in \mathbb{R}^{2\times 2}$。其雅可比行列式为\begin{align}J=|\boldsymbol{A}|^n|\boldsymbol{B}|^2\end{align}关于上式的详细解释参见附录A。因此\begin{align}p(\boldsymbol{X})&amp;=\frac{1}{(2\pi)^n}|\boldsymbol{A}|^n|\boldsymbol{B}|^2\exp\left[{-\frac{1}{2}\text{tr}(\boldsymbol{A}(\boldsymbol{X}-\boldsymbol{M})\boldsymbol{BB}^T(\boldsymbol{X}-\boldsymbol{M})^T\boldsymbol{A}^T)}\right]\\&amp;=\frac{1}{(2\pi)^n}|\boldsymbol{A}|^n|\boldsymbol{B}|^2\exp\left[{-\frac{1}{2}\text{tr}(\boldsymbol{A}^T\boldsymbol{A}(\boldsymbol{X}-\boldsymbol{M})\boldsymbol{BB}^T(\boldsymbol{X}-\boldsymbol{M})^T)}\right]\end{align}令$\boldsymbol{\Omega}^{-1}=\boldsymbol{A}^T\boldsymbol{A}$， $\boldsymbol{\Sigma}^{-1}=\boldsymbol{BB}^T$， 则\begin{align}p(\boldsymbol{X})=\frac{1}{(2\pi)^n}|\boldsymbol{\Omega}|^{-n/2}|\boldsymbol{\Sigma}|^{-2/2}\exp\left[{-\frac{1}{2}\text{tr}(\boldsymbol{\Omega}^{-1}(\boldsymbol{X}-\boldsymbol{M})\boldsymbol{\Sigma}^{-1}(\boldsymbol{X}-\boldsymbol{M})^T)}\right]\end{align}若$\boldsymbol{Z}$有$p$列，则\begin{align}p(\boldsymbol{X})=\frac{1}{(2\pi)^{np} }|\boldsymbol{\Omega}|^{-{n}/2}|\boldsymbol{\Sigma}|^{-p/2}\exp\left[{-\frac{1}{2}\text{tr}(\boldsymbol{\Omega}^{-1}(\boldsymbol{X}-\boldsymbol{M})\boldsymbol{\Sigma}^{-1}(\boldsymbol{X}-\boldsymbol{M})^T)}\right]\end{align} 附录 A设线性变换$\boldsymbol{Y}=\boldsymbol{AX}$, 其中$\boldsymbol{X}\in \mathbb{R}^{m\times n}$, $\boldsymbol{A}\in \mathbb{R}^{m\times m}$\begin{align}\text{vec}(\boldsymbol{Y})=\text{vec}(\boldsymbol{AX})=(\mathbf{I}_n \otimes \boldsymbol{A})\text{vec}(\boldsymbol{X})\end{align}因此该线性变换的雅可比行列式为\begin{align}J=|\mathbf{I}_n \otimes \boldsymbol{A}|=|\boldsymbol{A}|^{n}\end{align}设置线性变换$\boldsymbol{Y}=\boldsymbol{XB}$，其中$\boldsymbol{X}\in \mathbb{R}^{m\times n}$, $\boldsymbol{B}\in \mathbb{R}^{n\times n}$\begin{align}\text{vec}(\boldsymbol{Y})=\text{vec}(\boldsymbol{XB})=(\boldsymbol{B}^T\otimes \mathbf{I}_m)\text{vec}(\boldsymbol{X})\end{align}因此该线性变换的雅可比行列式为\begin{align}J=|\boldsymbol{B}^T\otimes \mathbf{I}_m|=|\boldsymbol{B}|^{m}\end{align}]]></content>
      <categories>
        <category>基础数学</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[深度学习笔记之主成分分析]]></title>
    <url>%2F2018%2F10%2F29%2F%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0%E4%B9%8B%E4%B8%BB%E6%88%90%E5%88%86%E5%88%86%E6%9E%90%2F</url>
    <content type="text"><![CDATA[Definitions（定义）主成分分析（principal components analysis, PCA）简称PCA，是一种广泛应用于数据降维（data dimensionality reduction）、有损数据压缩（lossy data compression）、特征提取（feature extraction）以及数据可视化等的一种技术，也被称为Karhunen-Lo`eve变换。 关于PCA的定义主要有两种： PCA是一种将数据投影到低维线性空间（principal subspace，主成子空间）使得投影之后的差异最大的正交投影。 PCA是一种最小化平均投影成本（average projection cost），投影点与数据点之间的均方距离最小，即数据损失精度最小。 Example （例子）我们通过一个有损压缩的例子来介绍PCA。假设我们有$m$个数据点$\left\{ { \boldsymbol{x}^{(1)},\cdots,\boldsymbol{x}^{(m)} }\right\}$，其中数据维度为$\boldsymbol{x}^{(1)}\in \mathbb{R}^n$。因此，存储这些数据，需要$m\times n$个单元。为了节省存储单元，我们考虑有损压缩。有损压缩，意味着我们可以用较少的存储单元储存数据，当然，这会损失些精度。因此我们要尽量的减少精度的损失。 我们将这些数据压缩成低维数据，即每一个$\boldsymbol{x}^{(i)}\in \mathbb{R}^n$都可以找到一个对应的$\boldsymbol{c}\in \mathbb{R}^l, (n\gg l)$。这里，我们用映射$f:\mathbb{R}^n\rightarrow \mathbb{R}^l$来表示，即$f(\boldsymbol{x})=\boldsymbol{c}$。对应的解压缩，我们用函数$\text{g}:\mathbb{R}^l\rightarrow \mathbb{R}^n$来表示，即$\hat{\boldsymbol{x} }=\text{g}(f(\boldsymbol{x}))$。 为了解压缩尽量简单，我们限制解压缩是经过一个线性变换矩阵$\boldsymbol{D}$来完成，则解压缩信号表示为$\text{g}(c)=\boldsymbol{D}\boldsymbol{c}$。这里，我们限制矩阵$\boldsymbol{D}$是列正交矩阵（矩阵中列两两正交）。一般来说，增大$\boldsymbol{D}$的能量，需要降低$\boldsymbol{c}$的能量，因此，我们对$\boldsymbol{D}$进行归一化处理（归一化与未归一化大部分情况的结果是相等的，但也存在一些情况下归一化的情况更好，因此通常我们会对$\boldsymbol{D}$进行归一化处理）。 我们从矩阵乘法的角度来理解$\boldsymbol{x}=\boldsymbol{Dc}$。通常，我们理解矩阵乘法从图的左图出发，但一般不会考虑右图的理解方式。这里，我们从右图的理解方式出发。由于$\boldsymbol{D}$是列正交的，因此$\left\{ { \boldsymbol{d}_i}\right\}_{i\in [l]}$张成了一个$l$空间，而$\left\{ { \boldsymbol{d}_i}\right\}_{i\in [l]}$则是这个$\mathbb{R}^l$空间的一组完备正交基。$\boldsymbol{x}$在基$\boldsymbol{d}_i$上的投影即为$c_i$。我们用$\left\{ { \boldsymbol{d}_i}\right\}_{i\in [l]}$线性表出$\boldsymbol{x}$。\begin{align}\boldsymbol{x}=\sum\limits_{i\in[l]} c_i\boldsymbol{d}_i\end{align}为了使得解压之后的数据损失尽可能少的精度，我们需要通过使得如下损失函数（也称，代价函数 cost function）最小来得到压缩数据的具体表达式\begin{align}\mathcal{R}= || \boldsymbol{x}-\boldsymbol{Dc} ||_2\end{align}损失函数是关于$\boldsymbol{c}$的线性变换$(\boldsymbol{x}-\boldsymbol{Dc})$的范数，因此是convex的（所有的范数均是convex function）。 通过导数工具，有\begin{align}\frac{\partial \mathcal{R} }{\partial \boldsymbol{c} }&amp;=\frac{\partial }{\partial \boldsymbol{c} } \left(\boldsymbol{x}^T\boldsymbol{x}-2\boldsymbol{x}^T\boldsymbol{Dc}+\boldsymbol{c}^T\boldsymbol{D}^T\boldsymbol{D}\boldsymbol{c}\right)\\&amp;\overset{(a)}{=}\frac{\partial }{\partial \boldsymbol{c} } \left(\boldsymbol{x}^T\boldsymbol{x}-2\boldsymbol{x}^T\boldsymbol{Dc}+\boldsymbol{c}^T\boldsymbol{c}\right)\\&amp;=-2\boldsymbol{D}^T\boldsymbol{x}-2\boldsymbol{c}\end{align}其中$(a)$成立，由于$\boldsymbol{D}$是列正交矩阵，因此$\boldsymbol{D}^T\boldsymbol{D}=\mathbf{I}_l$。通过令偏导为0，我们找到函数的驻点$：\boldsymbol{c}^{*}=\boldsymbol{D}^T\boldsymbol{x}$。 给定解压矩阵$\boldsymbol{D}$，我们从损失精度最小的角度出发，得到了压缩数据的表达式$\boldsymbol{c}=\boldsymbol{D}^T\boldsymbol{x}$，对应解压缩数据为$r(\boldsymbol{x})=\boldsymbol{D}\boldsymbol{D}^T\boldsymbol{x}$。为此，我们仍需要确定$\boldsymbol{D}$的形式。这里，我们仍然从损失精度最小的角度出发\begin{align}\boldsymbol{D}^{*}=\underset{\boldsymbol{D} }{\text{arg} \min} ||\boldsymbol{x}-\boldsymbol{DD}^T\boldsymbol{x}||\quad s.t. \quad \boldsymbol{D}^T\boldsymbol{D}=\mathbf{I}_{l}\end{align}显然，我们要从从这个方程中解出$\boldsymbol{D}$来是不可能的。 但是，我们是有$n$个$\boldsymbol{x}^{(i)}$数据的，定义$\boldsymbol{X}\in \mathbb{R}^{m\times n}$，其中$\boldsymbol{X}_{i:}=(\boldsymbol{x}^{(i)})^T$\begin{align}\boldsymbol{D}^*&amp;=\underset{\boldsymbol{D} }{\text{arg} \min }||\boldsymbol{X}^T-\boldsymbol{D}\boldsymbol{D}^T\boldsymbol{X}^T||_F\\&amp;=\underset{\boldsymbol{D} }{\text{arg} \min} ||\boldsymbol{X}-\boldsymbol{X}\boldsymbol{D}\boldsymbol{D}^T||_F\\&amp;=\underset{\boldsymbol{D} }{\text{arg} \min} \text{Tr}\left[{(\boldsymbol{X}-\boldsymbol{X}\boldsymbol{D}\boldsymbol{D}^T)^T(\boldsymbol{X}-\boldsymbol{X}\boldsymbol{D}\boldsymbol{D}^T) }\right]\\&amp;=\underset{\boldsymbol{D} }{\text{arg} \min} \text{Tr}\left({\boldsymbol{X}^T\boldsymbol{X} }\right)-\text{Tr}(\boldsymbol{D}^T\boldsymbol{X}^T\boldsymbol{X}\boldsymbol{D})\\&amp;=\underset{\boldsymbol{D} }{\text{arg} \max} \text{Tr}(\boldsymbol{D}^T\boldsymbol{X}^T\boldsymbol{X}\boldsymbol{D})\end{align} 这里$\boldsymbol{D}$是列正交矩阵，即$\boldsymbol{D}^T\boldsymbol{D}=\mathbf{I}_l$。为使$\text{Tr}(\boldsymbol{D}^T\boldsymbol{X}^T\boldsymbol{X}\boldsymbol{D})$最大，因此选择$\boldsymbol{X}^T\boldsymbol{X}$最大的$l$个特征矢量作为$\boldsymbol{D}$的列向量。对应地，\begin{align}\boldsymbol{X}^T\boldsymbol{X}\left[{\boldsymbol{\xi}_1,\cdots,\boldsymbol{\xi}_l}\right]=\left[{\boldsymbol{\xi}_1,\cdots,\boldsymbol{\xi}_l}\right]\text{Diag}(\lambda_1,\cdots,\lambda_l)\end{align}其中$\boldsymbol{\xi}_i$表示最大的$l$个特征值$\lambda_i\in [l]$对应的特征矢量，$[l]$表示最大的$l$个特征值组成的结合。因此$\max \text{Tr}(\boldsymbol{D}^T\boldsymbol{X}^T\boldsymbol{X}\boldsymbol{D})=\sum_i^l \lambda_i$。 Remarks: 基于上述表述，我们知道矩阵$\boldsymbol{D}$选择数据矩阵$\boldsymbol{X}^T\boldsymbol{X}$的最大$l$个特征值$\lambda_i\in [l]$所对应的特征向量$\boldsymbol{\xi}_i$作为其列，最大程度的保留了矩阵$\boldsymbol{X}^T\boldsymbol{X}$的能量。 这里所得到$\boldsymbol{D}=\left[{\boldsymbol{\xi}_1,\cdots,\boldsymbol{\xi}_l}\right]$并不是驻点，如果$|||\boldsymbol{X}^T-\boldsymbol{D}\boldsymbol{D}^T\boldsymbol{X}^T||_F$对$\boldsymbol{D}$求偏导，并令偏导为零，得到$\boldsymbol{D}=\boldsymbol{0}$，显然这并不是最后的计算结果。]]></content>
      <categories>
        <category>深度学习</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[高斯噪声与中心极限定理]]></title>
    <url>%2F2018%2F10%2F29%2F%E9%AB%98%E6%96%AF%E5%99%AA%E5%A3%B0%E4%B8%8E%E4%B8%AD%E5%BF%83%E6%9E%81%E9%99%90%E5%AE%9A%E7%90%86%2F</url>
    <content type="text"><![CDATA[前言 在众多的信号处理学科领域，噪声一直是衡量算法或系统抗噪声性能的一种指标，笔者是通信专业的学生。对于一个通信系统而言，衡量一个通信系统的质量有两个最重要的指标，一个是有效性，一个是可靠性。有效性的衡量标准是传输带宽，而可靠性的衡量准则是误码率。在误码率的计算中，取决于信噪比和码间串扰等因素。另外，信噪比的定义是信号的能量与噪声的能量的比值。那么如何合理的用数学模型来描述噪声呢？ 在长达四年的本科学习中，笔者发现，通信专业的书中一般假设噪声服从高斯分布（复信号服从循环对称高斯分布，其实部和虚部分别服从高斯分布）。笔者很是不解，为什么噪声是高斯的？记得在“通信原理”课上，当我问老师的时候，老师回答说“中心极限定理”。事实上，很多信号处理领域的学生一直不明白为什么噪声是高斯的，包括很多通信专业的学生。笔者觉得“为什么噪声是高斯的”这个问题是一个很重要的问题，它直接关系到绝大多数的理论的合理性。 实际系统中，由于存在众多噪声源，且大多噪声源（电子噪声，电磁噪声等）满足相互独立假设，当噪声源数量足够多时，且每个噪声源对于总体的贡献可忽略不计，根据中心极限定理可知，这些噪声源的累加的结果服从高斯分布。此篇推导是笔者在考研的时候完成的，现在重新整理与大家分享。由于本人所学知识有限，诚恳地希望读者批评指正。 辛钦大数定律设随机变量$X_1,X_2,\cdots ,X_n$是相互独立同分布的随机变量序列，且具有相同的数学期望$\mathbb{E}[X_i]=\mu,\ (i\in [n])$，作前$n$个随机变量的算数平均值$\frac{1} {n}\sum\nolimits_{i=1}^nX_i$，则$\forall \varepsilon &gt;0$，有\begin{align}\lim\limits_{n\rightarrow \infty} P\left\{ {\left|{\frac{1} {n}\sum\limits_{i=1}^nX_i-\mu}\right|&lt; \varepsilon }\right\}=1\end{align} 证：我们只在随机变量$D(x_i)= { {\sigma }^{2} } \ (i\in [n])$存在，这一条件下证明上述结果。因为\begin{align}\mathbb{E}\left({\frac{1} {n}\sum\limits_{i=1}^nX_i}\right)=\frac{1} {n}\sum\nolimits_{i=1}^n\mathbb{E}[X_i]=\mu\end{align}根据独立性，有\begin{align}D\left({\frac{1} {n}\sum\limits_{i=1}^nX_i}\right)=\frac{1} {n^2}\sum\limits_{i=1}^nD(x_i)=\frac{\sigma^2} {n}\end{align}由切比雪夫不等式【见附录A】，有\begin{align}1-\frac{\sigma^2/n} {\varepsilon^2}\leqP\left\{ {\left|{\frac{1} {n}\sum\limits_{i=1}^nX_i-\mu}\right|&lt; \varepsilon }\right\}\leq1\end{align}当$n\to \infty $时，由夹逼准则，可得\begin{align}\lim\limits_{n\rightarrow \infty} P\left\{ {\left|{\frac{1} {n}\sum\nolimits_{i=1}^nX_i-\mu}\right|&lt; \varepsilon }\right\}=1\end{align} Remarks: 辛钦大数定理所说明的是，当随机变量个数$n\rightarrow \infty$时，这些随机变量的算术平均$\frac{1} {n}\sum\nolimits_{i=1}^nX_i$逐渐趋于概率均值$\mu$。 另一方面，假设$\left\{ {x_i}\right\} (i\in [n])$为随机变量$X$的样本，则当样本个数$n\rightarrow \infty$时，有样本均值趋于统计均值，即$\frac{1} {n}\sum\nolimits_{i=1}^nx_i=\mathbb{E}[X]$。 特征函数大多数情况下，数字特征（均值，方差，各阶距）不能完全确定随机变量的分布（除少数分布，如高斯分布，仅需要一阶矩和二阶矩就可以确定概率分布，详见附录B），我们需要一种与概率分布对应的一种表示，并且相对于概率分布更有利于计算。特征函数就是这样的一种与随机变量对应的表示，既能完全决定随机变量的分布函数，又具有良好的性质。 定义：设$X$为实随机变量，其概率密度为$p_X(x)$，我们称\begin{align}\phi_X(t)=\mathbb{E}[\exp(itX)]=\int e^{itx}p_X(x)\text{d}x\end{align}为随机变量$X$的特征函数（characteristic funciton）这里的$t$是任意实数。 设随机变量$X$的特征函数为$\phi_X(t)$，则存在以下特性： 若随机变量具有相同的特征函数，则它们具有相同的概率分布，即若随机变量$Y$的特征函数$\phi_Y(t)=\phi_X(t)$，则有$p_Y(y)=p_X(x)$。 独立同分布随机变量和的特征函数，等于每个随机变量特征函数的乘积。 设$Z=aX$，则有$\phi_Z(t)=\phi_X(at)$。 Remarks: 从特征函数的定义上可以看出，$X$的特征函数$\phi_X(t)$也是概率密度$p_X(x)$的傅里叶变换的共轭复数。而，傅里叶变换正是一种将信号从时域投影到频域的信号分解技术，其存在的意义，就是将信号转换到频域更有利于相应的处理。因此，不难看出，特征函数与概率密度是对应关系。关于特征函数的这些特性，笔者将在附录B中给出详细证明。 中心极限定理设随机变量$X_1,\cdots ,X_n$相互独立同分布，且具有相同的数学期望和方差，即$\mathbb{E}( { {x}_{i} } )=\mu $，$D( { {x}_{i} } )= { {\sigma }^{2} } $，则随机变量之和的归一化变量\begin{align}Y_n=\frac{\sum\limits_{i=1}^nX_i-\mathbb{E}\left(\sum\limits_{i=1}^nX_i\right)}{\sqrt{D\left(\sum\limits_{i=1}^nX_i\right)} }=\frac{\sum\limits_{i=1}^nX_i-n\mu} {\sqrt{n}\sigma}\end{align}的分布函数$ { {F}_ { { { Y}_{n} } } } (x)$对$\forall x$，满足\begin{align}\lim\limits_{n\rightarrow \infty}F_{Y_n}(x)=\lim\limits_{n\rightarrow \infty}P\left\{ {\frac{\sum\limits_{i=1}^nX_i-n\mu} {\sqrt{n}\sigma}\leq x}\right\}=\int^x_{-\infty}\frac{1} {\sqrt{2\pi} } e^{-t^2/2}\text{d}t=\Phi(x)\end{align}即，$\lim\limits_{n\rightarrow \infty}\frac{\sum\nolimits_{i=1}^nX_i-n\mu} {\sqrt{n}\sigma}\sim \mathcal{N(0,1)}$。 证：$\underline{\text{step 1} } $：设$Z_i=X_i-\mu$，则$Z_i \ (i\in[n])$相互独立，且$\mathbb{E}[Z_i]=0$，$D(Z_i)=\sigma^2$。设$Z_i$的特征函数为$\phi_{Z_i}(t)$，根据特征函数的性质3，随机变量$\frac{1} {\sqrt{n}\sigma}Z_i$的特征函数为$\phi_{Z_i}(\frac{1} {\sqrt{n}\sigma}t)$。而\begin{align}Y_n=\frac{\sum\limits_{i=1}^nX_i-n\mu} {\sqrt{n}\sigma}=\sum\limits_{i=1}^n\left({\frac{Z_i} {\sqrt{n}\sigma} } \right)\end{align}根据，特征函数的性质2，得到$Y_n$的特征函数为$\prod\limits_{i=1}^n\left[{\phi_{Z_i}\left({\frac{1} {\sqrt{n}\sigma }t}\right)}\right]$。 $\underline{\text{step 2} } $：对$\phi_Z(t)$在$t=0$处，进行二阶泰勒展开，有\begin{align}\phi_{Z_i}(t)=\phi_{Z_i}(0)+\phi’_{Z_i}(t)|_{t=0}t+\phi’’_{Z_i}(t)(t)|_{t=0}t^2+o(t^2)\end{align}其中\begin{align}\phi_{Z_i}(0)&amp;=\int_{-\infty}^{+\infty}p_{Z_i}(z)\text{d}z=1\\\phi’_{Z_i}(t)|_{t=0}&amp;=\left[{\int_{-\infty}^{+\infty}jz e^{jtz}p_{Z_i}(z)\text{d}z}\right]_{t=0}=0\\\phi’’_{Z_i}(t)|_{t=0}&amp;=-\left[{\int_{-\infty}^{+\infty}z^2e^{jtz}p_{Z_i}(z)\text{d}z}\right]_{t=0}=-\sigma^2\end{align}故\begin{align}\phi_{Z_i}(t)=1-\frac{\sigma^2} {2}t^2+o(t^2)\end{align}相应地\begin{align}\phi_{Y_n}(t)=\prod\limits_{i=1}^n\left[{\phi_{Z_i}\left({\frac{1} {\sqrt{n}\sigma }t}\right)}\right]=\left[{1-\frac{1} {2n}t^2+o\left({\frac{t^2} {n\sigma^2} } \right)}\right]^n\end{align} $\underline{\text{step 3} } $：\begin{align}\lim\limits_{n\rightarrow \infty} \phi_{Y_n}(t)&amp;=\lim\limits_{n\rightarrow \infty} \left[{1-\frac{1} {2n}t^2+o\left({\frac{t^2} {n\sigma^2} } \right)}\right]^n\\&amp;=\lim\limits_{n\rightarrow \infty} \left({1-\frac{1} {2n}t^2}\right)^n\\&amp;=\lim\limits_{n\rightarrow \infty} \left({1-\frac{1} {2n}t^2}\right)^{\frac{2n} {t^2}\times \frac{t^2} {2} } \\&amp;=e^{-t^2/2}\end{align}其中，最后一个公式成立，根据极限公式$\lim\limits_{x\rightarrow \infty}\left({1+\frac{1} {x} } \right)^x=e$。因此，随机变量$Y_{n}=\lim\limits_{n\rightarrow \infty}\frac{\sum\nolimits_{i=1}^nX_i-n\mu} {\sqrt{n}\sigma}$的特征函数为$\phi_{Y_n}(t)=e^{-t^2/2}$。 $\underline{\text{step 4} } $：又因为标准正态分布的特征函数为$e^{-t^2/2}$【见附录C】，因此有\begin{align}Y_{n}=\lim\limits_{n\rightarrow \infty}\frac{\sum\nolimits_{i=1}^nX_i-n\mu} {\sqrt{n}\sigma}\sim \mathcal{N}(0,1)\end{align} Remarks： 本文所介绍的中心极限定理，是独立同分布的中心极限定理。这里假设$n$个相互独立的随机变量具有相同的均值和方差，因此该中心极限定理的条件相对较强，这中类型的中心极限定理，也称为独立同分布的中心极限定理 若假设$n$个相互独立的变量，具有不同的均值和方差，即$\mathbb{E}[X_i]=\mu_i$，$D(X_i)=\sigma_i^2,(i\in[n])$。该情况为独立同分布的中心极限定理的扩展，称为李亚普诺夫定理。 中心极限定理告诉我们，当相互独立的变量个数足够多，且每个个体对总体的贡献在$n\rightarrow \infty$时，均可忽略不计时，那么这些随机变量的算术平均，服从高斯分布，这也是为什么噪声服从高斯分布，这种假设的合理性解释。 附录A. 切比雪夫不等式设随机变量$X$具有数学期望$\mathbb{E}[X]=\mu$，方差$DX=\sigma^2$，则对于任意的正数$\varepsilon$，有\begin{align}P\left\{ {|X-\mu|\geq \varepsilon}\right\}\leq\frac{\sigma^2} {\varepsilon^2}\end{align} 证：设$X$的概率密度为$p_X(x)$，则有\begin{align}P\left\{ {|X-\mu|\geq \varepsilon}\right\}&amp;\leq \int_{|x-\mu|\geq \varepsilon} \frac{|x-\mu|^2} {\varepsilon^2}p_X(x)\text{d}x\\&amp;\leq \frac{1} {\varepsilon^2}\int_{-\infty}^{\infty} {(x-\mu)^2}p_X(x)\text{d}x\\&amp;=\frac{\sigma^2} {\varepsilon}\end{align} B. 特征函数性质的证明 若随机变量具有相同的特征函数，则它们具有相同的概率分布。证：设随机变量$X$，$Y$具有相同的特征函数，即$\phi_X(t)=\phi_Y(t)$，则有\begin{align}\phi_X(t)=\int_{-\infty}^{+\infty} p_X(x)e^{itx}\text{d}x=\int_{-\infty}^{+\infty} p_Y(y)e^{ity}\text{d}y\quad \Rightarrow \quad p_X(x)=p_Y(y)\end{align}反之，亦成立。 独立同分布随机变量和的特征函数，等于每个随机变量特征函数的乘积。证：设随机变量$X$，$Y$的特征函数分别为$\phi_X(t)$，$\phi_Y(t)$，令$Z=X+Y$，则随机变量$Z$的概率密度，可以由卷积公式得到\begin{align}p_Z(z)=p_X(x)*p_Y(y)=\int_{-\infty}^{+\infty}p_X(x)p_Y(z-x)\text{d}x\end{align}则随机变量$Z$的特征函数为\begin{align}\phi_Z(t)&amp;=\int_{-\infty}^{+\infty} p_Z(z)e^{itz}\text{d}z\\&amp;=\int_{-\infty}^{+\infty} \left({\int_{-\infty}^{+\infty}p_X(x)p_Y(z-x)\text{d}x}\right)e^{itz}\text{d}z\\&amp;=\int_{-\infty}^{+\infty} \left({\int_{-\infty}^{+\infty}p_X(x)p_Y(y)\text{d}x}\right)e^{it(x+y)}\text{d}(x+y)\\&amp;=\left({\int_{-\infty}^{+\infty}p_X(x)e^{jtx}\text{d}x}\right)\left({\int_{-\infty}^{+\infty}p_Y(y)e^{jty}\text{d}y}\right)\\&amp;=\phi_X(t)\phi_Y(t)\end{align} 设$Z=aX$，则有$\phi_Z(t)=\phi_X(at)$。证：设随机变量$X$的概率密度为$p_X(x)$，则随机变量$Z$的累积分布函数（CDF）可以表示为\begin{align}P(Z\leq z)=P\left({X\leq \frac{z} {a} } \right)=\int^{z/a}_{-\infty}p_X(x)\text{d}x\end{align}由于概率密度与累积分布函数互为导数关系，即\begin{align}p_Z(z)&amp;=\frac{\partial P(Z\leq z)} {\partial z}=\frac{\partial } {\partial z}\int^{z/a}_{-\infty}p_X(x)\text{d}x=\frac{1} {a}p_X(z/a)\end{align}因此，随机变量$Z=aX$的特征函数，表示为\begin{align}\phi_Z(t)&amp;=\int_{-\infty}^{+\infty} p_Z(z)e^{itz}\text{d}z\\&amp;=\int_{-\infty}^{+\infty}\frac{1} {a}p_X(x)e^{it(ax)}\text{d}(ax)\\&amp;=\int_{-\infty}^{+\infty}p_X(x)e^{i(at)x}\text{d}x\\&amp;=\phi_X(at)\end{align} C. 高斯分布的特征函数设随机变量$X\sim \mathcal{N}(a,A)$，则其特征函数为\begin{align}\phi_X(t)=e^{ita-\frac{At^2} {2} }\end{align}特别地，当$X\sim \mathcal{N}(0,1)$时，有$\phi_X(t)=e^{-\frac{t^2} {2} } $。证：随机变量$X$的特征函数为\begin{align}\phi_X(t)=\int_{-\infty}^{+\infty} e^{itx} \frac{1} {\sqrt{2\pi A} } \exp\left[-\frac{(x-a)^2} {2A}\right]\text{d}x\end{align}作变量替换$y=\frac{x-\mu} {\sqrt{A} } $，即$x=\sqrt{A}y+\mu$，则\begin{align}\phi_X(t)&amp;=\int_{-\infty}^{+\infty} e^{it(\sqrt{A}y+\mu)} \frac{1} {\sqrt{2\pi A} } \exp\left(-\frac{y^2} {2}\right)\text{d}y\cdot \sqrt{A}\\&amp;=\frac{1} {\sqrt{2\pi} } e^{it\mu} \cdot \int_{-\infty}^{+\infty} e^{it\sqrt{A}y-\frac{y^2} {2} } \text{d}y\\&amp;=\frac{1} {\sqrt{2\pi} } e^{it\mu-\frac{At^2} {2} } \underbrace{\int_{-\infty}^{+\infty}e^{-\frac{(y-it\sqrt{A})^2} {2} } \text{d}y}_{(\text{I})}\\&amp;=e^{it\mu-\frac{At^2} {2} }\end{align}其中，对于$\text{(I)}$的值，我们可以利用概率的归一性进行计算，即\begin{align}\int_{-\infty}^{+\infty} \frac{1} {\sqrt{2\pi} } e^{-(y-a)^2/2}\text{d}y=1\end{align}因此，可以得到\begin{align}\int_{-\infty}^{+\infty} e^{-(y-a)^2/2}\text{d}y=\sqrt{2\pi}\end{align}值得注意的是，$\text{(I)}$中的均值部分为$it\sqrt{A}$，是虚数，但是积分是对实数变量$y$积分，实际上，$\int_{-\infty}^{+\infty}e^{-\frac{(y-it\sqrt{A})^2} {2} } \text{d}y=\int_{-\infty}^{+\infty}e^{-\frac{y^2} {2} } \text{d}y$，具体我们可以由复高斯概率密度得到。]]></content>
      <categories>
        <category>统计信号处理</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[克拉美-罗下界]]></title>
    <url>%2F2018%2F10%2F28%2F%E5%85%8B%E6%8B%89%E7%BE%8E-%E7%BD%97%E4%B8%8B%E7%95%8C%2F</url>
    <content type="text"><![CDATA[估计量的衡量标准对于参数估计问题，目前存在着很多估计算法。那么如何去衡量一个估计器（estimator, 也称估计量或估计算法）的性能，我们主要考量以下三个方面 无偏性(unbiased)。对于参数估计问题，设未知参数$\theta$，估计器模型$\hat{\theta}$。则有$\mathbb{E}[\hat{\theta}]=\theta$。对于估计对象为随机变量，则有$\mathbb{E}[\hat{\theta}]=\mathbb{E}[\theta]$。我们称满足这个条件的估计量为无偏估计量。 有效性(availability)。有效性刻画估计量到真实值的偏离程度，$D(\hat{\theta})=\mathbb{E}[(\hat{\theta}-\mathbb{E}[\hat{\theta}])^2]$，即若存在多种无偏估计器，我们称估计量方差最小的估计器是有效的。 一致性(consistency)。设$\hat{\theta}$为未知参数$\theta$的估计量，若当样本数$N\rightarrow \infty$时，对于任意$\epsilon&gt;0$，有$\lim\limits_{N\rightarrow \infty} P\left\{ {|\hat{\theta}-\theta|&lt;\epsilon}\right\}=1$。我们称$\hat{\theta}$与$\theta$是一致的。一致性所体现的是，当样本总数逐渐增加时，估计量逐渐收敛于真实值。 基于这三点考量，那么很自然我们会问，如何衡量一个无偏估计器是否是有效的。统计信号处理理论中的克拉美罗下界（Cramer-Rao Lower Bound，CRLB）就是衡量一个无偏估计器的有力工具。 克拉美-罗下界（Scale Parameter 标量参数）对于估计参数$\theta$为标量时，假定PDF满足“正则”条件\begin{align}\mathbb{E}\left[{\frac{\partial \ln p(\boldsymbol{x};\theta)}{\partial \theta} }\right]=0\quad (\ \text{for any }\theta \ )\end{align}其中数学期望对$p(\boldsymbol{x};\theta)$取。那么无偏估计量$\hat{\theta}$的方差必然满足\begin{align}D(\hat{\theta}) \geq \frac{1}{-\mathbb{E}\left[{ \frac{\partial ^2\ln p(\boldsymbol{x};\theta)}{\partial \theta^2} }\right]}=\frac{1}{\mathbb{E}\left[{ \left(\frac{\partial \ln p(\boldsymbol{x};\theta)}{\partial \theta}\right)^2}\right]}\end{align}其中导数是在$\theta$的真实值处求，数学期望是对$p(\boldsymbol{x};\theta)$取。因此，我们可以说一个无偏估计量$g(\boldsymbol{x})$达到CRLB，当且仅当，该估计量满足\begin{align}\frac{\partial \ln p(\boldsymbol{x};\theta)}{\partial \theta} =\mathbf{I}(\theta)(g(\boldsymbol{x})-\theta)\end{align}其中，$\mathbf{I}(\theta)=-\mathbb{E}\left[{ \frac{\partial ^2\ln p(\boldsymbol{x};\theta)}{\partial \theta^2} }\right]$，称为Fisher information。证明见博客第四部分。 Remarks: CRLB是衡量一个无偏估计器是否有效的重要工具，也就是说，给定一个无偏估计器，我们可以利用克拉美-罗下界去判断这个估计器是否是最优的。 Example：线性高斯模型（Linear Gaussian model）\begin{align}\boldsymbol{x}=\boldsymbol{h}\theta+\boldsymbol{w}, \quad \boldsymbol{w}\sim \mathcal{N}(\boldsymbol{0},\boldsymbol{C}_{\boldsymbol{w} })\end{align}其中$\theta$是未知参数，$\boldsymbol{x}\in \mathbb{R}^p$是观测值（observed signal），$\boldsymbol{w}$是均值为$\boldsymbol{0}$，协方差矩阵为$\boldsymbol{C}_{\boldsymbol{w} }$的高斯噪声。 我们考虑如下估计器\begin{align}\hat{\theta}=(\boldsymbol{h}^T\boldsymbol{C}_{\boldsymbol{w} }^{-1}\boldsymbol{h})^{-1}\boldsymbol{h}^T\boldsymbol{C}_{\boldsymbol{w} }^{-1}\boldsymbol{x}\end{align}对于该模型，其似然函数$p(\boldsymbol{x};\theta)$为\begin{align}p(\boldsymbol{x};\theta)=\frac{1}{(2\pi)^{p/2}|\boldsymbol{C}_{\boldsymbol{w} }|^{1/2} } \exp \left[{-\frac{1}{2}(\boldsymbol{x}-\boldsymbol{h}\theta)^T\boldsymbol{C}_{\boldsymbol{w} }^{-1}(\boldsymbol{x}-\boldsymbol{h}\theta)}\right]\end{align}因此 无偏性\begin{align}\mathbb{E}[\hat{\theta}]=\int_{\boldsymbol{x} } \hat{\theta} p(\boldsymbol{x};\theta)\text{d}\boldsymbol{x}\end{align}我们可以将$p(\boldsymbol{x};\theta)$看作为自变量为$\boldsymbol{x}$均值为$\boldsymbol{h}\theta$，协方差矩阵为$\boldsymbol{C}_{\boldsymbol{w} }$的高斯PDF，即$\int_{\boldsymbol{x} }\boldsymbol{x}p(\boldsymbol{x};\theta)\text{d}\boldsymbol{x}=\boldsymbol{h}\theta$。因此$\mathbb{E}[\hat{\theta}]=(\boldsymbol{h}^T\boldsymbol{C}_{\boldsymbol{w} }^{-1}\boldsymbol{h})^{-1}\boldsymbol{h}^T\boldsymbol{C}_{\boldsymbol{w} }^{-1}\boldsymbol{h}\theta=\theta$，即$\hat{\theta}$为无偏估计量。 有效性\begin{align}\frac{\partial \ln p(\boldsymbol{x};\theta)}{\partial \theta}&amp;=(\boldsymbol{x}-\boldsymbol{h}\theta)^T\boldsymbol{C}_{\boldsymbol{w} }^{-1}\boldsymbol{h}\\\frac{\partial^2 \ln p(\boldsymbol{x};\theta)}{\partial \theta^2}&amp;=-\boldsymbol{h}^T\boldsymbol{C}_{\boldsymbol{w} }^{-1}\boldsymbol{h}\end{align}关于矩阵求导不太熟悉的朋友可以看下这个网站：https://en.wikipedia.org/wiki/Matrix_calculus。基于上述表述，该系统模型的CRLB为\begin{align}-\frac{1}{-\mathbb{E}\left[{ \frac{\partial ^2\ln p(\boldsymbol{x};\theta)}{\partial \theta^2} }\right]}=\frac{1}{\boldsymbol{h}^T\boldsymbol{C}_{\boldsymbol{w} }^{-1}\boldsymbol{h} }\end{align}而估计器$\hat{\theta}$的方差为\begin{align}D(\hat{\theta})=\left({(\boldsymbol{h}^T\boldsymbol{C}_{\boldsymbol{w} }^{-1}\boldsymbol{h})^{-1}\boldsymbol{h}^T\boldsymbol{C}_{\boldsymbol{w} }^{-1} }\right) \boldsymbol{C}_{\boldsymbol{w} } \left({(\boldsymbol{h}^T\boldsymbol{C}_{\boldsymbol{w} }^{-1}\boldsymbol{h})^{-1}\boldsymbol{h}^T\boldsymbol{C}_{\boldsymbol{w} }^{-1} }\right)^T=(\boldsymbol{h}^T\boldsymbol{C}_{\boldsymbol{w} }^{-1}\boldsymbol{h})^{-1}\end{align}由于$\boldsymbol{h}^T\boldsymbol{C}_{\boldsymbol{w} }^{-1}\boldsymbol{h}$是一维的，有$(\boldsymbol{h}^T\boldsymbol{C}_{\boldsymbol{w} }^{-1}\boldsymbol{h})^{-1}=\frac{1}{\boldsymbol{h}^T\boldsymbol{C}_{\boldsymbol{w} }^{-1}\boldsymbol{h} }$，因此，该估计量是有效的，即该无偏估计量$\theta$的方差可以达到CRLB。 一致性将系统模型$\boldsymbol{x}=\boldsymbol{h}\theta+\boldsymbol{w}$代入估计器中，有\begin{align}\hat{\theta}&amp;=(\boldsymbol{h}^T\boldsymbol{C}_{\boldsymbol{w} }^{-1}\boldsymbol{h})^{-1}\boldsymbol{h}^T\boldsymbol{C}_{\boldsymbol{w} }^{-1}(\boldsymbol{h}\theta+\boldsymbol{w})\\&amp;=\theta+(\boldsymbol{h}^T\boldsymbol{C}_{\boldsymbol{w} }^{-1}\boldsymbol{h})^{-1}\boldsymbol{h}^T\boldsymbol{C}_{\boldsymbol{w} }^{-1}\boldsymbol{w}\end{align}若假设噪声能量一定，即$\boldsymbol{C}_{\boldsymbol{w} }$元素值固定，随着观测样本$p\rightarrow\infty$，则噪声的方差\begin{align}D((\boldsymbol{h}^T\boldsymbol{C}_{\boldsymbol{w} }^{-1}\boldsymbol{h})^{-1}\boldsymbol{h}^T\boldsymbol{C}_{\boldsymbol{w} }^{-1}\boldsymbol{w})=\frac{1}{\boldsymbol{h}^T\boldsymbol{c}_{\boldsymbol{w} }^{-1}\boldsymbol{h} }\end{align}从公式可以看出，假设噪声$\boldsymbol{w}$的每个元素具有相同的方差，则必然$\lim\limits_{p\rightarrow \infty}\boldsymbol{h}^T\boldsymbol{c}_{\boldsymbol{w} }^{-1}\boldsymbol{h}\rightarrow \infty$。因此，当$p\rightarrow \infty$时，我们可以将估计量$\hat{\theta}$看作\begin{align}\hat{\theta}=\theta+n,\quad n\sim\mathcal{N}(0,(\boldsymbol{h}^T\boldsymbol{C}_\boldsymbol{w}^{-1}\boldsymbol{h})^{-1}) \ \ \text{and} \ \lim\limits_{p\rightarrow \infty}\boldsymbol{h}^T\boldsymbol{c}_{\boldsymbol{w} }^{-1}\boldsymbol{h}\rightarrow \infty\end{align}因此，对于任意$\epsilon&gt;0$，有\begin{align}\lim\limits_{N\rightarrow \infty} P\left\{ {|\hat{\theta}-\theta|&lt;\epsilon}\right\}=1\end{align}即，该估计量满足一致性。 CRLB证明由于$\hat{\theta}$是无偏估计，即\begin{align}&amp;\int_{\boldsymbol{x} } (\hat{\theta}-\theta)p(\boldsymbol{x};\theta)\text{d}\boldsymbol{x}=0\\\end{align}等式两边对$\theta$求偏导有\begin{align}&amp;\int (\hat{\theta}-\theta)\frac{\partial p(\boldsymbol{x};\theta)}{\partial \theta}\text{d}\boldsymbol{x}=1\\\Rightarrow&amp; \int (\hat{\theta}-\theta)\frac{\partial \ln p(\boldsymbol{x};\theta)}{\partial \theta}p(\boldsymbol{x};\theta)\text{d}\boldsymbol{x}=1\\\Rightarrow &amp; \int (\hat{\theta}-\theta)\sqrt{p(\boldsymbol{x};\theta)}\frac{\partial \ln p(\boldsymbol{x};\theta)}{\partial \theta}\sqrt{p(\boldsymbol{x};\theta)}\text{d}\boldsymbol{x}=1\end{align}由于柯西-施瓦茨不等式\begin{align}\int f^2(x)\text{d}x \int g^2(x)\text{d}x \geq\left({\int f(x)g(x)\text{d}x}\right)^2\end{align}当且仅当$f(x)=g(x)$时，取等号。 根据柯西-施瓦茨不等式（Cauchy-Schwarz inequality），有\begin{align}&amp;\left({ \int (\hat{\theta}-\theta)^2{p(\boldsymbol{x};\theta)}\text{d}\boldsymbol{x} }\right)\left({\int \left(\frac{\partial \ln p(\boldsymbol{x};\theta)}{\partial \theta}\right)^2p(\boldsymbol{x};\theta)\text{d}\boldsymbol{x} }\right)\geq 1\\\Rightarrow &amp;\int (\hat{\theta}-\theta)^2{p(\boldsymbol{x};\theta)}\text{d}\boldsymbol{x}\geq \frac{1}{\left({\int \left(\frac{\partial \ln p(\boldsymbol{x};\theta)}{\partial \theta}\right)^2p(\boldsymbol{x};\theta)\text{d}\boldsymbol{x} }\right)}\end{align}即\begin{align}D(\hat{\theta})\geq \frac{1}{\mathbb{E}\left[{\left(\frac{\partial \ln p(\boldsymbol{x};\theta)}{\partial \theta}\right)^2}\right]}\end{align}现在只需证明\begin{align}\mathbb{E}\left[{\left(\frac{\partial \ln p(\boldsymbol{x};\theta)}{\partial \theta}\right)^2}\right]=-\mathbb{E}\left[{ \frac{\partial ^2\ln p(\boldsymbol{x};\theta)}{\partial \theta^2} }\right]\end{align}证：由正则条件$\mathbb{E}\left[{\frac{\partial \ln p(\boldsymbol{x};\theta)}{\partial \theta} }\right]=0$，等式两边对$\theta$求偏导，有\begin{align}&amp;\frac{\partial }{\partial \theta} \int \frac{\partial \ln p(\boldsymbol{x};\theta)}{\partial \theta} p(\boldsymbol{x};\theta)\text{d}\boldsymbol{x}=0\\\Rightarrow&amp; \int \left[{\frac{\partial^2 \ln p(\boldsymbol{x};\theta)}{\partial \theta^2}p(\boldsymbol{x};\theta)+\frac{\partial \ln p(\boldsymbol{x};\theta)}{\partial \theta}\frac{\partial p(\boldsymbol{x};\theta)}{\partial \theta} }\right]\text{d}\boldsymbol{x}=0\\\Rightarrow &amp; \int \frac{\partial^2 \ln p(\boldsymbol{x};\theta)}{\partial \theta^2}p(\boldsymbol{x};\theta)\text{d}\boldsymbol{x}=-\int \left({\frac{\partial \ln p(\boldsymbol{x};\theta)}{\partial \theta} }\right)^2p(\boldsymbol{x};\theta)\text{d}\boldsymbol{x}\end{align} 现在证明，若估计量$\hat{\theta}=\text{g}(\boldsymbol{x})$可以达到CRLB，则有\begin{align}\frac{\partial \ln p(\boldsymbol{x};\theta)}{\partial \theta} =\mathbf{I}(\theta)(g(\boldsymbol{x})-\theta)\end{align}其中，$\mathbf{I}(\theta)=-\mathbb{E}\left[{ \frac{\partial ^2\ln p(\boldsymbol{x};\theta)}{\partial \theta^2} }\right]$。 证：等式两边同时对$\theta$求偏导，有\begin{align}\frac{\partial^2 \ln p(\boldsymbol{x};\theta)}{\partial \theta^2} =\frac{\partial \mathbf{I}(\theta)}{\partial \theta}(g(\boldsymbol{x})-\theta)-\mathbf{I}(\theta)\end{align}等式两边同时对乘上$p(\boldsymbol{x};\theta)$，并对$\boldsymbol{x}$积分，得\begin{align}\mathbb{E}\left[{\frac{\partial^2 \ln p(\boldsymbol{x};\theta)}{\partial \theta^2} }\right]=-\mathbf{I}(\theta)\end{align}证毕。]]></content>
      <categories>
        <category>统计信号处理</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[开通个人blog]]></title>
    <url>%2F2018%2F10%2F27%2F%E5%BC%80%E9%80%9A%E4%B8%AA%E4%BA%BAblog%2F</url>
    <content type="text"><![CDATA[学习的方法方式有很多种，做笔记是其中一种比较好的选择。我本人从大学本科阶段就有写一些学习笔记的习惯。从一开始是用word来记录，那时候还不会latex，所以用的是mathtype。这个工具比较容易上手，所见即所得嘛。那时候写写笔记，装装逼发给别人看看也确实会满足个人虚荣心，不过却实有帮助一些人。 到了研究生阶段，开始写blog。这一点主要是因为这时候我已经掌握了latex的代码编写规则。此外，刚好那时候，国内开始有一些博客论坛开始支持latex公式，所以在研一的时候，我开通了一个CSDN账号。CSDN那个账号到现在浏览量不是很多，但是所写的博客却也是我在学习过程中的积累。CSDN博客，我一直在用。不过近段时间，CSDN博客改版了。有一些功能，在下实在不敢苟同，比如：不支持公式组环境以及公式没有之前版本美观等。由于我所研究的领域，较多的是公式推导等工作。公式写下来，通常一大堆，多行公式也是家常便饭。无奈之下，我便心生开通个人博客的想法。经过一番努力，所创建的博客网站及其功能也大致能满足在下之需求。今后我会尽可能地利用到这个博客网站记录工作、学习、生活的点滴。O(∩_∩)O]]></content>
      <categories>
        <category>随笔</category>
      </categories>
  </entry>
</search>
